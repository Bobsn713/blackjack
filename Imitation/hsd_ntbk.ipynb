{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991806d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ceab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for data processing\n",
    "def card_to_num(card):\n",
    "    raw_rank = card[:-1]\n",
    "    \n",
    "    ranks = {\n",
    "        '2' : 0,\n",
    "        '3' : 1,\n",
    "        '4' : 2, \n",
    "        '5' : 3,\n",
    "        '6' : 4, \n",
    "        '7' : 5, \n",
    "        '8' : 6, \n",
    "        '9' : 7, \n",
    "        '10': 8, \n",
    "        'J' : 9, \n",
    "        'Q' : 10, \n",
    "        'K' : 11, \n",
    "        'A': 12\n",
    "    }\n",
    "\n",
    "    return ranks[raw_rank]\n",
    "\n",
    "def hand_to_list(hand):\n",
    "    '''Takes hand like KH-AC and outputs list of card numbers'''\n",
    "    hand_list_1 = hand.split(\"-\")\n",
    "    hand_list_2 = [card_to_num(card) for card in hand_list_1]\n",
    "    return hand_list_2\n",
    "\n",
    "result_mapping = {\n",
    "    'hit' : 0,\n",
    "    'stand' : 1,\n",
    "    'double down' : 2\n",
    "}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Defining Dataset Class\n",
    "class Blackjack_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9ef0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "hit_stand_dd_df = pd.read_csv('CSVs/hit_stand_dd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b8eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned hit_stand_dd\n",
    "MAX_LEN = 7\n",
    "hit_stand_dd_df['dealer_upcard'] = hit_stand_dd_df['dealer_upcard'].apply(card_to_num)\n",
    "hit_stand_dd_df['player_hand'] = hit_stand_dd_df['player_hand'].apply(hand_to_list)\n",
    "hit_stand_dd_df['result'] = hit_stand_dd_df['result'].map(result_mapping)\n",
    "hit_stand_dd_df['player_hand'] = [\n",
    "    hand + [0] * (MAX_LEN - len(hand)) if len(hand) < MAX_LEN else hand[:MAX_LEN] for hand in hit_stand_dd_df['player_hand']\n",
    "]\n",
    "\n",
    "\n",
    "# Turning into tensor matrices\n",
    "# hit_stand_dd\n",
    "x1 = torch.tensor(hit_stand_dd_df['player_hand'].to_list(), dtype=torch.float32)\n",
    "x2 = torch.tensor(hit_stand_dd_df['dealer_upcard'].values, dtype=torch.float32).unsqueeze(1)\n",
    "x3 = torch.tensor(hit_stand_dd_df['can_double'].values, dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(hit_stand_dd_df['result'].values, dtype=torch.long)\n",
    "\n",
    "X = torch.cat([x1,x2,x3], dim=1)\n",
    "\n",
    "hit_stand_dd_dataset = Blackjack_Dataset(X,y)\n",
    "train_hsd, test_hsd = train_test_split(hit_stand_dd_dataset, test_size=0.2)\n",
    "\n",
    "hsd_train_dataloader = DataLoader(train_hsd, batch_size=batch_size, shuffle=True)\n",
    "hsd_test_dataloader = DataLoader(test_hsd, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd46cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Printing Training Update on every 100th batch\n",
    "        if (batch + 1) % 100 == 0: \n",
    "            loss = loss.item()\n",
    "            current = batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    #Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader: \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93418e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hsd_NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "hsd_model = hsd_NeuralNetwork()\n",
    "\n",
    "learning_rate = 0.0005 \n",
    "epochs = 40\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "hsd_optimizer = torch.optim.SGD(hsd_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb83a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------\n",
      "loss: 0.970361  [ 3200/209452]\n",
      "loss: 0.867522  [ 6400/209452]\n",
      "loss: 0.894082  [ 9600/209452]\n",
      "loss: 0.726908  [12800/209452]\n",
      "loss: 0.900585  [16000/209452]\n",
      "loss: 0.751150  [19200/209452]\n",
      "loss: 0.729588  [22400/209452]\n",
      "loss: 0.797868  [25600/209452]\n",
      "loss: 0.697163  [28800/209452]\n",
      "loss: 0.939191  [32000/209452]\n",
      "loss: 0.846400  [35200/209452]\n",
      "loss: 0.874388  [38400/209452]\n",
      "loss: 0.601201  [41600/209452]\n",
      "loss: 0.797319  [44800/209452]\n",
      "loss: 0.665578  [48000/209452]\n",
      "loss: 0.650254  [51200/209452]\n",
      "loss: 0.704794  [54400/209452]\n",
      "loss: 0.537581  [57600/209452]\n",
      "loss: 0.709764  [60800/209452]\n",
      "loss: 0.664131  [64000/209452]\n",
      "loss: 0.654036  [67200/209452]\n",
      "loss: 0.504442  [70400/209452]\n",
      "loss: 0.672322  [73600/209452]\n",
      "loss: 0.782632  [76800/209452]\n",
      "loss: 0.698408  [80000/209452]\n",
      "loss: 0.832971  [83200/209452]\n",
      "loss: 0.465097  [86400/209452]\n",
      "loss: 0.739153  [89600/209452]\n",
      "loss: 0.813823  [92800/209452]\n",
      "loss: 0.682154  [96000/209452]\n",
      "loss: 0.641666  [99200/209452]\n",
      "loss: 0.636022  [102400/209452]\n",
      "loss: 0.547512  [105600/209452]\n",
      "loss: 0.603978  [108800/209452]\n",
      "loss: 0.763350  [112000/209452]\n",
      "loss: 0.715234  [115200/209452]\n",
      "loss: 0.950727  [118400/209452]\n",
      "loss: 0.707670  [121600/209452]\n",
      "loss: 0.596913  [124800/209452]\n",
      "loss: 0.665743  [128000/209452]\n",
      "loss: 0.872311  [131200/209452]\n",
      "loss: 0.812008  [134400/209452]\n",
      "loss: 0.681513  [137600/209452]\n",
      "loss: 0.590039  [140800/209452]\n",
      "loss: 0.584147  [144000/209452]\n",
      "loss: 0.665511  [147200/209452]\n",
      "loss: 0.733403  [150400/209452]\n",
      "loss: 0.658256  [153600/209452]\n",
      "loss: 0.629057  [156800/209452]\n",
      "loss: 0.769627  [160000/209452]\n",
      "loss: 0.631067  [163200/209452]\n",
      "loss: 0.646218  [166400/209452]\n",
      "loss: 0.921533  [169600/209452]\n",
      "loss: 0.785578  [172800/209452]\n",
      "loss: 0.733666  [176000/209452]\n",
      "loss: 0.583009  [179200/209452]\n",
      "loss: 0.629230  [182400/209452]\n",
      "loss: 0.654542  [185600/209452]\n",
      "loss: 0.838151  [188800/209452]\n",
      "loss: 0.681103  [192000/209452]\n",
      "loss: 0.632156  [195200/209452]\n",
      "loss: 0.668430  [198400/209452]\n",
      "loss: 0.489293  [201600/209452]\n",
      "loss: 0.720874  [204800/209452]\n",
      "loss: 0.683364  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.664481 \n",
      "\n",
      "Epoch 2\n",
      "---------------------------\n",
      "loss: 0.791436  [ 3200/209452]\n",
      "loss: 0.652366  [ 6400/209452]\n",
      "loss: 0.628289  [ 9600/209452]\n",
      "loss: 0.622728  [12800/209452]\n",
      "loss: 0.766938  [16000/209452]\n",
      "loss: 0.520667  [19200/209452]\n",
      "loss: 0.757701  [22400/209452]\n",
      "loss: 0.575412  [25600/209452]\n",
      "loss: 1.075223  [28800/209452]\n",
      "loss: 0.766046  [32000/209452]\n",
      "loss: 0.734918  [35200/209452]\n",
      "loss: 0.645547  [38400/209452]\n",
      "loss: 0.555481  [41600/209452]\n",
      "loss: 0.606832  [44800/209452]\n",
      "loss: 0.699652  [48000/209452]\n",
      "loss: 0.625587  [51200/209452]\n",
      "loss: 0.606438  [54400/209452]\n",
      "loss: 0.659598  [57600/209452]\n",
      "loss: 0.720584  [60800/209452]\n",
      "loss: 0.587833  [64000/209452]\n",
      "loss: 0.849719  [67200/209452]\n",
      "loss: 0.617951  [70400/209452]\n",
      "loss: 0.391904  [73600/209452]\n",
      "loss: 0.678357  [76800/209452]\n",
      "loss: 0.389475  [80000/209452]\n",
      "loss: 0.777641  [83200/209452]\n",
      "loss: 0.685684  [86400/209452]\n",
      "loss: 0.690947  [89600/209452]\n",
      "loss: 0.615681  [92800/209452]\n",
      "loss: 0.622429  [96000/209452]\n",
      "loss: 0.894433  [99200/209452]\n",
      "loss: 0.631791  [102400/209452]\n",
      "loss: 0.618093  [105600/209452]\n",
      "loss: 0.637007  [108800/209452]\n",
      "loss: 0.671355  [112000/209452]\n",
      "loss: 0.607089  [115200/209452]\n",
      "loss: 0.662994  [118400/209452]\n",
      "loss: 0.715556  [121600/209452]\n",
      "loss: 0.707976  [124800/209452]\n",
      "loss: 0.638503  [128000/209452]\n",
      "loss: 0.676350  [131200/209452]\n",
      "loss: 0.568827  [134400/209452]\n",
      "loss: 0.542935  [137600/209452]\n",
      "loss: 0.637269  [140800/209452]\n",
      "loss: 0.530843  [144000/209452]\n",
      "loss: 0.417300  [147200/209452]\n",
      "loss: 0.623724  [150400/209452]\n",
      "loss: 0.517509  [153600/209452]\n",
      "loss: 0.636792  [156800/209452]\n",
      "loss: 0.664431  [160000/209452]\n",
      "loss: 0.583572  [163200/209452]\n",
      "loss: 0.502851  [166400/209452]\n",
      "loss: 0.553002  [169600/209452]\n",
      "loss: 0.453077  [172800/209452]\n",
      "loss: 0.538160  [176000/209452]\n",
      "loss: 0.778906  [179200/209452]\n",
      "loss: 0.540312  [182400/209452]\n",
      "loss: 0.635766  [185600/209452]\n",
      "loss: 0.615374  [188800/209452]\n",
      "loss: 0.458992  [192000/209452]\n",
      "loss: 0.694517  [195200/209452]\n",
      "loss: 0.566473  [198400/209452]\n",
      "loss: 0.575503  [201600/209452]\n",
      "loss: 0.591296  [204800/209452]\n",
      "loss: 0.589047  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.618975 \n",
      "\n",
      "Epoch 3\n",
      "---------------------------\n",
      "loss: 0.610718  [ 3200/209452]\n",
      "loss: 0.602470  [ 6400/209452]\n",
      "loss: 0.786606  [ 9600/209452]\n",
      "loss: 0.607961  [12800/209452]\n",
      "loss: 0.489028  [16000/209452]\n",
      "loss: 0.644706  [19200/209452]\n",
      "loss: 0.425018  [22400/209452]\n",
      "loss: 0.560334  [25600/209452]\n",
      "loss: 0.750672  [28800/209452]\n",
      "loss: 0.442259  [32000/209452]\n",
      "loss: 0.846709  [35200/209452]\n",
      "loss: 0.718996  [38400/209452]\n",
      "loss: 0.810180  [41600/209452]\n",
      "loss: 0.471776  [44800/209452]\n",
      "loss: 0.684690  [48000/209452]\n",
      "loss: 0.550815  [51200/209452]\n",
      "loss: 0.540992  [54400/209452]\n",
      "loss: 0.379286  [57600/209452]\n",
      "loss: 0.507423  [60800/209452]\n",
      "loss: 0.618918  [64000/209452]\n",
      "loss: 0.573899  [67200/209452]\n",
      "loss: 0.617764  [70400/209452]\n",
      "loss: 0.614886  [73600/209452]\n",
      "loss: 0.643967  [76800/209452]\n",
      "loss: 0.555616  [80000/209452]\n",
      "loss: 0.743508  [83200/209452]\n",
      "loss: 0.494763  [86400/209452]\n",
      "loss: 0.443604  [89600/209452]\n",
      "loss: 0.662679  [92800/209452]\n",
      "loss: 0.559244  [96000/209452]\n",
      "loss: 0.845318  [99200/209452]\n",
      "loss: 0.540875  [102400/209452]\n",
      "loss: 0.599450  [105600/209452]\n",
      "loss: 0.807098  [108800/209452]\n",
      "loss: 0.594114  [112000/209452]\n",
      "loss: 0.764193  [115200/209452]\n",
      "loss: 0.579994  [118400/209452]\n",
      "loss: 0.491458  [121600/209452]\n",
      "loss: 0.650931  [124800/209452]\n",
      "loss: 0.522474  [128000/209452]\n",
      "loss: 0.524553  [131200/209452]\n",
      "loss: 0.481554  [134400/209452]\n",
      "loss: 0.626270  [137600/209452]\n",
      "loss: 0.722483  [140800/209452]\n",
      "loss: 0.654555  [144000/209452]\n",
      "loss: 0.583270  [147200/209452]\n",
      "loss: 0.597624  [150400/209452]\n",
      "loss: 0.424602  [153600/209452]\n",
      "loss: 0.444528  [156800/209452]\n",
      "loss: 0.711600  [160000/209452]\n",
      "loss: 0.566538  [163200/209452]\n",
      "loss: 0.441817  [166400/209452]\n",
      "loss: 0.374683  [169600/209452]\n",
      "loss: 0.650382  [172800/209452]\n",
      "loss: 0.512035  [176000/209452]\n",
      "loss: 0.651001  [179200/209452]\n",
      "loss: 0.572003  [182400/209452]\n",
      "loss: 0.703822  [185600/209452]\n",
      "loss: 0.646933  [188800/209452]\n",
      "loss: 0.546433  [192000/209452]\n",
      "loss: 0.640141  [195200/209452]\n",
      "loss: 0.570242  [198400/209452]\n",
      "loss: 0.669950  [201600/209452]\n",
      "loss: 0.490654  [204800/209452]\n",
      "loss: 0.540072  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.594316 \n",
      "\n",
      "Epoch 4\n",
      "---------------------------\n",
      "loss: 0.684810  [ 3200/209452]\n",
      "loss: 0.549897  [ 6400/209452]\n",
      "loss: 0.670235  [ 9600/209452]\n",
      "loss: 0.692002  [12800/209452]\n",
      "loss: 0.845076  [16000/209452]\n",
      "loss: 0.516553  [19200/209452]\n",
      "loss: 0.890987  [22400/209452]\n",
      "loss: 0.744131  [25600/209452]\n",
      "loss: 0.394117  [28800/209452]\n",
      "loss: 0.575978  [32000/209452]\n",
      "loss: 0.669134  [35200/209452]\n",
      "loss: 0.622970  [38400/209452]\n",
      "loss: 0.517583  [41600/209452]\n",
      "loss: 0.685132  [44800/209452]\n",
      "loss: 0.623676  [48000/209452]\n",
      "loss: 0.501180  [51200/209452]\n",
      "loss: 0.529440  [54400/209452]\n",
      "loss: 0.558273  [57600/209452]\n",
      "loss: 0.610116  [60800/209452]\n",
      "loss: 0.443904  [64000/209452]\n",
      "loss: 0.580154  [67200/209452]\n",
      "loss: 0.629784  [70400/209452]\n",
      "loss: 0.674372  [73600/209452]\n",
      "loss: 0.738791  [76800/209452]\n",
      "loss: 0.483222  [80000/209452]\n",
      "loss: 0.516645  [83200/209452]\n",
      "loss: 0.634522  [86400/209452]\n",
      "loss: 0.760061  [89600/209452]\n",
      "loss: 0.644479  [92800/209452]\n",
      "loss: 0.422634  [96000/209452]\n",
      "loss: 0.565925  [99200/209452]\n",
      "loss: 0.551707  [102400/209452]\n",
      "loss: 0.617441  [105600/209452]\n",
      "loss: 0.506583  [108800/209452]\n",
      "loss: 0.682517  [112000/209452]\n",
      "loss: 0.482747  [115200/209452]\n",
      "loss: 0.536361  [118400/209452]\n",
      "loss: 0.576529  [121600/209452]\n",
      "loss: 0.534724  [124800/209452]\n",
      "loss: 0.630365  [128000/209452]\n",
      "loss: 0.529055  [131200/209452]\n",
      "loss: 0.614302  [134400/209452]\n",
      "loss: 0.608756  [137600/209452]\n",
      "loss: 0.611848  [140800/209452]\n",
      "loss: 0.573246  [144000/209452]\n",
      "loss: 0.433971  [147200/209452]\n",
      "loss: 0.536711  [150400/209452]\n",
      "loss: 0.732864  [153600/209452]\n",
      "loss: 0.559730  [156800/209452]\n",
      "loss: 0.490718  [160000/209452]\n",
      "loss: 0.470091  [163200/209452]\n",
      "loss: 0.648572  [166400/209452]\n",
      "loss: 0.479200  [169600/209452]\n",
      "loss: 0.593846  [172800/209452]\n",
      "loss: 0.663995  [176000/209452]\n",
      "loss: 0.379956  [179200/209452]\n",
      "loss: 0.523072  [182400/209452]\n",
      "loss: 0.570999  [185600/209452]\n",
      "loss: 0.708908  [188800/209452]\n",
      "loss: 0.703697  [192000/209452]\n",
      "loss: 0.688476  [195200/209452]\n",
      "loss: 0.571431  [198400/209452]\n",
      "loss: 0.724699  [201600/209452]\n",
      "loss: 0.451416  [204800/209452]\n",
      "loss: 0.850244  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.576065 \n",
      "\n",
      "Epoch 5\n",
      "---------------------------\n",
      "loss: 0.470171  [ 3200/209452]\n",
      "loss: 0.472243  [ 6400/209452]\n",
      "loss: 0.711128  [ 9600/209452]\n",
      "loss: 0.487825  [12800/209452]\n",
      "loss: 0.511085  [16000/209452]\n",
      "loss: 0.906712  [19200/209452]\n",
      "loss: 0.543375  [22400/209452]\n",
      "loss: 0.634142  [25600/209452]\n",
      "loss: 0.640698  [28800/209452]\n",
      "loss: 0.343731  [32000/209452]\n",
      "loss: 0.720400  [35200/209452]\n",
      "loss: 0.652169  [38400/209452]\n",
      "loss: 0.452400  [41600/209452]\n",
      "loss: 0.308511  [44800/209452]\n",
      "loss: 0.531343  [48000/209452]\n",
      "loss: 0.640215  [51200/209452]\n",
      "loss: 0.536924  [54400/209452]\n",
      "loss: 0.512385  [57600/209452]\n",
      "loss: 0.620655  [60800/209452]\n",
      "loss: 0.573069  [64000/209452]\n",
      "loss: 0.416444  [67200/209452]\n",
      "loss: 0.541661  [70400/209452]\n",
      "loss: 0.449120  [73600/209452]\n",
      "loss: 0.637772  [76800/209452]\n",
      "loss: 0.556772  [80000/209452]\n",
      "loss: 0.538589  [83200/209452]\n",
      "loss: 0.687906  [86400/209452]\n",
      "loss: 0.680448  [89600/209452]\n",
      "loss: 0.621523  [92800/209452]\n",
      "loss: 0.574875  [96000/209452]\n",
      "loss: 0.644777  [99200/209452]\n",
      "loss: 0.640252  [102400/209452]\n",
      "loss: 0.607396  [105600/209452]\n",
      "loss: 0.703780  [108800/209452]\n",
      "loss: 0.618319  [112000/209452]\n",
      "loss: 0.865393  [115200/209452]\n",
      "loss: 0.791449  [118400/209452]\n",
      "loss: 0.704987  [121600/209452]\n",
      "loss: 0.595689  [124800/209452]\n",
      "loss: 0.609263  [128000/209452]\n",
      "loss: 0.466026  [131200/209452]\n",
      "loss: 0.373794  [134400/209452]\n",
      "loss: 0.527628  [137600/209452]\n",
      "loss: 0.472359  [140800/209452]\n",
      "loss: 0.598348  [144000/209452]\n",
      "loss: 0.713894  [147200/209452]\n",
      "loss: 0.568281  [150400/209452]\n",
      "loss: 0.721204  [153600/209452]\n",
      "loss: 0.417675  [156800/209452]\n",
      "loss: 0.492128  [160000/209452]\n",
      "loss: 0.455753  [163200/209452]\n",
      "loss: 0.373420  [166400/209452]\n",
      "loss: 0.562447  [169600/209452]\n",
      "loss: 0.612334  [172800/209452]\n",
      "loss: 0.453300  [176000/209452]\n",
      "loss: 0.464165  [179200/209452]\n",
      "loss: 0.511577  [182400/209452]\n",
      "loss: 0.789542  [185600/209452]\n",
      "loss: 0.600331  [188800/209452]\n",
      "loss: 0.522123  [192000/209452]\n",
      "loss: 0.634688  [195200/209452]\n",
      "loss: 0.650905  [198400/209452]\n",
      "loss: 0.290912  [201600/209452]\n",
      "loss: 0.690091  [204800/209452]\n",
      "loss: 0.483192  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.562193 \n",
      "\n",
      "Epoch 6\n",
      "---------------------------\n",
      "loss: 0.756399  [ 3200/209452]\n",
      "loss: 0.521214  [ 6400/209452]\n",
      "loss: 0.575951  [ 9600/209452]\n",
      "loss: 0.602503  [12800/209452]\n",
      "loss: 0.519036  [16000/209452]\n",
      "loss: 0.526998  [19200/209452]\n",
      "loss: 0.427666  [22400/209452]\n",
      "loss: 0.488824  [25600/209452]\n",
      "loss: 0.440252  [28800/209452]\n",
      "loss: 0.649604  [32000/209452]\n",
      "loss: 0.653167  [35200/209452]\n",
      "loss: 0.529108  [38400/209452]\n",
      "loss: 0.416874  [41600/209452]\n",
      "loss: 0.550885  [44800/209452]\n",
      "loss: 0.541762  [48000/209452]\n",
      "loss: 0.538930  [51200/209452]\n",
      "loss: 0.657013  [54400/209452]\n",
      "loss: 0.752057  [57600/209452]\n",
      "loss: 0.646202  [60800/209452]\n",
      "loss: 0.588386  [64000/209452]\n",
      "loss: 0.518536  [67200/209452]\n",
      "loss: 0.600820  [70400/209452]\n",
      "loss: 0.582364  [73600/209452]\n",
      "loss: 0.574433  [76800/209452]\n",
      "loss: 0.661506  [80000/209452]\n",
      "loss: 0.679374  [83200/209452]\n",
      "loss: 0.475979  [86400/209452]\n",
      "loss: 0.520637  [89600/209452]\n",
      "loss: 0.573131  [92800/209452]\n",
      "loss: 0.669167  [96000/209452]\n",
      "loss: 0.766603  [99200/209452]\n",
      "loss: 0.674918  [102400/209452]\n",
      "loss: 0.602257  [105600/209452]\n",
      "loss: 0.649201  [108800/209452]\n",
      "loss: 0.529697  [112000/209452]\n",
      "loss: 0.757796  [115200/209452]\n",
      "loss: 0.408509  [118400/209452]\n",
      "loss: 0.492916  [121600/209452]\n",
      "loss: 0.404321  [124800/209452]\n",
      "loss: 0.689246  [128000/209452]\n",
      "loss: 0.495730  [131200/209452]\n",
      "loss: 0.526486  [134400/209452]\n",
      "loss: 0.668630  [137600/209452]\n",
      "loss: 0.450036  [140800/209452]\n",
      "loss: 0.558829  [144000/209452]\n",
      "loss: 0.537496  [147200/209452]\n",
      "loss: 0.674029  [150400/209452]\n",
      "loss: 0.420691  [153600/209452]\n",
      "loss: 0.673793  [156800/209452]\n",
      "loss: 0.548535  [160000/209452]\n",
      "loss: 0.503219  [163200/209452]\n",
      "loss: 0.588021  [166400/209452]\n",
      "loss: 0.505865  [169600/209452]\n",
      "loss: 0.819045  [172800/209452]\n",
      "loss: 0.518619  [176000/209452]\n",
      "loss: 0.650023  [179200/209452]\n",
      "loss: 0.614185  [182400/209452]\n",
      "loss: 0.503225  [185600/209452]\n",
      "loss: 0.528434  [188800/209452]\n",
      "loss: 0.586082  [192000/209452]\n",
      "loss: 0.618940  [195200/209452]\n",
      "loss: 0.591456  [198400/209452]\n",
      "loss: 0.541330  [201600/209452]\n",
      "loss: 0.481375  [204800/209452]\n",
      "loss: 0.671828  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.550607 \n",
      "\n",
      "Epoch 7\n",
      "---------------------------\n",
      "loss: 0.387514  [ 3200/209452]\n",
      "loss: 0.631434  [ 6400/209452]\n",
      "loss: 0.673145  [ 9600/209452]\n",
      "loss: 0.415421  [12800/209452]\n",
      "loss: 0.389170  [16000/209452]\n",
      "loss: 0.478929  [19200/209452]\n",
      "loss: 0.383488  [22400/209452]\n",
      "loss: 0.475128  [25600/209452]\n",
      "loss: 0.423034  [28800/209452]\n",
      "loss: 0.779058  [32000/209452]\n",
      "loss: 0.474914  [35200/209452]\n",
      "loss: 0.538899  [38400/209452]\n",
      "loss: 0.434530  [41600/209452]\n",
      "loss: 0.469186  [44800/209452]\n",
      "loss: 0.567901  [48000/209452]\n",
      "loss: 0.719029  [51200/209452]\n",
      "loss: 0.472934  [54400/209452]\n",
      "loss: 0.463699  [57600/209452]\n",
      "loss: 0.646091  [60800/209452]\n",
      "loss: 0.443019  [64000/209452]\n",
      "loss: 0.591339  [67200/209452]\n",
      "loss: 0.603063  [70400/209452]\n",
      "loss: 0.449643  [73600/209452]\n",
      "loss: 0.705159  [76800/209452]\n",
      "loss: 0.697584  [80000/209452]\n",
      "loss: 0.595514  [83200/209452]\n",
      "loss: 0.635752  [86400/209452]\n",
      "loss: 0.389557  [89600/209452]\n",
      "loss: 0.454181  [92800/209452]\n",
      "loss: 0.429728  [96000/209452]\n",
      "loss: 0.652209  [99200/209452]\n",
      "loss: 0.559915  [102400/209452]\n",
      "loss: 0.742806  [105600/209452]\n",
      "loss: 0.490178  [108800/209452]\n",
      "loss: 0.558428  [112000/209452]\n",
      "loss: 0.687186  [115200/209452]\n",
      "loss: 0.448299  [118400/209452]\n",
      "loss: 0.316662  [121600/209452]\n",
      "loss: 0.358552  [124800/209452]\n",
      "loss: 0.558939  [128000/209452]\n",
      "loss: 0.653571  [131200/209452]\n",
      "loss: 0.498975  [134400/209452]\n",
      "loss: 0.606731  [137600/209452]\n",
      "loss: 0.500870  [140800/209452]\n",
      "loss: 0.567107  [144000/209452]\n",
      "loss: 0.447376  [147200/209452]\n",
      "loss: 0.501153  [150400/209452]\n",
      "loss: 0.529240  [153600/209452]\n",
      "loss: 0.499597  [156800/209452]\n",
      "loss: 0.405982  [160000/209452]\n",
      "loss: 0.564486  [163200/209452]\n",
      "loss: 0.356961  [166400/209452]\n",
      "loss: 0.387984  [169600/209452]\n",
      "loss: 0.513827  [172800/209452]\n",
      "loss: 0.597126  [176000/209452]\n",
      "loss: 0.313281  [179200/209452]\n",
      "loss: 0.494416  [182400/209452]\n",
      "loss: 0.442410  [185600/209452]\n",
      "loss: 0.392945  [188800/209452]\n",
      "loss: 0.643224  [192000/209452]\n",
      "loss: 0.706489  [195200/209452]\n",
      "loss: 0.398760  [198400/209452]\n",
      "loss: 0.594841  [201600/209452]\n",
      "loss: 0.496284  [204800/209452]\n",
      "loss: 0.495841  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.540233 \n",
      "\n",
      "Epoch 8\n",
      "---------------------------\n",
      "loss: 0.657283  [ 3200/209452]\n",
      "loss: 0.537785  [ 6400/209452]\n",
      "loss: 0.610570  [ 9600/209452]\n",
      "loss: 0.697168  [12800/209452]\n",
      "loss: 0.416074  [16000/209452]\n",
      "loss: 0.491781  [19200/209452]\n",
      "loss: 0.471454  [22400/209452]\n",
      "loss: 0.532771  [25600/209452]\n",
      "loss: 0.476469  [28800/209452]\n",
      "loss: 0.369254  [32000/209452]\n",
      "loss: 0.660303  [35200/209452]\n",
      "loss: 0.405913  [38400/209452]\n",
      "loss: 0.489316  [41600/209452]\n",
      "loss: 0.481336  [44800/209452]\n",
      "loss: 0.411974  [48000/209452]\n",
      "loss: 0.394037  [51200/209452]\n",
      "loss: 0.501599  [54400/209452]\n",
      "loss: 0.711264  [57600/209452]\n",
      "loss: 0.467258  [60800/209452]\n",
      "loss: 0.575543  [64000/209452]\n",
      "loss: 0.491331  [67200/209452]\n",
      "loss: 0.364235  [70400/209452]\n",
      "loss: 0.583524  [73600/209452]\n",
      "loss: 0.491339  [76800/209452]\n",
      "loss: 0.419694  [80000/209452]\n",
      "loss: 0.471839  [83200/209452]\n",
      "loss: 0.438904  [86400/209452]\n",
      "loss: 0.464537  [89600/209452]\n",
      "loss: 0.458701  [92800/209452]\n",
      "loss: 0.547350  [96000/209452]\n",
      "loss: 0.352822  [99200/209452]\n",
      "loss: 0.436765  [102400/209452]\n",
      "loss: 0.432055  [105600/209452]\n",
      "loss: 0.550945  [108800/209452]\n",
      "loss: 0.943395  [112000/209452]\n",
      "loss: 0.418565  [115200/209452]\n",
      "loss: 0.486576  [118400/209452]\n",
      "loss: 0.357232  [121600/209452]\n",
      "loss: 0.600525  [124800/209452]\n",
      "loss: 0.413066  [128000/209452]\n",
      "loss: 0.552688  [131200/209452]\n",
      "loss: 0.460847  [134400/209452]\n",
      "loss: 0.691874  [137600/209452]\n",
      "loss: 0.628487  [140800/209452]\n",
      "loss: 0.545466  [144000/209452]\n",
      "loss: 0.606767  [147200/209452]\n",
      "loss: 0.488003  [150400/209452]\n",
      "loss: 0.538559  [153600/209452]\n",
      "loss: 0.590776  [156800/209452]\n",
      "loss: 0.449609  [160000/209452]\n",
      "loss: 0.403551  [163200/209452]\n",
      "loss: 0.474734  [166400/209452]\n",
      "loss: 0.565826  [169600/209452]\n",
      "loss: 0.418786  [172800/209452]\n",
      "loss: 0.509706  [176000/209452]\n",
      "loss: 0.282034  [179200/209452]\n",
      "loss: 0.464727  [182400/209452]\n",
      "loss: 0.449681  [185600/209452]\n",
      "loss: 0.723151  [188800/209452]\n",
      "loss: 0.450150  [192000/209452]\n",
      "loss: 0.731565  [195200/209452]\n",
      "loss: 0.661096  [198400/209452]\n",
      "loss: 0.805888  [201600/209452]\n",
      "loss: 0.488165  [204800/209452]\n",
      "loss: 0.732918  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.530907 \n",
      "\n",
      "Epoch 9\n",
      "---------------------------\n",
      "loss: 0.538112  [ 3200/209452]\n",
      "loss: 0.529108  [ 6400/209452]\n",
      "loss: 0.484459  [ 9600/209452]\n",
      "loss: 0.405874  [12800/209452]\n",
      "loss: 0.550928  [16000/209452]\n",
      "loss: 0.415011  [19200/209452]\n",
      "loss: 0.437826  [22400/209452]\n",
      "loss: 0.463012  [25600/209452]\n",
      "loss: 0.462351  [28800/209452]\n",
      "loss: 0.549962  [32000/209452]\n",
      "loss: 0.512258  [35200/209452]\n",
      "loss: 0.763540  [38400/209452]\n",
      "loss: 0.569299  [41600/209452]\n",
      "loss: 0.503547  [44800/209452]\n",
      "loss: 0.597762  [48000/209452]\n",
      "loss: 0.688616  [51200/209452]\n",
      "loss: 0.434580  [54400/209452]\n",
      "loss: 0.492934  [57600/209452]\n",
      "loss: 0.384003  [60800/209452]\n",
      "loss: 0.600106  [64000/209452]\n",
      "loss: 0.532448  [67200/209452]\n",
      "loss: 0.427302  [70400/209452]\n",
      "loss: 0.404607  [73600/209452]\n",
      "loss: 0.631122  [76800/209452]\n",
      "loss: 0.643544  [80000/209452]\n",
      "loss: 0.434507  [83200/209452]\n",
      "loss: 0.585741  [86400/209452]\n",
      "loss: 0.574246  [89600/209452]\n",
      "loss: 0.436566  [92800/209452]\n",
      "loss: 0.543836  [96000/209452]\n",
      "loss: 0.457679  [99200/209452]\n",
      "loss: 0.458093  [102400/209452]\n",
      "loss: 0.447998  [105600/209452]\n",
      "loss: 0.528704  [108800/209452]\n",
      "loss: 0.452857  [112000/209452]\n",
      "loss: 0.658234  [115200/209452]\n",
      "loss: 0.564758  [118400/209452]\n",
      "loss: 0.496460  [121600/209452]\n",
      "loss: 0.436433  [124800/209452]\n",
      "loss: 0.376395  [128000/209452]\n",
      "loss: 0.801377  [131200/209452]\n",
      "loss: 0.473774  [134400/209452]\n",
      "loss: 0.565704  [137600/209452]\n",
      "loss: 0.829411  [140800/209452]\n",
      "loss: 0.435667  [144000/209452]\n",
      "loss: 0.527724  [147200/209452]\n",
      "loss: 0.418494  [150400/209452]\n",
      "loss: 0.344069  [153600/209452]\n",
      "loss: 0.536294  [156800/209452]\n",
      "loss: 0.456794  [160000/209452]\n",
      "loss: 0.574222  [163200/209452]\n",
      "loss: 0.495529  [166400/209452]\n",
      "loss: 0.386517  [169600/209452]\n",
      "loss: 0.532436  [172800/209452]\n",
      "loss: 0.481858  [176000/209452]\n",
      "loss: 0.440617  [179200/209452]\n",
      "loss: 0.428666  [182400/209452]\n",
      "loss: 0.633507  [185600/209452]\n",
      "loss: 0.431754  [188800/209452]\n",
      "loss: 0.479551  [192000/209452]\n",
      "loss: 0.566406  [195200/209452]\n",
      "loss: 0.461436  [198400/209452]\n",
      "loss: 0.608417  [201600/209452]\n",
      "loss: 0.570820  [204800/209452]\n",
      "loss: 0.656232  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.522276 \n",
      "\n",
      "Epoch 10\n",
      "---------------------------\n",
      "loss: 0.806176  [ 3200/209452]\n",
      "loss: 0.501090  [ 6400/209452]\n",
      "loss: 0.399701  [ 9600/209452]\n",
      "loss: 0.431109  [12800/209452]\n",
      "loss: 0.628242  [16000/209452]\n",
      "loss: 0.281091  [19200/209452]\n",
      "loss: 0.558398  [22400/209452]\n",
      "loss: 0.378884  [25600/209452]\n",
      "loss: 0.537763  [28800/209452]\n",
      "loss: 0.579413  [32000/209452]\n",
      "loss: 0.536257  [35200/209452]\n",
      "loss: 0.479256  [38400/209452]\n",
      "loss: 0.457234  [41600/209452]\n",
      "loss: 0.502306  [44800/209452]\n",
      "loss: 0.522196  [48000/209452]\n",
      "loss: 0.665225  [51200/209452]\n",
      "loss: 0.578383  [54400/209452]\n",
      "loss: 0.390620  [57600/209452]\n",
      "loss: 0.504472  [60800/209452]\n",
      "loss: 0.649490  [64000/209452]\n",
      "loss: 0.682535  [67200/209452]\n",
      "loss: 0.413757  [70400/209452]\n",
      "loss: 0.480868  [73600/209452]\n",
      "loss: 0.465017  [76800/209452]\n",
      "loss: 0.497086  [80000/209452]\n",
      "loss: 0.536547  [83200/209452]\n",
      "loss: 0.687507  [86400/209452]\n",
      "loss: 0.500709  [89600/209452]\n",
      "loss: 0.536698  [92800/209452]\n",
      "loss: 0.522493  [96000/209452]\n",
      "loss: 0.648088  [99200/209452]\n",
      "loss: 0.530431  [102400/209452]\n",
      "loss: 0.600989  [105600/209452]\n",
      "loss: 0.413763  [108800/209452]\n",
      "loss: 0.424571  [112000/209452]\n",
      "loss: 0.567299  [115200/209452]\n",
      "loss: 0.541468  [118400/209452]\n",
      "loss: 0.553011  [121600/209452]\n",
      "loss: 0.445072  [124800/209452]\n",
      "loss: 0.484700  [128000/209452]\n",
      "loss: 0.426840  [131200/209452]\n",
      "loss: 0.301672  [134400/209452]\n",
      "loss: 0.613443  [137600/209452]\n",
      "loss: 0.568301  [140800/209452]\n",
      "loss: 0.663791  [144000/209452]\n",
      "loss: 0.394114  [147200/209452]\n",
      "loss: 0.651253  [150400/209452]\n",
      "loss: 0.561117  [153600/209452]\n",
      "loss: 0.476626  [156800/209452]\n",
      "loss: 0.511331  [160000/209452]\n",
      "loss: 0.413656  [163200/209452]\n",
      "loss: 0.407248  [166400/209452]\n",
      "loss: 0.784965  [169600/209452]\n",
      "loss: 0.428960  [172800/209452]\n",
      "loss: 0.606771  [176000/209452]\n",
      "loss: 0.530271  [179200/209452]\n",
      "loss: 0.395736  [182400/209452]\n",
      "loss: 0.380053  [185600/209452]\n",
      "loss: 0.631499  [188800/209452]\n",
      "loss: 0.539223  [192000/209452]\n",
      "loss: 0.693698  [195200/209452]\n",
      "loss: 0.828114  [198400/209452]\n",
      "loss: 0.382601  [201600/209452]\n",
      "loss: 0.467748  [204800/209452]\n",
      "loss: 0.384731  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.514076 \n",
      "\n",
      "Epoch 11\n",
      "---------------------------\n",
      "loss: 0.547503  [ 3200/209452]\n",
      "loss: 0.475061  [ 6400/209452]\n",
      "loss: 0.507288  [ 9600/209452]\n",
      "loss: 0.589120  [12800/209452]\n",
      "loss: 0.566646  [16000/209452]\n",
      "loss: 0.742010  [19200/209452]\n",
      "loss: 0.684893  [22400/209452]\n",
      "loss: 0.255201  [25600/209452]\n",
      "loss: 0.584588  [28800/209452]\n",
      "loss: 0.553308  [32000/209452]\n",
      "loss: 0.636511  [35200/209452]\n",
      "loss: 0.565768  [38400/209452]\n",
      "loss: 0.408177  [41600/209452]\n",
      "loss: 0.702368  [44800/209452]\n",
      "loss: 0.492784  [48000/209452]\n",
      "loss: 0.496204  [51200/209452]\n",
      "loss: 0.536319  [54400/209452]\n",
      "loss: 0.403164  [57600/209452]\n",
      "loss: 0.507197  [60800/209452]\n",
      "loss: 0.497041  [64000/209452]\n",
      "loss: 0.595518  [67200/209452]\n",
      "loss: 0.591329  [70400/209452]\n",
      "loss: 0.335297  [73600/209452]\n",
      "loss: 0.371431  [76800/209452]\n",
      "loss: 0.454125  [80000/209452]\n",
      "loss: 0.426226  [83200/209452]\n",
      "loss: 0.516215  [86400/209452]\n",
      "loss: 0.645353  [89600/209452]\n",
      "loss: 0.760705  [92800/209452]\n",
      "loss: 0.629072  [96000/209452]\n",
      "loss: 0.672296  [99200/209452]\n",
      "loss: 0.473295  [102400/209452]\n",
      "loss: 0.428566  [105600/209452]\n",
      "loss: 0.437446  [108800/209452]\n",
      "loss: 0.374013  [112000/209452]\n",
      "loss: 0.521909  [115200/209452]\n",
      "loss: 0.314770  [118400/209452]\n",
      "loss: 0.662008  [121600/209452]\n",
      "loss: 0.538411  [124800/209452]\n",
      "loss: 0.757195  [128000/209452]\n",
      "loss: 0.406705  [131200/209452]\n",
      "loss: 0.553794  [134400/209452]\n",
      "loss: 0.386119  [137600/209452]\n",
      "loss: 0.532038  [140800/209452]\n",
      "loss: 0.586342  [144000/209452]\n",
      "loss: 0.463845  [147200/209452]\n",
      "loss: 0.601741  [150400/209452]\n",
      "loss: 0.448413  [153600/209452]\n",
      "loss: 0.357943  [156800/209452]\n",
      "loss: 0.383180  [160000/209452]\n",
      "loss: 0.464212  [163200/209452]\n",
      "loss: 0.364444  [166400/209452]\n",
      "loss: 0.431390  [169600/209452]\n",
      "loss: 0.680560  [172800/209452]\n",
      "loss: 0.573663  [176000/209452]\n",
      "loss: 0.577722  [179200/209452]\n",
      "loss: 0.506843  [182400/209452]\n",
      "loss: 0.396814  [185600/209452]\n",
      "loss: 0.470118  [188800/209452]\n",
      "loss: 0.648114  [192000/209452]\n",
      "loss: 0.466439  [195200/209452]\n",
      "loss: 0.520542  [198400/209452]\n",
      "loss: 0.359921  [201600/209452]\n",
      "loss: 0.510709  [204800/209452]\n",
      "loss: 0.417945  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.506385 \n",
      "\n",
      "Epoch 12\n",
      "---------------------------\n",
      "loss: 0.520968  [ 3200/209452]\n",
      "loss: 0.540746  [ 6400/209452]\n",
      "loss: 0.503596  [ 9600/209452]\n",
      "loss: 0.517839  [12800/209452]\n",
      "loss: 0.501194  [16000/209452]\n",
      "loss: 0.547683  [19200/209452]\n",
      "loss: 0.458735  [22400/209452]\n",
      "loss: 0.367601  [25600/209452]\n",
      "loss: 0.381915  [28800/209452]\n",
      "loss: 0.507149  [32000/209452]\n",
      "loss: 0.539544  [35200/209452]\n",
      "loss: 0.489494  [38400/209452]\n",
      "loss: 0.373395  [41600/209452]\n",
      "loss: 0.565978  [44800/209452]\n",
      "loss: 0.567535  [48000/209452]\n",
      "loss: 0.466798  [51200/209452]\n",
      "loss: 0.549553  [54400/209452]\n",
      "loss: 0.636653  [57600/209452]\n",
      "loss: 0.408749  [60800/209452]\n",
      "loss: 0.387984  [64000/209452]\n",
      "loss: 0.545041  [67200/209452]\n",
      "loss: 0.469951  [70400/209452]\n",
      "loss: 0.307673  [73600/209452]\n",
      "loss: 0.601057  [76800/209452]\n",
      "loss: 0.444002  [80000/209452]\n",
      "loss: 0.483435  [83200/209452]\n",
      "loss: 0.375648  [86400/209452]\n",
      "loss: 0.593526  [89600/209452]\n",
      "loss: 0.445229  [92800/209452]\n",
      "loss: 0.459545  [96000/209452]\n",
      "loss: 0.558761  [99200/209452]\n",
      "loss: 0.414233  [102400/209452]\n",
      "loss: 0.586250  [105600/209452]\n",
      "loss: 0.706812  [108800/209452]\n",
      "loss: 0.392939  [112000/209452]\n",
      "loss: 0.410777  [115200/209452]\n",
      "loss: 0.526171  [118400/209452]\n",
      "loss: 0.434082  [121600/209452]\n",
      "loss: 0.564809  [124800/209452]\n",
      "loss: 0.606335  [128000/209452]\n",
      "loss: 0.537725  [131200/209452]\n",
      "loss: 0.600442  [134400/209452]\n",
      "loss: 0.300127  [137600/209452]\n",
      "loss: 0.564990  [140800/209452]\n",
      "loss: 0.693513  [144000/209452]\n",
      "loss: 0.418134  [147200/209452]\n",
      "loss: 0.563873  [150400/209452]\n",
      "loss: 0.481099  [153600/209452]\n",
      "loss: 0.620545  [156800/209452]\n",
      "loss: 0.366293  [160000/209452]\n",
      "loss: 0.532488  [163200/209452]\n",
      "loss: 0.449329  [166400/209452]\n",
      "loss: 0.523735  [169600/209452]\n",
      "loss: 0.465429  [172800/209452]\n",
      "loss: 0.339916  [176000/209452]\n",
      "loss: 0.466569  [179200/209452]\n",
      "loss: 0.503372  [182400/209452]\n",
      "loss: 0.425687  [185600/209452]\n",
      "loss: 0.452773  [188800/209452]\n",
      "loss: 0.533015  [192000/209452]\n",
      "loss: 0.447323  [195200/209452]\n",
      "loss: 0.466570  [198400/209452]\n",
      "loss: 0.436041  [201600/209452]\n",
      "loss: 0.442710  [204800/209452]\n",
      "loss: 0.624280  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.499510 \n",
      "\n",
      "Epoch 13\n",
      "---------------------------\n",
      "loss: 0.357827  [ 3200/209452]\n",
      "loss: 0.424384  [ 6400/209452]\n",
      "loss: 0.515669  [ 9600/209452]\n",
      "loss: 0.648615  [12800/209452]\n",
      "loss: 0.430044  [16000/209452]\n",
      "loss: 0.510785  [19200/209452]\n",
      "loss: 0.454830  [22400/209452]\n",
      "loss: 0.638716  [25600/209452]\n",
      "loss: 0.578177  [28800/209452]\n",
      "loss: 0.478453  [32000/209452]\n",
      "loss: 0.435736  [35200/209452]\n",
      "loss: 0.568331  [38400/209452]\n",
      "loss: 0.433421  [41600/209452]\n",
      "loss: 0.442073  [44800/209452]\n",
      "loss: 0.617563  [48000/209452]\n",
      "loss: 0.401643  [51200/209452]\n",
      "loss: 0.533455  [54400/209452]\n",
      "loss: 0.350494  [57600/209452]\n",
      "loss: 0.435308  [60800/209452]\n",
      "loss: 0.552418  [64000/209452]\n",
      "loss: 0.632757  [67200/209452]\n",
      "loss: 0.679478  [70400/209452]\n",
      "loss: 0.406341  [73600/209452]\n",
      "loss: 0.306904  [76800/209452]\n",
      "loss: 0.344241  [80000/209452]\n",
      "loss: 0.364095  [83200/209452]\n",
      "loss: 0.420070  [86400/209452]\n",
      "loss: 0.355282  [89600/209452]\n",
      "loss: 0.567223  [92800/209452]\n",
      "loss: 0.481038  [96000/209452]\n",
      "loss: 0.404060  [99200/209452]\n",
      "loss: 0.429729  [102400/209452]\n",
      "loss: 0.446184  [105600/209452]\n",
      "loss: 0.523023  [108800/209452]\n",
      "loss: 0.488031  [112000/209452]\n",
      "loss: 0.514246  [115200/209452]\n",
      "loss: 0.510225  [118400/209452]\n",
      "loss: 0.457995  [121600/209452]\n",
      "loss: 0.397337  [124800/209452]\n",
      "loss: 0.523843  [128000/209452]\n",
      "loss: 0.418867  [131200/209452]\n",
      "loss: 0.527179  [134400/209452]\n",
      "loss: 0.385165  [137600/209452]\n",
      "loss: 0.405536  [140800/209452]\n",
      "loss: 0.383420  [144000/209452]\n",
      "loss: 0.375295  [147200/209452]\n",
      "loss: 0.371616  [150400/209452]\n",
      "loss: 0.504813  [153600/209452]\n",
      "loss: 0.569046  [156800/209452]\n",
      "loss: 0.343296  [160000/209452]\n",
      "loss: 0.462968  [163200/209452]\n",
      "loss: 0.366870  [166400/209452]\n",
      "loss: 0.552554  [169600/209452]\n",
      "loss: 0.568367  [172800/209452]\n",
      "loss: 0.455045  [176000/209452]\n",
      "loss: 0.399725  [179200/209452]\n",
      "loss: 0.449906  [182400/209452]\n",
      "loss: 0.417094  [185600/209452]\n",
      "loss: 0.532706  [188800/209452]\n",
      "loss: 0.489256  [192000/209452]\n",
      "loss: 0.435570  [195200/209452]\n",
      "loss: 0.450221  [198400/209452]\n",
      "loss: 0.626789  [201600/209452]\n",
      "loss: 0.419003  [204800/209452]\n",
      "loss: 0.614731  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.491342 \n",
      "\n",
      "Epoch 14\n",
      "---------------------------\n",
      "loss: 0.538985  [ 3200/209452]\n",
      "loss: 0.533790  [ 6400/209452]\n",
      "loss: 0.511131  [ 9600/209452]\n",
      "loss: 0.709548  [12800/209452]\n",
      "loss: 0.676589  [16000/209452]\n",
      "loss: 0.637469  [19200/209452]\n",
      "loss: 0.473335  [22400/209452]\n",
      "loss: 0.488681  [25600/209452]\n",
      "loss: 0.533240  [28800/209452]\n",
      "loss: 0.435916  [32000/209452]\n",
      "loss: 0.401409  [35200/209452]\n",
      "loss: 0.477335  [38400/209452]\n",
      "loss: 0.457665  [41600/209452]\n",
      "loss: 0.495087  [44800/209452]\n",
      "loss: 0.524547  [48000/209452]\n",
      "loss: 0.420621  [51200/209452]\n",
      "loss: 0.447693  [54400/209452]\n",
      "loss: 0.436360  [57600/209452]\n",
      "loss: 0.575358  [60800/209452]\n",
      "loss: 0.508188  [64000/209452]\n",
      "loss: 0.270804  [67200/209452]\n",
      "loss: 0.713948  [70400/209452]\n",
      "loss: 0.293114  [73600/209452]\n",
      "loss: 0.455878  [76800/209452]\n",
      "loss: 0.423968  [80000/209452]\n",
      "loss: 0.380342  [83200/209452]\n",
      "loss: 0.523422  [86400/209452]\n",
      "loss: 0.458594  [89600/209452]\n",
      "loss: 0.461199  [92800/209452]\n",
      "loss: 0.547276  [96000/209452]\n",
      "loss: 0.421277  [99200/209452]\n",
      "loss: 0.811621  [102400/209452]\n",
      "loss: 0.503935  [105600/209452]\n",
      "loss: 0.532629  [108800/209452]\n",
      "loss: 0.427722  [112000/209452]\n",
      "loss: 0.560710  [115200/209452]\n",
      "loss: 0.572515  [118400/209452]\n",
      "loss: 0.603621  [121600/209452]\n",
      "loss: 0.412652  [124800/209452]\n",
      "loss: 0.404971  [128000/209452]\n",
      "loss: 0.519360  [131200/209452]\n",
      "loss: 0.497694  [134400/209452]\n",
      "loss: 0.494195  [137600/209452]\n",
      "loss: 0.556849  [140800/209452]\n",
      "loss: 0.360748  [144000/209452]\n",
      "loss: 0.346340  [147200/209452]\n",
      "loss: 0.633090  [150400/209452]\n",
      "loss: 0.466134  [153600/209452]\n",
      "loss: 0.323128  [156800/209452]\n",
      "loss: 0.410456  [160000/209452]\n",
      "loss: 0.733688  [163200/209452]\n",
      "loss: 0.511745  [166400/209452]\n",
      "loss: 0.615337  [169600/209452]\n",
      "loss: 0.661630  [172800/209452]\n",
      "loss: 0.482427  [176000/209452]\n",
      "loss: 0.612674  [179200/209452]\n",
      "loss: 0.668247  [182400/209452]\n",
      "loss: 0.577537  [185600/209452]\n",
      "loss: 0.659508  [188800/209452]\n",
      "loss: 0.449021  [192000/209452]\n",
      "loss: 0.466439  [195200/209452]\n",
      "loss: 0.522216  [198400/209452]\n",
      "loss: 0.420943  [201600/209452]\n",
      "loss: 0.515091  [204800/209452]\n",
      "loss: 0.506716  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.484552 \n",
      "\n",
      "Epoch 15\n",
      "---------------------------\n",
      "loss: 0.603851  [ 3200/209452]\n",
      "loss: 0.443288  [ 6400/209452]\n",
      "loss: 0.532046  [ 9600/209452]\n",
      "loss: 0.284994  [12800/209452]\n",
      "loss: 0.445214  [16000/209452]\n",
      "loss: 0.549753  [19200/209452]\n",
      "loss: 0.540955  [22400/209452]\n",
      "loss: 0.408651  [25600/209452]\n",
      "loss: 0.521159  [28800/209452]\n",
      "loss: 0.446079  [32000/209452]\n",
      "loss: 0.511004  [35200/209452]\n",
      "loss: 0.528302  [38400/209452]\n",
      "loss: 0.575342  [41600/209452]\n",
      "loss: 0.600723  [44800/209452]\n",
      "loss: 0.546393  [48000/209452]\n",
      "loss: 0.261975  [51200/209452]\n",
      "loss: 0.514256  [54400/209452]\n",
      "loss: 0.560751  [57600/209452]\n",
      "loss: 0.556552  [60800/209452]\n",
      "loss: 0.771679  [64000/209452]\n",
      "loss: 0.449278  [67200/209452]\n",
      "loss: 0.364567  [70400/209452]\n",
      "loss: 0.373394  [73600/209452]\n",
      "loss: 0.586478  [76800/209452]\n",
      "loss: 0.476221  [80000/209452]\n",
      "loss: 0.425811  [83200/209452]\n",
      "loss: 0.404098  [86400/209452]\n",
      "loss: 0.279449  [89600/209452]\n",
      "loss: 0.466835  [92800/209452]\n",
      "loss: 0.439519  [96000/209452]\n",
      "loss: 0.387101  [99200/209452]\n",
      "loss: 0.476093  [102400/209452]\n",
      "loss: 0.555538  [105600/209452]\n",
      "loss: 0.511896  [108800/209452]\n",
      "loss: 0.369079  [112000/209452]\n",
      "loss: 0.536207  [115200/209452]\n",
      "loss: 0.422404  [118400/209452]\n",
      "loss: 0.625838  [121600/209452]\n",
      "loss: 0.309031  [124800/209452]\n",
      "loss: 0.335912  [128000/209452]\n",
      "loss: 0.477623  [131200/209452]\n",
      "loss: 0.333306  [134400/209452]\n",
      "loss: 0.435417  [137600/209452]\n",
      "loss: 0.363910  [140800/209452]\n",
      "loss: 0.477083  [144000/209452]\n",
      "loss: 0.477899  [147200/209452]\n",
      "loss: 0.435589  [150400/209452]\n",
      "loss: 0.576641  [153600/209452]\n",
      "loss: 0.369489  [156800/209452]\n",
      "loss: 0.410251  [160000/209452]\n",
      "loss: 0.365872  [163200/209452]\n",
      "loss: 0.394076  [166400/209452]\n",
      "loss: 0.477083  [169600/209452]\n",
      "loss: 0.472897  [172800/209452]\n",
      "loss: 0.519310  [176000/209452]\n",
      "loss: 0.511892  [179200/209452]\n",
      "loss: 0.438687  [182400/209452]\n",
      "loss: 0.566323  [185600/209452]\n",
      "loss: 0.425005  [188800/209452]\n",
      "loss: 0.344114  [192000/209452]\n",
      "loss: 0.291147  [195200/209452]\n",
      "loss: 0.333540  [198400/209452]\n",
      "loss: 0.295723  [201600/209452]\n",
      "loss: 0.663032  [204800/209452]\n",
      "loss: 0.502340  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.478377 \n",
      "\n",
      "Epoch 16\n",
      "---------------------------\n",
      "loss: 0.581962  [ 3200/209452]\n",
      "loss: 0.506189  [ 6400/209452]\n",
      "loss: 0.519196  [ 9600/209452]\n",
      "loss: 0.480680  [12800/209452]\n",
      "loss: 0.636396  [16000/209452]\n",
      "loss: 0.292144  [19200/209452]\n",
      "loss: 0.343125  [22400/209452]\n",
      "loss: 0.506138  [25600/209452]\n",
      "loss: 0.486517  [28800/209452]\n",
      "loss: 0.652405  [32000/209452]\n",
      "loss: 0.524067  [35200/209452]\n",
      "loss: 0.627892  [38400/209452]\n",
      "loss: 0.478593  [41600/209452]\n",
      "loss: 0.582522  [44800/209452]\n",
      "loss: 0.592088  [48000/209452]\n",
      "loss: 0.344973  [51200/209452]\n",
      "loss: 0.389518  [54400/209452]\n",
      "loss: 0.573024  [57600/209452]\n",
      "loss: 0.524914  [60800/209452]\n",
      "loss: 0.542446  [64000/209452]\n",
      "loss: 0.403513  [67200/209452]\n",
      "loss: 0.475881  [70400/209452]\n",
      "loss: 0.457469  [73600/209452]\n",
      "loss: 0.353215  [76800/209452]\n",
      "loss: 0.373901  [80000/209452]\n",
      "loss: 0.392273  [83200/209452]\n",
      "loss: 0.360470  [86400/209452]\n",
      "loss: 0.511418  [89600/209452]\n",
      "loss: 0.495993  [92800/209452]\n",
      "loss: 0.472988  [96000/209452]\n",
      "loss: 0.524588  [99200/209452]\n",
      "loss: 0.482959  [102400/209452]\n",
      "loss: 0.826399  [105600/209452]\n",
      "loss: 0.567074  [108800/209452]\n",
      "loss: 0.547692  [112000/209452]\n",
      "loss: 0.313583  [115200/209452]\n",
      "loss: 0.370401  [118400/209452]\n",
      "loss: 0.629981  [121600/209452]\n",
      "loss: 0.296831  [124800/209452]\n",
      "loss: 0.570669  [128000/209452]\n",
      "loss: 0.488293  [131200/209452]\n",
      "loss: 0.441472  [134400/209452]\n",
      "loss: 0.414878  [137600/209452]\n",
      "loss: 0.375282  [140800/209452]\n",
      "loss: 0.356150  [144000/209452]\n",
      "loss: 0.446276  [147200/209452]\n",
      "loss: 0.463958  [150400/209452]\n",
      "loss: 0.365435  [153600/209452]\n",
      "loss: 0.231960  [156800/209452]\n",
      "loss: 0.399364  [160000/209452]\n",
      "loss: 0.573569  [163200/209452]\n",
      "loss: 0.319687  [166400/209452]\n",
      "loss: 0.527170  [169600/209452]\n",
      "loss: 0.490331  [172800/209452]\n",
      "loss: 0.553069  [176000/209452]\n",
      "loss: 0.524081  [179200/209452]\n",
      "loss: 0.525798  [182400/209452]\n",
      "loss: 0.474754  [185600/209452]\n",
      "loss: 0.429655  [188800/209452]\n",
      "loss: 0.480155  [192000/209452]\n",
      "loss: 0.614661  [195200/209452]\n",
      "loss: 0.607093  [198400/209452]\n",
      "loss: 0.490781  [201600/209452]\n",
      "loss: 0.579406  [204800/209452]\n",
      "loss: 0.506055  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.470033 \n",
      "\n",
      "Epoch 17\n",
      "---------------------------\n",
      "loss: 0.451495  [ 3200/209452]\n",
      "loss: 0.357066  [ 6400/209452]\n",
      "loss: 0.380958  [ 9600/209452]\n",
      "loss: 0.520745  [12800/209452]\n",
      "loss: 0.585209  [16000/209452]\n",
      "loss: 0.356540  [19200/209452]\n",
      "loss: 0.433263  [22400/209452]\n",
      "loss: 0.447489  [25600/209452]\n",
      "loss: 0.299527  [28800/209452]\n",
      "loss: 0.360369  [32000/209452]\n",
      "loss: 0.376757  [35200/209452]\n",
      "loss: 0.576454  [38400/209452]\n",
      "loss: 0.419370  [41600/209452]\n",
      "loss: 0.671745  [44800/209452]\n",
      "loss: 0.584414  [48000/209452]\n",
      "loss: 0.485372  [51200/209452]\n",
      "loss: 0.598055  [54400/209452]\n",
      "loss: 0.472568  [57600/209452]\n",
      "loss: 0.457082  [60800/209452]\n",
      "loss: 0.334357  [64000/209452]\n",
      "loss: 0.460406  [67200/209452]\n",
      "loss: 0.551636  [70400/209452]\n",
      "loss: 0.484891  [73600/209452]\n",
      "loss: 0.541615  [76800/209452]\n",
      "loss: 0.595116  [80000/209452]\n",
      "loss: 0.442696  [83200/209452]\n",
      "loss: 0.404360  [86400/209452]\n",
      "loss: 0.524733  [89600/209452]\n",
      "loss: 0.494630  [92800/209452]\n",
      "loss: 0.571143  [96000/209452]\n",
      "loss: 0.551488  [99200/209452]\n",
      "loss: 0.415808  [102400/209452]\n",
      "loss: 0.421228  [105600/209452]\n",
      "loss: 0.310343  [108800/209452]\n",
      "loss: 0.404982  [112000/209452]\n",
      "loss: 0.430241  [115200/209452]\n",
      "loss: 0.514320  [118400/209452]\n",
      "loss: 0.613946  [121600/209452]\n",
      "loss: 0.401679  [124800/209452]\n",
      "loss: 0.506448  [128000/209452]\n",
      "loss: 0.483804  [131200/209452]\n",
      "loss: 0.437713  [134400/209452]\n",
      "loss: 0.552550  [137600/209452]\n",
      "loss: 0.447861  [140800/209452]\n",
      "loss: 0.315555  [144000/209452]\n",
      "loss: 0.395884  [147200/209452]\n",
      "loss: 0.454298  [150400/209452]\n",
      "loss: 0.413023  [153600/209452]\n",
      "loss: 0.318055  [156800/209452]\n",
      "loss: 0.523116  [160000/209452]\n",
      "loss: 0.313436  [163200/209452]\n",
      "loss: 0.742390  [166400/209452]\n",
      "loss: 0.272844  [169600/209452]\n",
      "loss: 0.670311  [172800/209452]\n",
      "loss: 0.493393  [176000/209452]\n",
      "loss: 0.338033  [179200/209452]\n",
      "loss: 0.414550  [182400/209452]\n",
      "loss: 0.513511  [185600/209452]\n",
      "loss: 0.615853  [188800/209452]\n",
      "loss: 0.642547  [192000/209452]\n",
      "loss: 0.538847  [195200/209452]\n",
      "loss: 0.487151  [198400/209452]\n",
      "loss: 0.435448  [201600/209452]\n",
      "loss: 0.282435  [204800/209452]\n",
      "loss: 0.332842  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.464794 \n",
      "\n",
      "Epoch 18\n",
      "---------------------------\n",
      "loss: 0.559145  [ 3200/209452]\n",
      "loss: 0.418327  [ 6400/209452]\n",
      "loss: 0.469939  [ 9600/209452]\n",
      "loss: 0.455349  [12800/209452]\n",
      "loss: 0.523413  [16000/209452]\n",
      "loss: 0.307713  [19200/209452]\n",
      "loss: 0.514431  [22400/209452]\n",
      "loss: 0.286826  [25600/209452]\n",
      "loss: 0.264849  [28800/209452]\n",
      "loss: 0.469814  [32000/209452]\n",
      "loss: 0.420364  [35200/209452]\n",
      "loss: 0.352506  [38400/209452]\n",
      "loss: 0.507425  [41600/209452]\n",
      "loss: 0.410826  [44800/209452]\n",
      "loss: 0.576342  [48000/209452]\n",
      "loss: 0.401687  [51200/209452]\n",
      "loss: 0.425904  [54400/209452]\n",
      "loss: 0.667919  [57600/209452]\n",
      "loss: 0.427365  [60800/209452]\n",
      "loss: 0.498768  [64000/209452]\n",
      "loss: 0.395397  [67200/209452]\n",
      "loss: 0.608437  [70400/209452]\n",
      "loss: 0.428520  [73600/209452]\n",
      "loss: 0.467809  [76800/209452]\n",
      "loss: 0.565466  [80000/209452]\n",
      "loss: 0.448946  [83200/209452]\n",
      "loss: 0.481041  [86400/209452]\n",
      "loss: 0.457234  [89600/209452]\n",
      "loss: 0.600657  [92800/209452]\n",
      "loss: 0.530707  [96000/209452]\n",
      "loss: 0.587927  [99200/209452]\n",
      "loss: 0.598357  [102400/209452]\n",
      "loss: 0.514001  [105600/209452]\n",
      "loss: 0.613324  [108800/209452]\n",
      "loss: 0.519518  [112000/209452]\n",
      "loss: 0.410050  [115200/209452]\n",
      "loss: 0.554805  [118400/209452]\n",
      "loss: 0.400344  [121600/209452]\n",
      "loss: 0.491412  [124800/209452]\n",
      "loss: 0.439772  [128000/209452]\n",
      "loss: 0.640849  [131200/209452]\n",
      "loss: 0.545539  [134400/209452]\n",
      "loss: 0.271661  [137600/209452]\n",
      "loss: 0.698155  [140800/209452]\n",
      "loss: 0.420139  [144000/209452]\n",
      "loss: 0.525022  [147200/209452]\n",
      "loss: 0.595855  [150400/209452]\n",
      "loss: 0.320335  [153600/209452]\n",
      "loss: 0.686380  [156800/209452]\n",
      "loss: 0.561755  [160000/209452]\n",
      "loss: 0.427614  [163200/209452]\n",
      "loss: 0.424242  [166400/209452]\n",
      "loss: 0.382074  [169600/209452]\n",
      "loss: 0.545520  [172800/209452]\n",
      "loss: 0.486284  [176000/209452]\n",
      "loss: 0.586916  [179200/209452]\n",
      "loss: 0.572180  [182400/209452]\n",
      "loss: 0.357761  [185600/209452]\n",
      "loss: 0.495492  [188800/209452]\n",
      "loss: 0.489902  [192000/209452]\n",
      "loss: 0.551875  [195200/209452]\n",
      "loss: 0.392720  [198400/209452]\n",
      "loss: 0.308535  [201600/209452]\n",
      "loss: 0.632444  [204800/209452]\n",
      "loss: 0.533259  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.456123 \n",
      "\n",
      "Epoch 19\n",
      "---------------------------\n",
      "loss: 0.316512  [ 3200/209452]\n",
      "loss: 0.373353  [ 6400/209452]\n",
      "loss: 0.497583  [ 9600/209452]\n",
      "loss: 0.384736  [12800/209452]\n",
      "loss: 0.564370  [16000/209452]\n",
      "loss: 0.315979  [19200/209452]\n",
      "loss: 0.458214  [22400/209452]\n",
      "loss: 0.493092  [25600/209452]\n",
      "loss: 0.568749  [28800/209452]\n",
      "loss: 0.686706  [32000/209452]\n",
      "loss: 0.462466  [35200/209452]\n",
      "loss: 0.347104  [38400/209452]\n",
      "loss: 0.466925  [41600/209452]\n",
      "loss: 0.378535  [44800/209452]\n",
      "loss: 0.372020  [48000/209452]\n",
      "loss: 0.343077  [51200/209452]\n",
      "loss: 0.546361  [54400/209452]\n",
      "loss: 0.346086  [57600/209452]\n",
      "loss: 0.478632  [60800/209452]\n",
      "loss: 0.497332  [64000/209452]\n",
      "loss: 0.355445  [67200/209452]\n",
      "loss: 0.433170  [70400/209452]\n",
      "loss: 0.503827  [73600/209452]\n",
      "loss: 0.558686  [76800/209452]\n",
      "loss: 0.365505  [80000/209452]\n",
      "loss: 0.394494  [83200/209452]\n",
      "loss: 0.349828  [86400/209452]\n",
      "loss: 0.529632  [89600/209452]\n",
      "loss: 0.369603  [92800/209452]\n",
      "loss: 0.563071  [96000/209452]\n",
      "loss: 0.317805  [99200/209452]\n",
      "loss: 0.360931  [102400/209452]\n",
      "loss: 0.572424  [105600/209452]\n",
      "loss: 0.555515  [108800/209452]\n",
      "loss: 0.457182  [112000/209452]\n",
      "loss: 0.586822  [115200/209452]\n",
      "loss: 0.298484  [118400/209452]\n",
      "loss: 0.620800  [121600/209452]\n",
      "loss: 0.590360  [124800/209452]\n",
      "loss: 0.498680  [128000/209452]\n",
      "loss: 0.411604  [131200/209452]\n",
      "loss: 0.322977  [134400/209452]\n",
      "loss: 0.442456  [137600/209452]\n",
      "loss: 0.495946  [140800/209452]\n",
      "loss: 0.648447  [144000/209452]\n",
      "loss: 0.467192  [147200/209452]\n",
      "loss: 0.422961  [150400/209452]\n",
      "loss: 0.470923  [153600/209452]\n",
      "loss: 0.347055  [156800/209452]\n",
      "loss: 0.519200  [160000/209452]\n",
      "loss: 0.445838  [163200/209452]\n",
      "loss: 0.489051  [166400/209452]\n",
      "loss: 0.451762  [169600/209452]\n",
      "loss: 0.373475  [172800/209452]\n",
      "loss: 0.440111  [176000/209452]\n",
      "loss: 0.279225  [179200/209452]\n",
      "loss: 0.422425  [182400/209452]\n",
      "loss: 0.461835  [185600/209452]\n",
      "loss: 0.401892  [188800/209452]\n",
      "loss: 0.501728  [192000/209452]\n",
      "loss: 0.477542  [195200/209452]\n",
      "loss: 0.418751  [198400/209452]\n",
      "loss: 0.428034  [201600/209452]\n",
      "loss: 0.540181  [204800/209452]\n",
      "loss: 0.364525  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.447353 \n",
      "\n",
      "Epoch 20\n",
      "---------------------------\n",
      "loss: 0.502244  [ 3200/209452]\n",
      "loss: 0.411495  [ 6400/209452]\n",
      "loss: 0.483435  [ 9600/209452]\n",
      "loss: 0.521350  [12800/209452]\n",
      "loss: 0.243978  [16000/209452]\n",
      "loss: 0.297468  [19200/209452]\n",
      "loss: 0.503570  [22400/209452]\n",
      "loss: 0.401962  [25600/209452]\n",
      "loss: 0.477140  [28800/209452]\n",
      "loss: 0.661881  [32000/209452]\n",
      "loss: 0.365395  [35200/209452]\n",
      "loss: 0.327359  [38400/209452]\n",
      "loss: 0.440017  [41600/209452]\n",
      "loss: 0.389775  [44800/209452]\n",
      "loss: 0.739373  [48000/209452]\n",
      "loss: 0.415222  [51200/209452]\n",
      "loss: 0.368513  [54400/209452]\n",
      "loss: 0.522338  [57600/209452]\n",
      "loss: 0.490546  [60800/209452]\n",
      "loss: 0.339060  [64000/209452]\n",
      "loss: 0.576146  [67200/209452]\n",
      "loss: 0.394822  [70400/209452]\n",
      "loss: 0.381167  [73600/209452]\n",
      "loss: 0.407925  [76800/209452]\n",
      "loss: 0.328182  [80000/209452]\n",
      "loss: 0.287952  [83200/209452]\n",
      "loss: 0.604778  [86400/209452]\n",
      "loss: 0.471881  [89600/209452]\n",
      "loss: 0.477623  [92800/209452]\n",
      "loss: 0.474767  [96000/209452]\n",
      "loss: 0.464862  [99200/209452]\n",
      "loss: 0.551073  [102400/209452]\n",
      "loss: 0.398786  [105600/209452]\n",
      "loss: 0.466303  [108800/209452]\n",
      "loss: 0.313365  [112000/209452]\n",
      "loss: 0.461245  [115200/209452]\n",
      "loss: 0.477675  [118400/209452]\n",
      "loss: 0.515479  [121600/209452]\n",
      "loss: 0.485272  [124800/209452]\n",
      "loss: 0.590843  [128000/209452]\n",
      "loss: 0.328957  [131200/209452]\n",
      "loss: 0.475598  [134400/209452]\n",
      "loss: 0.477674  [137600/209452]\n",
      "loss: 0.335923  [140800/209452]\n",
      "loss: 0.568246  [144000/209452]\n",
      "loss: 0.324654  [147200/209452]\n",
      "loss: 0.400372  [150400/209452]\n",
      "loss: 0.454965  [153600/209452]\n",
      "loss: 0.418237  [156800/209452]\n",
      "loss: 0.549682  [160000/209452]\n",
      "loss: 0.430674  [163200/209452]\n",
      "loss: 0.335263  [166400/209452]\n",
      "loss: 0.352719  [169600/209452]\n",
      "loss: 0.353480  [172800/209452]\n",
      "loss: 0.428473  [176000/209452]\n",
      "loss: 0.330349  [179200/209452]\n",
      "loss: 0.301249  [182400/209452]\n",
      "loss: 0.592835  [185600/209452]\n",
      "loss: 0.387344  [188800/209452]\n",
      "loss: 0.484749  [192000/209452]\n",
      "loss: 0.375134  [195200/209452]\n",
      "loss: 0.363514  [198400/209452]\n",
      "loss: 0.491453  [201600/209452]\n",
      "loss: 0.452707  [204800/209452]\n",
      "loss: 0.444638  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.439568 \n",
      "\n",
      "Epoch 21\n",
      "---------------------------\n",
      "loss: 0.711560  [ 3200/209452]\n",
      "loss: 0.457955  [ 6400/209452]\n",
      "loss: 0.459448  [ 9600/209452]\n",
      "loss: 0.282572  [12800/209452]\n",
      "loss: 0.672627  [16000/209452]\n",
      "loss: 0.458548  [19200/209452]\n",
      "loss: 0.304702  [22400/209452]\n",
      "loss: 0.481691  [25600/209452]\n",
      "loss: 0.525152  [28800/209452]\n",
      "loss: 0.515255  [32000/209452]\n",
      "loss: 0.554748  [35200/209452]\n",
      "loss: 0.347068  [38400/209452]\n",
      "loss: 0.461222  [41600/209452]\n",
      "loss: 0.283036  [44800/209452]\n",
      "loss: 0.492026  [48000/209452]\n",
      "loss: 0.308219  [51200/209452]\n",
      "loss: 0.392609  [54400/209452]\n",
      "loss: 0.428325  [57600/209452]\n",
      "loss: 0.413616  [60800/209452]\n",
      "loss: 0.539568  [64000/209452]\n",
      "loss: 0.590403  [67200/209452]\n",
      "loss: 0.371023  [70400/209452]\n",
      "loss: 0.254129  [73600/209452]\n",
      "loss: 0.366193  [76800/209452]\n",
      "loss: 0.333363  [80000/209452]\n",
      "loss: 0.377164  [83200/209452]\n",
      "loss: 0.597202  [86400/209452]\n",
      "loss: 0.408303  [89600/209452]\n",
      "loss: 0.293264  [92800/209452]\n",
      "loss: 0.511412  [96000/209452]\n",
      "loss: 0.529856  [99200/209452]\n",
      "loss: 0.346472  [102400/209452]\n",
      "loss: 0.342913  [105600/209452]\n",
      "loss: 0.415248  [108800/209452]\n",
      "loss: 0.316923  [112000/209452]\n",
      "loss: 0.466234  [115200/209452]\n",
      "loss: 0.275211  [118400/209452]\n",
      "loss: 0.412885  [121600/209452]\n",
      "loss: 0.473782  [124800/209452]\n",
      "loss: 0.372105  [128000/209452]\n",
      "loss: 0.462282  [131200/209452]\n",
      "loss: 0.557283  [134400/209452]\n",
      "loss: 0.371963  [137600/209452]\n",
      "loss: 0.401660  [140800/209452]\n",
      "loss: 0.425563  [144000/209452]\n",
      "loss: 0.448050  [147200/209452]\n",
      "loss: 0.309528  [150400/209452]\n",
      "loss: 0.487941  [153600/209452]\n",
      "loss: 0.446174  [156800/209452]\n",
      "loss: 0.513596  [160000/209452]\n",
      "loss: 0.472491  [163200/209452]\n",
      "loss: 0.457433  [166400/209452]\n",
      "loss: 0.245802  [169600/209452]\n",
      "loss: 0.404726  [172800/209452]\n",
      "loss: 0.477129  [176000/209452]\n",
      "loss: 0.560097  [179200/209452]\n",
      "loss: 0.471024  [182400/209452]\n",
      "loss: 0.450890  [185600/209452]\n",
      "loss: 0.431972  [188800/209452]\n",
      "loss: 0.302632  [192000/209452]\n",
      "loss: 0.396501  [195200/209452]\n",
      "loss: 0.357132  [198400/209452]\n",
      "loss: 0.366763  [201600/209452]\n",
      "loss: 0.486771  [204800/209452]\n",
      "loss: 0.432017  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.437318 \n",
      "\n",
      "Epoch 22\n",
      "---------------------------\n",
      "loss: 0.458457  [ 3200/209452]\n",
      "loss: 0.386230  [ 6400/209452]\n",
      "loss: 0.258573  [ 9600/209452]\n",
      "loss: 0.342627  [12800/209452]\n",
      "loss: 0.426739  [16000/209452]\n",
      "loss: 0.334987  [19200/209452]\n",
      "loss: 0.690843  [22400/209452]\n",
      "loss: 0.361962  [25600/209452]\n",
      "loss: 0.530697  [28800/209452]\n",
      "loss: 0.693796  [32000/209452]\n",
      "loss: 0.512207  [35200/209452]\n",
      "loss: 0.394750  [38400/209452]\n",
      "loss: 0.482145  [41600/209452]\n",
      "loss: 0.349134  [44800/209452]\n",
      "loss: 0.411050  [48000/209452]\n",
      "loss: 0.477828  [51200/209452]\n",
      "loss: 0.511803  [54400/209452]\n",
      "loss: 0.512312  [57600/209452]\n",
      "loss: 0.370447  [60800/209452]\n",
      "loss: 0.588919  [64000/209452]\n",
      "loss: 0.537920  [67200/209452]\n",
      "loss: 0.303125  [70400/209452]\n",
      "loss: 0.444558  [73600/209452]\n",
      "loss: 0.512253  [76800/209452]\n",
      "loss: 0.532788  [80000/209452]\n",
      "loss: 0.432151  [83200/209452]\n",
      "loss: 0.276104  [86400/209452]\n",
      "loss: 0.294099  [89600/209452]\n",
      "loss: 0.398119  [92800/209452]\n",
      "loss: 0.324139  [96000/209452]\n",
      "loss: 0.569791  [99200/209452]\n",
      "loss: 0.421357  [102400/209452]\n",
      "loss: 0.346678  [105600/209452]\n",
      "loss: 0.559612  [108800/209452]\n",
      "loss: 0.608069  [112000/209452]\n",
      "loss: 0.308624  [115200/209452]\n",
      "loss: 0.352885  [118400/209452]\n",
      "loss: 0.305348  [121600/209452]\n",
      "loss: 0.353774  [124800/209452]\n",
      "loss: 0.437249  [128000/209452]\n",
      "loss: 0.427798  [131200/209452]\n",
      "loss: 0.461663  [134400/209452]\n",
      "loss: 0.442606  [137600/209452]\n",
      "loss: 0.341790  [140800/209452]\n",
      "loss: 0.408400  [144000/209452]\n",
      "loss: 0.496652  [147200/209452]\n",
      "loss: 0.491574  [150400/209452]\n",
      "loss: 0.539360  [153600/209452]\n",
      "loss: 0.470264  [156800/209452]\n",
      "loss: 0.391445  [160000/209452]\n",
      "loss: 0.451411  [163200/209452]\n",
      "loss: 0.375810  [166400/209452]\n",
      "loss: 0.525564  [169600/209452]\n",
      "loss: 0.298588  [172800/209452]\n",
      "loss: 0.637135  [176000/209452]\n",
      "loss: 0.388907  [179200/209452]\n",
      "loss: 0.327697  [182400/209452]\n",
      "loss: 0.442459  [185600/209452]\n",
      "loss: 0.515253  [188800/209452]\n",
      "loss: 0.368732  [192000/209452]\n",
      "loss: 0.552067  [195200/209452]\n",
      "loss: 0.419650  [198400/209452]\n",
      "loss: 0.361639  [201600/209452]\n",
      "loss: 0.385623  [204800/209452]\n",
      "loss: 0.534426  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.425149 \n",
      "\n",
      "Epoch 23\n",
      "---------------------------\n",
      "loss: 0.461788  [ 3200/209452]\n",
      "loss: 0.450782  [ 6400/209452]\n",
      "loss: 0.428663  [ 9600/209452]\n",
      "loss: 0.668613  [12800/209452]\n",
      "loss: 0.397934  [16000/209452]\n",
      "loss: 0.366596  [19200/209452]\n",
      "loss: 0.427171  [22400/209452]\n",
      "loss: 0.409766  [25600/209452]\n",
      "loss: 0.478621  [28800/209452]\n",
      "loss: 0.312393  [32000/209452]\n",
      "loss: 0.317324  [35200/209452]\n",
      "loss: 0.607068  [38400/209452]\n",
      "loss: 0.512308  [41600/209452]\n",
      "loss: 0.462352  [44800/209452]\n",
      "loss: 0.387560  [48000/209452]\n",
      "loss: 0.267932  [51200/209452]\n",
      "loss: 0.330224  [54400/209452]\n",
      "loss: 0.336460  [57600/209452]\n",
      "loss: 0.503481  [60800/209452]\n",
      "loss: 0.302100  [64000/209452]\n",
      "loss: 0.476537  [67200/209452]\n",
      "loss: 0.375478  [70400/209452]\n",
      "loss: 0.379460  [73600/209452]\n",
      "loss: 0.343941  [76800/209452]\n",
      "loss: 0.452458  [80000/209452]\n",
      "loss: 0.663036  [83200/209452]\n",
      "loss: 0.442552  [86400/209452]\n",
      "loss: 0.388646  [89600/209452]\n",
      "loss: 0.391379  [92800/209452]\n",
      "loss: 0.433450  [96000/209452]\n",
      "loss: 0.342247  [99200/209452]\n",
      "loss: 0.267584  [102400/209452]\n",
      "loss: 0.427319  [105600/209452]\n",
      "loss: 0.410795  [108800/209452]\n",
      "loss: 0.322832  [112000/209452]\n",
      "loss: 0.499276  [115200/209452]\n",
      "loss: 0.445439  [118400/209452]\n",
      "loss: 0.472477  [121600/209452]\n",
      "loss: 0.374475  [124800/209452]\n",
      "loss: 0.311453  [128000/209452]\n",
      "loss: 0.395768  [131200/209452]\n",
      "loss: 0.356186  [134400/209452]\n",
      "loss: 0.403617  [137600/209452]\n",
      "loss: 0.307693  [140800/209452]\n",
      "loss: 0.335513  [144000/209452]\n",
      "loss: 0.402977  [147200/209452]\n",
      "loss: 0.322905  [150400/209452]\n",
      "loss: 0.340524  [153600/209452]\n",
      "loss: 0.382009  [156800/209452]\n",
      "loss: 0.519974  [160000/209452]\n",
      "loss: 0.448806  [163200/209452]\n",
      "loss: 0.442584  [166400/209452]\n",
      "loss: 0.355128  [169600/209452]\n",
      "loss: 0.346052  [172800/209452]\n",
      "loss: 0.608285  [176000/209452]\n",
      "loss: 0.404602  [179200/209452]\n",
      "loss: 0.393729  [182400/209452]\n",
      "loss: 0.361405  [185600/209452]\n",
      "loss: 0.310987  [188800/209452]\n",
      "loss: 0.415641  [192000/209452]\n",
      "loss: 0.532678  [195200/209452]\n",
      "loss: 0.589066  [198400/209452]\n",
      "loss: 0.358583  [201600/209452]\n",
      "loss: 0.409325  [204800/209452]\n",
      "loss: 0.329510  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.415610 \n",
      "\n",
      "Epoch 24\n",
      "---------------------------\n",
      "loss: 0.434520  [ 3200/209452]\n",
      "loss: 0.446631  [ 6400/209452]\n",
      "loss: 0.365039  [ 9600/209452]\n",
      "loss: 0.391961  [12800/209452]\n",
      "loss: 0.385457  [16000/209452]\n",
      "loss: 0.447136  [19200/209452]\n",
      "loss: 0.418513  [22400/209452]\n",
      "loss: 0.332249  [25600/209452]\n",
      "loss: 0.420601  [28800/209452]\n",
      "loss: 0.356367  [32000/209452]\n",
      "loss: 0.361523  [35200/209452]\n",
      "loss: 0.279463  [38400/209452]\n",
      "loss: 0.372837  [41600/209452]\n",
      "loss: 0.340293  [44800/209452]\n",
      "loss: 0.731185  [48000/209452]\n",
      "loss: 0.259813  [51200/209452]\n",
      "loss: 0.378652  [54400/209452]\n",
      "loss: 0.340991  [57600/209452]\n",
      "loss: 0.292715  [60800/209452]\n",
      "loss: 0.426423  [64000/209452]\n",
      "loss: 0.452393  [67200/209452]\n",
      "loss: 0.233611  [70400/209452]\n",
      "loss: 0.458892  [73600/209452]\n",
      "loss: 0.414611  [76800/209452]\n",
      "loss: 0.334548  [80000/209452]\n",
      "loss: 0.658223  [83200/209452]\n",
      "loss: 0.342954  [86400/209452]\n",
      "loss: 0.383751  [89600/209452]\n",
      "loss: 0.227266  [92800/209452]\n",
      "loss: 0.364715  [96000/209452]\n",
      "loss: 0.487814  [99200/209452]\n",
      "loss: 0.373590  [102400/209452]\n",
      "loss: 0.433473  [105600/209452]\n",
      "loss: 0.354081  [108800/209452]\n",
      "loss: 0.593053  [112000/209452]\n",
      "loss: 0.343332  [115200/209452]\n",
      "loss: 0.538045  [118400/209452]\n",
      "loss: 0.524120  [121600/209452]\n",
      "loss: 0.395454  [124800/209452]\n",
      "loss: 0.321736  [128000/209452]\n",
      "loss: 0.376525  [131200/209452]\n",
      "loss: 0.327348  [134400/209452]\n",
      "loss: 0.315866  [137600/209452]\n",
      "loss: 0.319733  [140800/209452]\n",
      "loss: 0.342890  [144000/209452]\n",
      "loss: 0.380620  [147200/209452]\n",
      "loss: 0.316371  [150400/209452]\n",
      "loss: 0.289406  [153600/209452]\n",
      "loss: 0.355734  [156800/209452]\n",
      "loss: 0.351894  [160000/209452]\n",
      "loss: 0.448076  [163200/209452]\n",
      "loss: 0.391905  [166400/209452]\n",
      "loss: 0.632636  [169600/209452]\n",
      "loss: 0.277132  [172800/209452]\n",
      "loss: 0.500496  [176000/209452]\n",
      "loss: 0.462920  [179200/209452]\n",
      "loss: 0.429325  [182400/209452]\n",
      "loss: 0.617049  [185600/209452]\n",
      "loss: 0.413432  [188800/209452]\n",
      "loss: 0.398916  [192000/209452]\n",
      "loss: 0.439385  [195200/209452]\n",
      "loss: 0.394096  [198400/209452]\n",
      "loss: 0.358066  [201600/209452]\n",
      "loss: 0.315220  [204800/209452]\n",
      "loss: 0.675503  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.407822 \n",
      "\n",
      "Epoch 25\n",
      "---------------------------\n",
      "loss: 0.474154  [ 3200/209452]\n",
      "loss: 0.492485  [ 6400/209452]\n",
      "loss: 0.568789  [ 9600/209452]\n",
      "loss: 0.426775  [12800/209452]\n",
      "loss: 0.433986  [16000/209452]\n",
      "loss: 0.286952  [19200/209452]\n",
      "loss: 0.370001  [22400/209452]\n",
      "loss: 0.449298  [25600/209452]\n",
      "loss: 0.527897  [28800/209452]\n",
      "loss: 0.507259  [32000/209452]\n",
      "loss: 0.554332  [35200/209452]\n",
      "loss: 0.384094  [38400/209452]\n",
      "loss: 0.525988  [41600/209452]\n",
      "loss: 0.453190  [44800/209452]\n",
      "loss: 0.524725  [48000/209452]\n",
      "loss: 0.449640  [51200/209452]\n",
      "loss: 0.392582  [54400/209452]\n",
      "loss: 0.299684  [57600/209452]\n",
      "loss: 0.337879  [60800/209452]\n",
      "loss: 0.360913  [64000/209452]\n",
      "loss: 0.440781  [67200/209452]\n",
      "loss: 0.232478  [70400/209452]\n",
      "loss: 0.390087  [73600/209452]\n",
      "loss: 0.319302  [76800/209452]\n",
      "loss: 0.653269  [80000/209452]\n",
      "loss: 0.360966  [83200/209452]\n",
      "loss: 0.433505  [86400/209452]\n",
      "loss: 0.361617  [89600/209452]\n",
      "loss: 0.233873  [92800/209452]\n",
      "loss: 0.385591  [96000/209452]\n",
      "loss: 0.424708  [99200/209452]\n",
      "loss: 0.444054  [102400/209452]\n",
      "loss: 0.499403  [105600/209452]\n",
      "loss: 0.375839  [108800/209452]\n",
      "loss: 0.424753  [112000/209452]\n",
      "loss: 0.314462  [115200/209452]\n",
      "loss: 0.293650  [118400/209452]\n",
      "loss: 0.408657  [121600/209452]\n",
      "loss: 0.416352  [124800/209452]\n",
      "loss: 0.456596  [128000/209452]\n",
      "loss: 0.483265  [131200/209452]\n",
      "loss: 0.306561  [134400/209452]\n",
      "loss: 0.422162  [137600/209452]\n",
      "loss: 0.271360  [140800/209452]\n",
      "loss: 0.496390  [144000/209452]\n",
      "loss: 0.290826  [147200/209452]\n",
      "loss: 0.671326  [150400/209452]\n",
      "loss: 0.357754  [153600/209452]\n",
      "loss: 0.343414  [156800/209452]\n",
      "loss: 0.403039  [160000/209452]\n",
      "loss: 0.397546  [163200/209452]\n",
      "loss: 0.344052  [166400/209452]\n",
      "loss: 0.336037  [169600/209452]\n",
      "loss: 0.291454  [172800/209452]\n",
      "loss: 0.497361  [176000/209452]\n",
      "loss: 0.305292  [179200/209452]\n",
      "loss: 0.419883  [182400/209452]\n",
      "loss: 0.483287  [185600/209452]\n",
      "loss: 0.318961  [188800/209452]\n",
      "loss: 0.452300  [192000/209452]\n",
      "loss: 0.558688  [195200/209452]\n",
      "loss: 0.376069  [198400/209452]\n",
      "loss: 0.334450  [201600/209452]\n",
      "loss: 0.382929  [204800/209452]\n",
      "loss: 0.523714  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.402089 \n",
      "\n",
      "Epoch 26\n",
      "---------------------------\n",
      "loss: 0.454107  [ 3200/209452]\n",
      "loss: 0.571134  [ 6400/209452]\n",
      "loss: 0.376482  [ 9600/209452]\n",
      "loss: 0.351796  [12800/209452]\n",
      "loss: 0.352882  [16000/209452]\n",
      "loss: 0.456905  [19200/209452]\n",
      "loss: 0.283514  [22400/209452]\n",
      "loss: 0.511144  [25600/209452]\n",
      "loss: 0.325958  [28800/209452]\n",
      "loss: 0.320626  [32000/209452]\n",
      "loss: 0.432061  [35200/209452]\n",
      "loss: 0.422636  [38400/209452]\n",
      "loss: 0.359678  [41600/209452]\n",
      "loss: 0.255255  [44800/209452]\n",
      "loss: 0.333779  [48000/209452]\n",
      "loss: 0.191969  [51200/209452]\n",
      "loss: 0.348617  [54400/209452]\n",
      "loss: 0.401936  [57600/209452]\n",
      "loss: 0.448023  [60800/209452]\n",
      "loss: 0.303992  [64000/209452]\n",
      "loss: 0.391619  [67200/209452]\n",
      "loss: 0.393262  [70400/209452]\n",
      "loss: 0.361682  [73600/209452]\n",
      "loss: 0.363218  [76800/209452]\n",
      "loss: 0.326337  [80000/209452]\n",
      "loss: 0.303351  [83200/209452]\n",
      "loss: 0.467595  [86400/209452]\n",
      "loss: 0.387851  [89600/209452]\n",
      "loss: 0.399680  [92800/209452]\n",
      "loss: 0.328890  [96000/209452]\n",
      "loss: 0.455819  [99200/209452]\n",
      "loss: 0.276544  [102400/209452]\n",
      "loss: 0.412063  [105600/209452]\n",
      "loss: 0.383621  [108800/209452]\n",
      "loss: 0.295008  [112000/209452]\n",
      "loss: 0.381778  [115200/209452]\n",
      "loss: 0.237506  [118400/209452]\n",
      "loss: 0.395066  [121600/209452]\n",
      "loss: 0.600796  [124800/209452]\n",
      "loss: 0.390208  [128000/209452]\n",
      "loss: 0.371193  [131200/209452]\n",
      "loss: 0.346007  [134400/209452]\n",
      "loss: 0.437518  [137600/209452]\n",
      "loss: 0.310970  [140800/209452]\n",
      "loss: 0.350810  [144000/209452]\n",
      "loss: 0.471793  [147200/209452]\n",
      "loss: 0.392015  [150400/209452]\n",
      "loss: 0.259598  [153600/209452]\n",
      "loss: 0.407969  [156800/209452]\n",
      "loss: 0.382453  [160000/209452]\n",
      "loss: 0.390243  [163200/209452]\n",
      "loss: 0.250936  [166400/209452]\n",
      "loss: 0.393839  [169600/209452]\n",
      "loss: 0.395177  [172800/209452]\n",
      "loss: 0.248512  [176000/209452]\n",
      "loss: 0.339480  [179200/209452]\n",
      "loss: 0.555427  [182400/209452]\n",
      "loss: 0.479232  [185600/209452]\n",
      "loss: 0.220389  [188800/209452]\n",
      "loss: 0.443230  [192000/209452]\n",
      "loss: 0.509148  [195200/209452]\n",
      "loss: 0.409005  [198400/209452]\n",
      "loss: 0.440177  [201600/209452]\n",
      "loss: 0.340019  [204800/209452]\n",
      "loss: 0.318584  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.392379 \n",
      "\n",
      "Epoch 27\n",
      "---------------------------\n",
      "loss: 0.403913  [ 3200/209452]\n",
      "loss: 0.351178  [ 6400/209452]\n",
      "loss: 0.380075  [ 9600/209452]\n",
      "loss: 0.285439  [12800/209452]\n",
      "loss: 0.366040  [16000/209452]\n",
      "loss: 0.384404  [19200/209452]\n",
      "loss: 0.304007  [22400/209452]\n",
      "loss: 0.391133  [25600/209452]\n",
      "loss: 0.529325  [28800/209452]\n",
      "loss: 0.566627  [32000/209452]\n",
      "loss: 0.532311  [35200/209452]\n",
      "loss: 0.401879  [38400/209452]\n",
      "loss: 0.380054  [41600/209452]\n",
      "loss: 0.331866  [44800/209452]\n",
      "loss: 0.297278  [48000/209452]\n",
      "loss: 0.457859  [51200/209452]\n",
      "loss: 0.505197  [54400/209452]\n",
      "loss: 0.281067  [57600/209452]\n",
      "loss: 0.209673  [60800/209452]\n",
      "loss: 0.357540  [64000/209452]\n",
      "loss: 0.394204  [67200/209452]\n",
      "loss: 0.594163  [70400/209452]\n",
      "loss: 0.406362  [73600/209452]\n",
      "loss: 0.508154  [76800/209452]\n",
      "loss: 0.292239  [80000/209452]\n",
      "loss: 0.370706  [83200/209452]\n",
      "loss: 0.360769  [86400/209452]\n",
      "loss: 0.271862  [89600/209452]\n",
      "loss: 0.463574  [92800/209452]\n",
      "loss: 0.379886  [96000/209452]\n",
      "loss: 0.371140  [99200/209452]\n",
      "loss: 0.305962  [102400/209452]\n",
      "loss: 0.392789  [105600/209452]\n",
      "loss: 0.279352  [108800/209452]\n",
      "loss: 0.456357  [112000/209452]\n",
      "loss: 0.475276  [115200/209452]\n",
      "loss: 0.609591  [118400/209452]\n",
      "loss: 0.404919  [121600/209452]\n",
      "loss: 0.357363  [124800/209452]\n",
      "loss: 0.394301  [128000/209452]\n",
      "loss: 0.432842  [131200/209452]\n",
      "loss: 0.282421  [134400/209452]\n",
      "loss: 0.522956  [137600/209452]\n",
      "loss: 0.360275  [140800/209452]\n",
      "loss: 0.466768  [144000/209452]\n",
      "loss: 0.475611  [147200/209452]\n",
      "loss: 0.324293  [150400/209452]\n",
      "loss: 0.387190  [153600/209452]\n",
      "loss: 0.216383  [156800/209452]\n",
      "loss: 0.370353  [160000/209452]\n",
      "loss: 0.379618  [163200/209452]\n",
      "loss: 0.356524  [166400/209452]\n",
      "loss: 0.386719  [169600/209452]\n",
      "loss: 0.499689  [172800/209452]\n",
      "loss: 0.416589  [176000/209452]\n",
      "loss: 0.454616  [179200/209452]\n",
      "loss: 0.501207  [182400/209452]\n",
      "loss: 0.408674  [185600/209452]\n",
      "loss: 0.333560  [188800/209452]\n",
      "loss: 0.268248  [192000/209452]\n",
      "loss: 0.379037  [195200/209452]\n",
      "loss: 0.434363  [198400/209452]\n",
      "loss: 0.437432  [201600/209452]\n",
      "loss: 0.467145  [204800/209452]\n",
      "loss: 0.428021  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.384555 \n",
      "\n",
      "Epoch 28\n",
      "---------------------------\n",
      "loss: 0.431237  [ 3200/209452]\n",
      "loss: 0.323575  [ 6400/209452]\n",
      "loss: 0.329493  [ 9600/209452]\n",
      "loss: 0.402803  [12800/209452]\n",
      "loss: 0.410717  [16000/209452]\n",
      "loss: 0.311616  [19200/209452]\n",
      "loss: 0.707049  [22400/209452]\n",
      "loss: 0.428457  [25600/209452]\n",
      "loss: 0.406132  [28800/209452]\n",
      "loss: 0.418320  [32000/209452]\n",
      "loss: 0.327089  [35200/209452]\n",
      "loss: 0.401537  [38400/209452]\n",
      "loss: 0.424369  [41600/209452]\n",
      "loss: 0.336003  [44800/209452]\n",
      "loss: 0.278472  [48000/209452]\n",
      "loss: 0.256069  [51200/209452]\n",
      "loss: 0.330271  [54400/209452]\n",
      "loss: 0.279231  [57600/209452]\n",
      "loss: 0.243405  [60800/209452]\n",
      "loss: 0.433775  [64000/209452]\n",
      "loss: 0.317719  [67200/209452]\n",
      "loss: 0.448041  [70400/209452]\n",
      "loss: 0.432420  [73600/209452]\n",
      "loss: 0.339689  [76800/209452]\n",
      "loss: 0.315705  [80000/209452]\n",
      "loss: 0.463751  [83200/209452]\n",
      "loss: 0.418508  [86400/209452]\n",
      "loss: 0.451012  [89600/209452]\n",
      "loss: 0.326242  [92800/209452]\n",
      "loss: 0.274978  [96000/209452]\n",
      "loss: 0.328411  [99200/209452]\n",
      "loss: 0.350971  [102400/209452]\n",
      "loss: 0.443582  [105600/209452]\n",
      "loss: 0.383567  [108800/209452]\n",
      "loss: 0.419295  [112000/209452]\n",
      "loss: 0.344732  [115200/209452]\n",
      "loss: 0.257960  [118400/209452]\n",
      "loss: 0.420462  [121600/209452]\n",
      "loss: 0.598217  [124800/209452]\n",
      "loss: 0.418079  [128000/209452]\n",
      "loss: 0.295585  [131200/209452]\n",
      "loss: 0.281527  [134400/209452]\n",
      "loss: 0.286775  [137600/209452]\n",
      "loss: 0.287883  [140800/209452]\n",
      "loss: 0.364437  [144000/209452]\n",
      "loss: 0.452882  [147200/209452]\n",
      "loss: 0.592469  [150400/209452]\n",
      "loss: 0.374826  [153600/209452]\n",
      "loss: 0.305196  [156800/209452]\n",
      "loss: 0.356851  [160000/209452]\n",
      "loss: 0.421123  [163200/209452]\n",
      "loss: 0.366393  [166400/209452]\n",
      "loss: 0.425949  [169600/209452]\n",
      "loss: 0.335259  [172800/209452]\n",
      "loss: 0.345550  [176000/209452]\n",
      "loss: 0.407622  [179200/209452]\n",
      "loss: 0.462229  [182400/209452]\n",
      "loss: 0.312222  [185600/209452]\n",
      "loss: 0.397248  [188800/209452]\n",
      "loss: 0.323588  [192000/209452]\n",
      "loss: 0.386525  [195200/209452]\n",
      "loss: 0.359463  [198400/209452]\n",
      "loss: 0.348409  [201600/209452]\n",
      "loss: 0.301547  [204800/209452]\n",
      "loss: 0.305990  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.378203 \n",
      "\n",
      "Epoch 29\n",
      "---------------------------\n",
      "loss: 0.199738  [ 3200/209452]\n",
      "loss: 0.515583  [ 6400/209452]\n",
      "loss: 0.235599  [ 9600/209452]\n",
      "loss: 0.429271  [12800/209452]\n",
      "loss: 0.324145  [16000/209452]\n",
      "loss: 0.475025  [19200/209452]\n",
      "loss: 0.378081  [22400/209452]\n",
      "loss: 0.373730  [25600/209452]\n",
      "loss: 0.390801  [28800/209452]\n",
      "loss: 0.578708  [32000/209452]\n",
      "loss: 0.399129  [35200/209452]\n",
      "loss: 0.311213  [38400/209452]\n",
      "loss: 0.587138  [41600/209452]\n",
      "loss: 0.552716  [44800/209452]\n",
      "loss: 0.336845  [48000/209452]\n",
      "loss: 0.318084  [51200/209452]\n",
      "loss: 0.385152  [54400/209452]\n",
      "loss: 0.285618  [57600/209452]\n",
      "loss: 0.419356  [60800/209452]\n",
      "loss: 0.163489  [64000/209452]\n",
      "loss: 0.429666  [67200/209452]\n",
      "loss: 0.340021  [70400/209452]\n",
      "loss: 0.282504  [73600/209452]\n",
      "loss: 0.439065  [76800/209452]\n",
      "loss: 0.292315  [80000/209452]\n",
      "loss: 0.256664  [83200/209452]\n",
      "loss: 0.283308  [86400/209452]\n",
      "loss: 0.424790  [89600/209452]\n",
      "loss: 0.269615  [92800/209452]\n",
      "loss: 0.411315  [96000/209452]\n",
      "loss: 0.405612  [99200/209452]\n",
      "loss: 0.303544  [102400/209452]\n",
      "loss: 0.350145  [105600/209452]\n",
      "loss: 0.241693  [108800/209452]\n",
      "loss: 0.349717  [112000/209452]\n",
      "loss: 0.325072  [115200/209452]\n",
      "loss: 0.340160  [118400/209452]\n",
      "loss: 0.507345  [121600/209452]\n",
      "loss: 0.468164  [124800/209452]\n",
      "loss: 0.515564  [128000/209452]\n",
      "loss: 0.257437  [131200/209452]\n",
      "loss: 0.507254  [134400/209452]\n",
      "loss: 0.492762  [137600/209452]\n",
      "loss: 0.312203  [140800/209452]\n",
      "loss: 0.458330  [144000/209452]\n",
      "loss: 0.394124  [147200/209452]\n",
      "loss: 0.371187  [150400/209452]\n",
      "loss: 0.455963  [153600/209452]\n",
      "loss: 0.383545  [156800/209452]\n",
      "loss: 0.449865  [160000/209452]\n",
      "loss: 0.401323  [163200/209452]\n",
      "loss: 0.421123  [166400/209452]\n",
      "loss: 0.415391  [169600/209452]\n",
      "loss: 0.325311  [172800/209452]\n",
      "loss: 0.334926  [176000/209452]\n",
      "loss: 0.263300  [179200/209452]\n",
      "loss: 0.331637  [182400/209452]\n",
      "loss: 0.327734  [185600/209452]\n",
      "loss: 0.439849  [188800/209452]\n",
      "loss: 0.445762  [192000/209452]\n",
      "loss: 0.271725  [195200/209452]\n",
      "loss: 0.229083  [198400/209452]\n",
      "loss: 0.375833  [201600/209452]\n",
      "loss: 0.417838  [204800/209452]\n",
      "loss: 0.445141  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.368903 \n",
      "\n",
      "Epoch 30\n",
      "---------------------------\n",
      "loss: 0.390507  [ 3200/209452]\n",
      "loss: 0.509821  [ 6400/209452]\n",
      "loss: 0.421621  [ 9600/209452]\n",
      "loss: 0.260129  [12800/209452]\n",
      "loss: 0.370711  [16000/209452]\n",
      "loss: 0.425801  [19200/209452]\n",
      "loss: 0.338195  [22400/209452]\n",
      "loss: 0.339248  [25600/209452]\n",
      "loss: 0.474297  [28800/209452]\n",
      "loss: 0.413714  [32000/209452]\n",
      "loss: 0.439109  [35200/209452]\n",
      "loss: 0.428688  [38400/209452]\n",
      "loss: 0.262847  [41600/209452]\n",
      "loss: 0.300413  [44800/209452]\n",
      "loss: 0.235076  [48000/209452]\n",
      "loss: 0.439033  [51200/209452]\n",
      "loss: 0.436273  [54400/209452]\n",
      "loss: 0.445518  [57600/209452]\n",
      "loss: 0.445848  [60800/209452]\n",
      "loss: 0.209650  [64000/209452]\n",
      "loss: 0.341021  [67200/209452]\n",
      "loss: 0.370074  [70400/209452]\n",
      "loss: 0.363989  [73600/209452]\n",
      "loss: 0.231857  [76800/209452]\n",
      "loss: 0.318385  [80000/209452]\n",
      "loss: 0.254719  [83200/209452]\n",
      "loss: 0.290375  [86400/209452]\n",
      "loss: 0.420513  [89600/209452]\n",
      "loss: 0.389571  [92800/209452]\n",
      "loss: 0.539126  [96000/209452]\n",
      "loss: 0.475790  [99200/209452]\n",
      "loss: 0.415670  [102400/209452]\n",
      "loss: 0.277554  [105600/209452]\n",
      "loss: 0.338967  [108800/209452]\n",
      "loss: 0.314858  [112000/209452]\n",
      "loss: 0.335059  [115200/209452]\n",
      "loss: 0.320698  [118400/209452]\n",
      "loss: 0.488077  [121600/209452]\n",
      "loss: 0.293899  [124800/209452]\n",
      "loss: 0.470082  [128000/209452]\n",
      "loss: 0.344422  [131200/209452]\n",
      "loss: 0.427128  [134400/209452]\n",
      "loss: 0.252051  [137600/209452]\n",
      "loss: 0.426057  [140800/209452]\n",
      "loss: 0.434891  [144000/209452]\n",
      "loss: 0.458093  [147200/209452]\n",
      "loss: 0.315149  [150400/209452]\n",
      "loss: 0.302298  [153600/209452]\n",
      "loss: 0.289881  [156800/209452]\n",
      "loss: 0.267535  [160000/209452]\n",
      "loss: 0.409611  [163200/209452]\n",
      "loss: 0.328708  [166400/209452]\n",
      "loss: 0.446292  [169600/209452]\n",
      "loss: 0.336581  [172800/209452]\n",
      "loss: 0.400769  [176000/209452]\n",
      "loss: 0.319984  [179200/209452]\n",
      "loss: 0.415666  [182400/209452]\n",
      "loss: 0.268704  [185600/209452]\n",
      "loss: 0.411740  [188800/209452]\n",
      "loss: 0.372393  [192000/209452]\n",
      "loss: 0.128100  [195200/209452]\n",
      "loss: 0.318102  [198400/209452]\n",
      "loss: 0.302901  [201600/209452]\n",
      "loss: 0.392890  [204800/209452]\n",
      "loss: 0.377779  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.363397 \n",
      "\n",
      "Epoch 31\n",
      "---------------------------\n",
      "loss: 0.410156  [ 3200/209452]\n",
      "loss: 0.223163  [ 6400/209452]\n",
      "loss: 0.296009  [ 9600/209452]\n",
      "loss: 0.284614  [12800/209452]\n",
      "loss: 0.351061  [16000/209452]\n",
      "loss: 0.276419  [19200/209452]\n",
      "loss: 0.489064  [22400/209452]\n",
      "loss: 0.378244  [25600/209452]\n",
      "loss: 0.392300  [28800/209452]\n",
      "loss: 0.407790  [32000/209452]\n",
      "loss: 0.349600  [35200/209452]\n",
      "loss: 0.402267  [38400/209452]\n",
      "loss: 0.466985  [41600/209452]\n",
      "loss: 0.445334  [44800/209452]\n",
      "loss: 0.378654  [48000/209452]\n",
      "loss: 0.421455  [51200/209452]\n",
      "loss: 0.291879  [54400/209452]\n",
      "loss: 0.290951  [57600/209452]\n",
      "loss: 0.288683  [60800/209452]\n",
      "loss: 0.404847  [64000/209452]\n",
      "loss: 0.398529  [67200/209452]\n",
      "loss: 0.507280  [70400/209452]\n",
      "loss: 0.328064  [73600/209452]\n",
      "loss: 0.595194  [76800/209452]\n",
      "loss: 0.324643  [80000/209452]\n",
      "loss: 0.461213  [83200/209452]\n",
      "loss: 0.339647  [86400/209452]\n",
      "loss: 0.430850  [89600/209452]\n",
      "loss: 0.409683  [92800/209452]\n",
      "loss: 0.345404  [96000/209452]\n",
      "loss: 0.417809  [99200/209452]\n",
      "loss: 0.301709  [102400/209452]\n",
      "loss: 0.231278  [105600/209452]\n",
      "loss: 0.190679  [108800/209452]\n",
      "loss: 0.196910  [112000/209452]\n",
      "loss: 0.360306  [115200/209452]\n",
      "loss: 0.498282  [118400/209452]\n",
      "loss: 0.502718  [121600/209452]\n",
      "loss: 0.239962  [124800/209452]\n",
      "loss: 0.486427  [128000/209452]\n",
      "loss: 0.491698  [131200/209452]\n",
      "loss: 0.337911  [134400/209452]\n",
      "loss: 0.339774  [137600/209452]\n",
      "loss: 0.322661  [140800/209452]\n",
      "loss: 0.420805  [144000/209452]\n",
      "loss: 0.316914  [147200/209452]\n",
      "loss: 0.419061  [150400/209452]\n",
      "loss: 0.317634  [153600/209452]\n",
      "loss: 0.240493  [156800/209452]\n",
      "loss: 0.229387  [160000/209452]\n",
      "loss: 0.332666  [163200/209452]\n",
      "loss: 0.311697  [166400/209452]\n",
      "loss: 0.338159  [169600/209452]\n",
      "loss: 0.369318  [172800/209452]\n",
      "loss: 0.264935  [176000/209452]\n",
      "loss: 0.698697  [179200/209452]\n",
      "loss: 0.313816  [182400/209452]\n",
      "loss: 0.475553  [185600/209452]\n",
      "loss: 0.494172  [188800/209452]\n",
      "loss: 0.405253  [192000/209452]\n",
      "loss: 0.366025  [195200/209452]\n",
      "loss: 0.619229  [198400/209452]\n",
      "loss: 0.216736  [201600/209452]\n",
      "loss: 0.401473  [204800/209452]\n",
      "loss: 0.280310  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.357594 \n",
      "\n",
      "Epoch 32\n",
      "---------------------------\n",
      "loss: 0.454085  [ 3200/209452]\n",
      "loss: 0.235587  [ 6400/209452]\n",
      "loss: 0.388813  [ 9600/209452]\n",
      "loss: 0.310941  [12800/209452]\n",
      "loss: 0.412183  [16000/209452]\n",
      "loss: 0.490974  [19200/209452]\n",
      "loss: 0.304635  [22400/209452]\n",
      "loss: 0.420705  [25600/209452]\n",
      "loss: 0.332260  [28800/209452]\n",
      "loss: 0.614424  [32000/209452]\n",
      "loss: 0.372240  [35200/209452]\n",
      "loss: 0.352690  [38400/209452]\n",
      "loss: 0.287112  [41600/209452]\n",
      "loss: 0.462145  [44800/209452]\n",
      "loss: 0.172118  [48000/209452]\n",
      "loss: 0.327176  [51200/209452]\n",
      "loss: 0.278434  [54400/209452]\n",
      "loss: 0.518172  [57600/209452]\n",
      "loss: 0.386763  [60800/209452]\n",
      "loss: 0.402459  [64000/209452]\n",
      "loss: 0.430339  [67200/209452]\n",
      "loss: 0.329368  [70400/209452]\n",
      "loss: 0.413597  [73600/209452]\n",
      "loss: 0.274074  [76800/209452]\n",
      "loss: 0.201618  [80000/209452]\n",
      "loss: 0.362552  [83200/209452]\n",
      "loss: 0.345622  [86400/209452]\n",
      "loss: 0.267490  [89600/209452]\n",
      "loss: 0.386496  [92800/209452]\n",
      "loss: 0.391215  [96000/209452]\n",
      "loss: 0.213822  [99200/209452]\n",
      "loss: 0.583185  [102400/209452]\n",
      "loss: 0.245137  [105600/209452]\n",
      "loss: 0.457964  [108800/209452]\n",
      "loss: 0.276754  [112000/209452]\n",
      "loss: 0.418297  [115200/209452]\n",
      "loss: 0.417176  [118400/209452]\n",
      "loss: 0.262045  [121600/209452]\n",
      "loss: 0.439321  [124800/209452]\n",
      "loss: 0.299684  [128000/209452]\n",
      "loss: 0.346437  [131200/209452]\n",
      "loss: 0.362760  [134400/209452]\n",
      "loss: 0.422745  [137600/209452]\n",
      "loss: 0.291454  [140800/209452]\n",
      "loss: 0.487003  [144000/209452]\n",
      "loss: 0.453695  [147200/209452]\n",
      "loss: 0.385669  [150400/209452]\n",
      "loss: 0.585560  [153600/209452]\n",
      "loss: 0.400786  [156800/209452]\n",
      "loss: 0.261099  [160000/209452]\n",
      "loss: 0.401478  [163200/209452]\n",
      "loss: 0.336641  [166400/209452]\n",
      "loss: 0.285398  [169600/209452]\n",
      "loss: 0.333472  [172800/209452]\n",
      "loss: 0.410345  [176000/209452]\n",
      "loss: 0.366049  [179200/209452]\n",
      "loss: 0.311852  [182400/209452]\n",
      "loss: 0.287437  [185600/209452]\n",
      "loss: 0.213069  [188800/209452]\n",
      "loss: 0.347859  [192000/209452]\n",
      "loss: 0.218358  [195200/209452]\n",
      "loss: 0.336264  [198400/209452]\n",
      "loss: 0.352439  [201600/209452]\n",
      "loss: 0.387888  [204800/209452]\n",
      "loss: 0.261338  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.345257 \n",
      "\n",
      "Epoch 33\n",
      "---------------------------\n",
      "loss: 0.248519  [ 3200/209452]\n",
      "loss: 0.327946  [ 6400/209452]\n",
      "loss: 0.414700  [ 9600/209452]\n",
      "loss: 0.401434  [12800/209452]\n",
      "loss: 0.308238  [16000/209452]\n",
      "loss: 0.235365  [19200/209452]\n",
      "loss: 0.216946  [22400/209452]\n",
      "loss: 0.384152  [25600/209452]\n",
      "loss: 0.229728  [28800/209452]\n",
      "loss: 0.380894  [32000/209452]\n",
      "loss: 0.438502  [35200/209452]\n",
      "loss: 0.348923  [38400/209452]\n",
      "loss: 0.348133  [41600/209452]\n",
      "loss: 0.388482  [44800/209452]\n",
      "loss: 0.308209  [48000/209452]\n",
      "loss: 0.321888  [51200/209452]\n",
      "loss: 0.454940  [54400/209452]\n",
      "loss: 0.190225  [57600/209452]\n",
      "loss: 0.357630  [60800/209452]\n",
      "loss: 0.539062  [64000/209452]\n",
      "loss: 0.369694  [67200/209452]\n",
      "loss: 0.348981  [70400/209452]\n",
      "loss: 0.356101  [73600/209452]\n",
      "loss: 0.372000  [76800/209452]\n",
      "loss: 0.466611  [80000/209452]\n",
      "loss: 0.343785  [83200/209452]\n",
      "loss: 0.382478  [86400/209452]\n",
      "loss: 0.404203  [89600/209452]\n",
      "loss: 0.375723  [92800/209452]\n",
      "loss: 0.395614  [96000/209452]\n",
      "loss: 0.322588  [99200/209452]\n",
      "loss: 0.311207  [102400/209452]\n",
      "loss: 0.292744  [105600/209452]\n",
      "loss: 0.464358  [108800/209452]\n",
      "loss: 0.266214  [112000/209452]\n",
      "loss: 0.344216  [115200/209452]\n",
      "loss: 0.367743  [118400/209452]\n",
      "loss: 0.268183  [121600/209452]\n",
      "loss: 0.292497  [124800/209452]\n",
      "loss: 0.325463  [128000/209452]\n",
      "loss: 0.335724  [131200/209452]\n",
      "loss: 0.269133  [134400/209452]\n",
      "loss: 0.361549  [137600/209452]\n",
      "loss: 0.345134  [140800/209452]\n",
      "loss: 0.263796  [144000/209452]\n",
      "loss: 0.245810  [147200/209452]\n",
      "loss: 0.328109  [150400/209452]\n",
      "loss: 0.547237  [153600/209452]\n",
      "loss: 0.256280  [156800/209452]\n",
      "loss: 0.335241  [160000/209452]\n",
      "loss: 0.441959  [163200/209452]\n",
      "loss: 0.398131  [166400/209452]\n",
      "loss: 0.201275  [169600/209452]\n",
      "loss: 0.509831  [172800/209452]\n",
      "loss: 0.387226  [176000/209452]\n",
      "loss: 0.559944  [179200/209452]\n",
      "loss: 0.417273  [182400/209452]\n",
      "loss: 0.395651  [185600/209452]\n",
      "loss: 0.722957  [188800/209452]\n",
      "loss: 0.234164  [192000/209452]\n",
      "loss: 0.383352  [195200/209452]\n",
      "loss: 0.507414  [198400/209452]\n",
      "loss: 0.369803  [201600/209452]\n",
      "loss: 0.285190  [204800/209452]\n",
      "loss: 0.371957  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.337602 \n",
      "\n",
      "Epoch 34\n",
      "---------------------------\n",
      "loss: 0.398959  [ 3200/209452]\n",
      "loss: 0.197342  [ 6400/209452]\n",
      "loss: 0.271925  [ 9600/209452]\n",
      "loss: 0.393812  [12800/209452]\n",
      "loss: 0.309179  [16000/209452]\n",
      "loss: 0.230714  [19200/209452]\n",
      "loss: 0.359719  [22400/209452]\n",
      "loss: 0.624186  [25600/209452]\n",
      "loss: 0.383942  [28800/209452]\n",
      "loss: 0.341406  [32000/209452]\n",
      "loss: 0.292357  [35200/209452]\n",
      "loss: 0.279019  [38400/209452]\n",
      "loss: 0.332467  [41600/209452]\n",
      "loss: 0.169966  [44800/209452]\n",
      "loss: 0.304821  [48000/209452]\n",
      "loss: 0.340401  [51200/209452]\n",
      "loss: 0.456002  [54400/209452]\n",
      "loss: 0.424568  [57600/209452]\n",
      "loss: 0.277746  [60800/209452]\n",
      "loss: 0.375785  [64000/209452]\n",
      "loss: 0.240963  [67200/209452]\n",
      "loss: 0.404993  [70400/209452]\n",
      "loss: 0.352120  [73600/209452]\n",
      "loss: 0.223615  [76800/209452]\n",
      "loss: 0.516376  [80000/209452]\n",
      "loss: 0.259826  [83200/209452]\n",
      "loss: 0.401948  [86400/209452]\n",
      "loss: 0.294624  [89600/209452]\n",
      "loss: 0.292696  [92800/209452]\n",
      "loss: 0.253288  [96000/209452]\n",
      "loss: 0.304390  [99200/209452]\n",
      "loss: 0.459715  [102400/209452]\n",
      "loss: 0.286507  [105600/209452]\n",
      "loss: 0.313697  [108800/209452]\n",
      "loss: 0.280673  [112000/209452]\n",
      "loss: 0.503862  [115200/209452]\n",
      "loss: 0.393593  [118400/209452]\n",
      "loss: 0.275658  [121600/209452]\n",
      "loss: 0.368455  [124800/209452]\n",
      "loss: 0.465385  [128000/209452]\n",
      "loss: 0.342208  [131200/209452]\n",
      "loss: 0.345895  [134400/209452]\n",
      "loss: 0.358145  [137600/209452]\n",
      "loss: 0.376475  [140800/209452]\n",
      "loss: 0.374249  [144000/209452]\n",
      "loss: 0.552650  [147200/209452]\n",
      "loss: 0.431667  [150400/209452]\n",
      "loss: 0.342337  [153600/209452]\n",
      "loss: 0.383497  [156800/209452]\n",
      "loss: 0.380248  [160000/209452]\n",
      "loss: 0.442636  [163200/209452]\n",
      "loss: 0.252952  [166400/209452]\n",
      "loss: 0.303582  [169600/209452]\n",
      "loss: 0.231472  [172800/209452]\n",
      "loss: 0.296789  [176000/209452]\n",
      "loss: 0.400157  [179200/209452]\n",
      "loss: 0.206983  [182400/209452]\n",
      "loss: 0.226933  [185600/209452]\n",
      "loss: 0.206380  [188800/209452]\n",
      "loss: 0.444431  [192000/209452]\n",
      "loss: 0.240956  [195200/209452]\n",
      "loss: 0.270479  [198400/209452]\n",
      "loss: 0.357832  [201600/209452]\n",
      "loss: 0.273685  [204800/209452]\n",
      "loss: 0.338841  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.330280 \n",
      "\n",
      "Epoch 35\n",
      "---------------------------\n",
      "loss: 0.244326  [ 3200/209452]\n",
      "loss: 0.443766  [ 6400/209452]\n",
      "loss: 0.310113  [ 9600/209452]\n",
      "loss: 0.324558  [12800/209452]\n",
      "loss: 0.302672  [16000/209452]\n",
      "loss: 0.319532  [19200/209452]\n",
      "loss: 0.391988  [22400/209452]\n",
      "loss: 0.220820  [25600/209452]\n",
      "loss: 0.228279  [28800/209452]\n",
      "loss: 0.297083  [32000/209452]\n",
      "loss: 0.324640  [35200/209452]\n",
      "loss: 0.371792  [38400/209452]\n",
      "loss: 0.349141  [41600/209452]\n",
      "loss: 0.460059  [44800/209452]\n",
      "loss: 0.565237  [48000/209452]\n",
      "loss: 0.408094  [51200/209452]\n",
      "loss: 0.337497  [54400/209452]\n",
      "loss: 0.294869  [57600/209452]\n",
      "loss: 0.417031  [60800/209452]\n",
      "loss: 0.251699  [64000/209452]\n",
      "loss: 0.349882  [67200/209452]\n",
      "loss: 0.636984  [70400/209452]\n",
      "loss: 0.413607  [73600/209452]\n",
      "loss: 0.283183  [76800/209452]\n",
      "loss: 0.286532  [80000/209452]\n",
      "loss: 0.375093  [83200/209452]\n",
      "loss: 0.271059  [86400/209452]\n",
      "loss: 0.231557  [89600/209452]\n",
      "loss: 0.219082  [92800/209452]\n",
      "loss: 0.392871  [96000/209452]\n",
      "loss: 0.366702  [99200/209452]\n",
      "loss: 0.521960  [102400/209452]\n",
      "loss: 0.465569  [105600/209452]\n",
      "loss: 0.262031  [108800/209452]\n",
      "loss: 0.398551  [112000/209452]\n",
      "loss: 0.394315  [115200/209452]\n",
      "loss: 0.237290  [118400/209452]\n",
      "loss: 0.296959  [121600/209452]\n",
      "loss: 0.267675  [124800/209452]\n",
      "loss: 0.331202  [128000/209452]\n",
      "loss: 0.284334  [131200/209452]\n",
      "loss: 0.306847  [134400/209452]\n",
      "loss: 0.512666  [137600/209452]\n",
      "loss: 0.253286  [140800/209452]\n",
      "loss: 0.245607  [144000/209452]\n",
      "loss: 0.279196  [147200/209452]\n",
      "loss: 0.459614  [150400/209452]\n",
      "loss: 0.251059  [153600/209452]\n",
      "loss: 0.404795  [156800/209452]\n",
      "loss: 0.243668  [160000/209452]\n",
      "loss: 0.338895  [163200/209452]\n",
      "loss: 0.359802  [166400/209452]\n",
      "loss: 0.179039  [169600/209452]\n",
      "loss: 0.294800  [172800/209452]\n",
      "loss: 0.231874  [176000/209452]\n",
      "loss: 0.302312  [179200/209452]\n",
      "loss: 0.160646  [182400/209452]\n",
      "loss: 0.300311  [185600/209452]\n",
      "loss: 0.373696  [188800/209452]\n",
      "loss: 0.242629  [192000/209452]\n",
      "loss: 0.209673  [195200/209452]\n",
      "loss: 0.342820  [198400/209452]\n",
      "loss: 0.324651  [201600/209452]\n",
      "loss: 0.267179  [204800/209452]\n",
      "loss: 0.290221  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.322612 \n",
      "\n",
      "Epoch 36\n",
      "---------------------------\n",
      "loss: 0.181731  [ 3200/209452]\n",
      "loss: 0.179311  [ 6400/209452]\n",
      "loss: 0.410408  [ 9600/209452]\n",
      "loss: 0.288259  [12800/209452]\n",
      "loss: 0.285455  [16000/209452]\n",
      "loss: 0.200148  [19200/209452]\n",
      "loss: 0.143095  [22400/209452]\n",
      "loss: 0.321374  [25600/209452]\n",
      "loss: 0.405377  [28800/209452]\n",
      "loss: 0.251037  [32000/209452]\n",
      "loss: 0.310520  [35200/209452]\n",
      "loss: 0.410080  [38400/209452]\n",
      "loss: 0.258091  [41600/209452]\n",
      "loss: 0.200569  [44800/209452]\n",
      "loss: 0.413369  [48000/209452]\n",
      "loss: 0.336499  [51200/209452]\n",
      "loss: 0.201268  [54400/209452]\n",
      "loss: 0.209276  [57600/209452]\n",
      "loss: 0.207246  [60800/209452]\n",
      "loss: 0.385577  [64000/209452]\n",
      "loss: 0.323844  [67200/209452]\n",
      "loss: 0.473386  [70400/209452]\n",
      "loss: 0.193055  [73600/209452]\n",
      "loss: 0.192442  [76800/209452]\n",
      "loss: 0.279203  [80000/209452]\n",
      "loss: 0.209625  [83200/209452]\n",
      "loss: 0.419651  [86400/209452]\n",
      "loss: 0.206324  [89600/209452]\n",
      "loss: 0.258000  [92800/209452]\n",
      "loss: 0.193967  [96000/209452]\n",
      "loss: 0.400971  [99200/209452]\n",
      "loss: 0.339465  [102400/209452]\n",
      "loss: 0.239077  [105600/209452]\n",
      "loss: 0.410932  [108800/209452]\n",
      "loss: 0.308273  [112000/209452]\n",
      "loss: 0.249253  [115200/209452]\n",
      "loss: 0.406967  [118400/209452]\n",
      "loss: 0.305436  [121600/209452]\n",
      "loss: 0.341344  [124800/209452]\n",
      "loss: 0.308556  [128000/209452]\n",
      "loss: 0.529811  [131200/209452]\n",
      "loss: 0.196084  [134400/209452]\n",
      "loss: 0.258203  [137600/209452]\n",
      "loss: 0.371154  [140800/209452]\n",
      "loss: 0.341784  [144000/209452]\n",
      "loss: 0.463236  [147200/209452]\n",
      "loss: 0.213587  [150400/209452]\n",
      "loss: 0.492026  [153600/209452]\n",
      "loss: 0.379793  [156800/209452]\n",
      "loss: 0.150415  [160000/209452]\n",
      "loss: 0.356567  [163200/209452]\n",
      "loss: 0.264849  [166400/209452]\n",
      "loss: 0.313432  [169600/209452]\n",
      "loss: 0.408281  [172800/209452]\n",
      "loss: 0.411971  [176000/209452]\n",
      "loss: 0.289032  [179200/209452]\n",
      "loss: 0.306134  [182400/209452]\n",
      "loss: 0.241604  [185600/209452]\n",
      "loss: 0.256965  [188800/209452]\n",
      "loss: 0.345829  [192000/209452]\n",
      "loss: 0.351252  [195200/209452]\n",
      "loss: 0.375407  [198400/209452]\n",
      "loss: 0.279773  [201600/209452]\n",
      "loss: 0.414119  [204800/209452]\n",
      "loss: 0.348657  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.317111 \n",
      "\n",
      "Epoch 37\n",
      "---------------------------\n",
      "loss: 0.228897  [ 3200/209452]\n",
      "loss: 0.233451  [ 6400/209452]\n",
      "loss: 0.262345  [ 9600/209452]\n",
      "loss: 0.348485  [12800/209452]\n",
      "loss: 0.197789  [16000/209452]\n",
      "loss: 0.407590  [19200/209452]\n",
      "loss: 0.342206  [22400/209452]\n",
      "loss: 0.397367  [25600/209452]\n",
      "loss: 0.153940  [28800/209452]\n",
      "loss: 0.469049  [32000/209452]\n",
      "loss: 0.343623  [35200/209452]\n",
      "loss: 0.260098  [38400/209452]\n",
      "loss: 0.461482  [41600/209452]\n",
      "loss: 0.291151  [44800/209452]\n",
      "loss: 0.299749  [48000/209452]\n",
      "loss: 0.482434  [51200/209452]\n",
      "loss: 0.190988  [54400/209452]\n",
      "loss: 0.257549  [57600/209452]\n",
      "loss: 0.279509  [60800/209452]\n",
      "loss: 0.276000  [64000/209452]\n",
      "loss: 0.308717  [67200/209452]\n",
      "loss: 0.265504  [70400/209452]\n",
      "loss: 0.281494  [73600/209452]\n",
      "loss: 0.358141  [76800/209452]\n",
      "loss: 0.248395  [80000/209452]\n",
      "loss: 0.343659  [83200/209452]\n",
      "loss: 0.370896  [86400/209452]\n",
      "loss: 0.283537  [89600/209452]\n",
      "loss: 0.199793  [92800/209452]\n",
      "loss: 0.382336  [96000/209452]\n",
      "loss: 0.266065  [99200/209452]\n",
      "loss: 0.244219  [102400/209452]\n",
      "loss: 0.330540  [105600/209452]\n",
      "loss: 0.269124  [108800/209452]\n",
      "loss: 0.239863  [112000/209452]\n",
      "loss: 0.406000  [115200/209452]\n",
      "loss: 0.357690  [118400/209452]\n",
      "loss: 0.374516  [121600/209452]\n",
      "loss: 0.348503  [124800/209452]\n",
      "loss: 0.326033  [128000/209452]\n",
      "loss: 0.215547  [131200/209452]\n",
      "loss: 0.364233  [134400/209452]\n",
      "loss: 0.302914  [137600/209452]\n",
      "loss: 0.206416  [140800/209452]\n",
      "loss: 0.235947  [144000/209452]\n",
      "loss: 0.291635  [147200/209452]\n",
      "loss: 0.332532  [150400/209452]\n",
      "loss: 0.188543  [153600/209452]\n",
      "loss: 0.309179  [156800/209452]\n",
      "loss: 0.189641  [160000/209452]\n",
      "loss: 0.348523  [163200/209452]\n",
      "loss: 0.263673  [166400/209452]\n",
      "loss: 0.280295  [169600/209452]\n",
      "loss: 0.312855  [172800/209452]\n",
      "loss: 0.199842  [176000/209452]\n",
      "loss: 0.359476  [179200/209452]\n",
      "loss: 0.119411  [182400/209452]\n",
      "loss: 0.285220  [185600/209452]\n",
      "loss: 0.305667  [188800/209452]\n",
      "loss: 0.389191  [192000/209452]\n",
      "loss: 0.248733  [195200/209452]\n",
      "loss: 0.332398  [198400/209452]\n",
      "loss: 0.150155  [201600/209452]\n",
      "loss: 0.433101  [204800/209452]\n",
      "loss: 0.247260  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.309331 \n",
      "\n",
      "Epoch 38\n",
      "---------------------------\n",
      "loss: 0.332205  [ 3200/209452]\n",
      "loss: 0.412953  [ 6400/209452]\n",
      "loss: 0.212003  [ 9600/209452]\n",
      "loss: 0.294324  [12800/209452]\n",
      "loss: 0.358164  [16000/209452]\n",
      "loss: 0.403452  [19200/209452]\n",
      "loss: 0.283471  [22400/209452]\n",
      "loss: 0.418727  [25600/209452]\n",
      "loss: 0.271588  [28800/209452]\n",
      "loss: 0.232173  [32000/209452]\n",
      "loss: 0.318730  [35200/209452]\n",
      "loss: 0.310449  [38400/209452]\n",
      "loss: 0.271662  [41600/209452]\n",
      "loss: 0.221234  [44800/209452]\n",
      "loss: 0.296832  [48000/209452]\n",
      "loss: 0.323431  [51200/209452]\n",
      "loss: 0.431401  [54400/209452]\n",
      "loss: 0.291389  [57600/209452]\n",
      "loss: 0.352949  [60800/209452]\n",
      "loss: 0.178605  [64000/209452]\n",
      "loss: 0.219954  [67200/209452]\n",
      "loss: 0.383812  [70400/209452]\n",
      "loss: 0.419014  [73600/209452]\n",
      "loss: 0.231586  [76800/209452]\n",
      "loss: 0.336105  [80000/209452]\n",
      "loss: 0.313122  [83200/209452]\n",
      "loss: 0.327013  [86400/209452]\n",
      "loss: 0.299990  [89600/209452]\n",
      "loss: 0.308096  [92800/209452]\n",
      "loss: 0.505801  [96000/209452]\n",
      "loss: 0.136291  [99200/209452]\n",
      "loss: 0.497880  [102400/209452]\n",
      "loss: 0.382842  [105600/209452]\n",
      "loss: 0.278308  [108800/209452]\n",
      "loss: 0.229518  [112000/209452]\n",
      "loss: 0.456141  [115200/209452]\n",
      "loss: 0.292009  [118400/209452]\n",
      "loss: 0.436598  [121600/209452]\n",
      "loss: 0.208822  [124800/209452]\n",
      "loss: 0.224430  [128000/209452]\n",
      "loss: 0.294059  [131200/209452]\n",
      "loss: 0.254452  [134400/209452]\n",
      "loss: 0.264662  [137600/209452]\n",
      "loss: 0.300036  [140800/209452]\n",
      "loss: 0.458790  [144000/209452]\n",
      "loss: 0.335255  [147200/209452]\n",
      "loss: 0.142853  [150400/209452]\n",
      "loss: 0.253552  [153600/209452]\n",
      "loss: 0.271651  [156800/209452]\n",
      "loss: 0.238967  [160000/209452]\n",
      "loss: 0.164294  [163200/209452]\n",
      "loss: 0.181778  [166400/209452]\n",
      "loss: 0.240119  [169600/209452]\n",
      "loss: 0.392653  [172800/209452]\n",
      "loss: 0.275160  [176000/209452]\n",
      "loss: 0.445210  [179200/209452]\n",
      "loss: 0.428129  [182400/209452]\n",
      "loss: 0.285078  [185600/209452]\n",
      "loss: 0.263144  [188800/209452]\n",
      "loss: 0.364572  [192000/209452]\n",
      "loss: 0.267889  [195200/209452]\n",
      "loss: 0.458318  [198400/209452]\n",
      "loss: 0.290259  [201600/209452]\n",
      "loss: 0.253692  [204800/209452]\n",
      "loss: 0.268369  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.302062 \n",
      "\n",
      "Epoch 39\n",
      "---------------------------\n",
      "loss: 0.373316  [ 3200/209452]\n",
      "loss: 0.259279  [ 6400/209452]\n",
      "loss: 0.304718  [ 9600/209452]\n",
      "loss: 0.251892  [12800/209452]\n",
      "loss: 0.253903  [16000/209452]\n",
      "loss: 0.218410  [19200/209452]\n",
      "loss: 0.240512  [22400/209452]\n",
      "loss: 0.195103  [25600/209452]\n",
      "loss: 0.354178  [28800/209452]\n",
      "loss: 0.340413  [32000/209452]\n",
      "loss: 0.206059  [35200/209452]\n",
      "loss: 0.242629  [38400/209452]\n",
      "loss: 0.168480  [41600/209452]\n",
      "loss: 0.252666  [44800/209452]\n",
      "loss: 0.346171  [48000/209452]\n",
      "loss: 0.211741  [51200/209452]\n",
      "loss: 0.290377  [54400/209452]\n",
      "loss: 0.415884  [57600/209452]\n",
      "loss: 0.378714  [60800/209452]\n",
      "loss: 0.161500  [64000/209452]\n",
      "loss: 0.389444  [67200/209452]\n",
      "loss: 0.272951  [70400/209452]\n",
      "loss: 0.391650  [73600/209452]\n",
      "loss: 0.299143  [76800/209452]\n",
      "loss: 0.379627  [80000/209452]\n",
      "loss: 0.408815  [83200/209452]\n",
      "loss: 0.200166  [86400/209452]\n",
      "loss: 0.355111  [89600/209452]\n",
      "loss: 0.223697  [92800/209452]\n",
      "loss: 0.214793  [96000/209452]\n",
      "loss: 0.309497  [99200/209452]\n",
      "loss: 0.271457  [102400/209452]\n",
      "loss: 0.381686  [105600/209452]\n",
      "loss: 0.287623  [108800/209452]\n",
      "loss: 0.276368  [112000/209452]\n",
      "loss: 0.366682  [115200/209452]\n",
      "loss: 0.392057  [118400/209452]\n",
      "loss: 0.329022  [121600/209452]\n",
      "loss: 0.312931  [124800/209452]\n",
      "loss: 0.418269  [128000/209452]\n",
      "loss: 0.298851  [131200/209452]\n",
      "loss: 0.245570  [134400/209452]\n",
      "loss: 0.213002  [137600/209452]\n",
      "loss: 0.247085  [140800/209452]\n",
      "loss: 0.388806  [144000/209452]\n",
      "loss: 0.394235  [147200/209452]\n",
      "loss: 0.310900  [150400/209452]\n",
      "loss: 0.424051  [153600/209452]\n",
      "loss: 0.128778  [156800/209452]\n",
      "loss: 0.292927  [160000/209452]\n",
      "loss: 0.301730  [163200/209452]\n",
      "loss: 0.223904  [166400/209452]\n",
      "loss: 0.254458  [169600/209452]\n",
      "loss: 0.327852  [172800/209452]\n",
      "loss: 0.202322  [176000/209452]\n",
      "loss: 0.270197  [179200/209452]\n",
      "loss: 0.305757  [182400/209452]\n",
      "loss: 0.273309  [185600/209452]\n",
      "loss: 0.422224  [188800/209452]\n",
      "loss: 0.231592  [192000/209452]\n",
      "loss: 0.243006  [195200/209452]\n",
      "loss: 0.387993  [198400/209452]\n",
      "loss: 0.434816  [201600/209452]\n",
      "loss: 0.237678  [204800/209452]\n",
      "loss: 0.312292  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.295183 \n",
      "\n",
      "Epoch 40\n",
      "---------------------------\n",
      "loss: 0.329800  [ 3200/209452]\n",
      "loss: 0.218945  [ 6400/209452]\n",
      "loss: 0.289563  [ 9600/209452]\n",
      "loss: 0.340925  [12800/209452]\n",
      "loss: 0.346539  [16000/209452]\n",
      "loss: 0.334775  [19200/209452]\n",
      "loss: 0.244481  [22400/209452]\n",
      "loss: 0.228099  [25600/209452]\n",
      "loss: 0.335688  [28800/209452]\n",
      "loss: 0.267341  [32000/209452]\n",
      "loss: 0.526692  [35200/209452]\n",
      "loss: 0.324144  [38400/209452]\n",
      "loss: 0.260655  [41600/209452]\n",
      "loss: 0.344709  [44800/209452]\n",
      "loss: 0.346119  [48000/209452]\n",
      "loss: 0.290846  [51200/209452]\n",
      "loss: 0.331468  [54400/209452]\n",
      "loss: 0.387711  [57600/209452]\n",
      "loss: 0.217156  [60800/209452]\n",
      "loss: 0.317233  [64000/209452]\n",
      "loss: 0.245250  [67200/209452]\n",
      "loss: 0.269480  [70400/209452]\n",
      "loss: 0.358523  [73600/209452]\n",
      "loss: 0.346113  [76800/209452]\n",
      "loss: 0.228859  [80000/209452]\n",
      "loss: 0.369223  [83200/209452]\n",
      "loss: 0.408369  [86400/209452]\n",
      "loss: 0.293181  [89600/209452]\n",
      "loss: 0.307942  [92800/209452]\n",
      "loss: 0.214131  [96000/209452]\n",
      "loss: 0.300251  [99200/209452]\n",
      "loss: 0.339493  [102400/209452]\n",
      "loss: 0.298635  [105600/209452]\n",
      "loss: 0.119876  [108800/209452]\n",
      "loss: 0.286347  [112000/209452]\n",
      "loss: 0.256156  [115200/209452]\n",
      "loss: 0.341962  [118400/209452]\n",
      "loss: 0.391633  [121600/209452]\n",
      "loss: 0.373780  [124800/209452]\n",
      "loss: 0.259695  [128000/209452]\n",
      "loss: 0.342696  [131200/209452]\n",
      "loss: 0.200101  [134400/209452]\n",
      "loss: 0.167288  [137600/209452]\n",
      "loss: 0.179043  [140800/209452]\n",
      "loss: 0.335496  [144000/209452]\n",
      "loss: 0.248151  [147200/209452]\n",
      "loss: 0.385641  [150400/209452]\n",
      "loss: 0.188456  [153600/209452]\n",
      "loss: 0.345921  [156800/209452]\n",
      "loss: 0.424011  [160000/209452]\n",
      "loss: 0.214607  [163200/209452]\n",
      "loss: 0.170106  [166400/209452]\n",
      "loss: 0.423568  [169600/209452]\n",
      "loss: 0.182746  [172800/209452]\n",
      "loss: 0.245806  [176000/209452]\n",
      "loss: 0.433430  [179200/209452]\n",
      "loss: 0.253997  [182400/209452]\n",
      "loss: 0.165313  [185600/209452]\n",
      "loss: 0.258292  [188800/209452]\n",
      "loss: 0.268375  [192000/209452]\n",
      "loss: 0.393302  [195200/209452]\n",
      "loss: 0.274320  [198400/209452]\n",
      "loss: 0.144287  [201600/209452]\n",
      "loss: 0.243586  [204800/209452]\n",
      "loss: 0.276392  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.295664 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------\")\n",
    "    train_loop(hsd_train_dataloader, hsd_model, loss_fn, hsd_optimizer)\n",
    "    test_loop(hsd_test_dataloader, hsd_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
