{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991806d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ceab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "rank_to_index = {rank: i for i, rank in enumerate(ranks)}\n",
    "rank_len = len(ranks)\n",
    "\n",
    "#Helper functions for data processing\n",
    "def card_to_vec(card):\n",
    "    raw_rank = card[:-1]\n",
    "    one_hot_vector = [0] * rank_len\n",
    "    one_hot_vector[rank_to_index[raw_rank]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def hand_to_list(hand):\n",
    "    '''Takes hand like KH-AC and outputs list of card numbers'''\n",
    "    hand_list_1 = hand.split(\"-\")\n",
    "    hand_list_2 = [card_to_vec(card) for card in hand_list_1]\n",
    "    return hand_list_2\n",
    "\n",
    "result_mapping = {\n",
    "    'hit' : 0,\n",
    "    'stand' : 1,\n",
    "    'double down' : 2\n",
    "}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Defining Dataset Class\n",
    "class Blackjack_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9ef0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "hit_stand_dd_df = pd.read_csv('CSVs/hit_stand_dd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b8eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsd_train_df, hsd_test_df = train_test_split(hit_stand_dd_df, test_size = 0.2)\n",
    "\n",
    "def clean_up(dataframe_raw):\n",
    "    # Cleaned hit_stand_dd\n",
    "    max_len = 7\n",
    "    dataframe_raw['dealer_upcard'] = dataframe_raw['dealer_upcard'].apply(card_to_vec)\n",
    "    dataframe_raw['player_hand'] = dataframe_raw['player_hand'].apply(hand_to_list)\n",
    "    dataframe_raw['result'] = dataframe_raw['result'].map(result_mapping)\n",
    "    \n",
    "    zero_vector = [0] * rank_len\n",
    "    padded_hands = []\n",
    "    for hand in dataframe_raw['player_hand']:\n",
    "        if len(hand) < max_len:\n",
    "            padded_hand = hand + [zero_vector] * (max_len - len(hand))\n",
    "        else: \n",
    "            padded_hand = hand[:max_len]\n",
    "        padded_hands.append(padded_hand)\n",
    "    dataframe_raw['player_hand'] = padded_hands\n",
    "\n",
    "    # Flattening\n",
    "    dataframe_raw['player_hand'] = dataframe_raw['player_hand'].apply(lambda hand: [item for card_vec in hand for item in card_vec])\n",
    "    \n",
    "    # Turning into tensor matrices\n",
    "    # hit_stand_dd\n",
    "    x1 = torch.tensor(dataframe_raw['player_hand'].to_list(), dtype=torch.float32)\n",
    "    x2 = torch.tensor(dataframe_raw['dealer_upcard'].to_list(), dtype=torch.float32)\n",
    "    x3 = torch.tensor(dataframe_raw['can_double'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y = torch.tensor(dataframe_raw['result'].values, dtype=torch.long)\n",
    "\n",
    "    X = torch.cat([x1,x2,x3], dim=1)\n",
    "\n",
    "    return Blackjack_Dataset(X,y)\n",
    "\n",
    "hsd_train_dataset = clean_up(hsd_train_df)\n",
    "hsd_test_dataset = clean_up(hsd_test_df)\n",
    "\n",
    "hsd_train_dataloader = DataLoader(hsd_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "hsd_test_dataloader = DataLoader(hsd_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c76ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 105])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for X, y in hsd_train_dataloader:\n",
    "    print(X.shape)  # input batch shape: (batch_size, input_features)\n",
    "    print(y.shape)  # target batch shape: (batch_size,) or (batch_size, something)\n",
    "    break  # just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd46cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Printing Training Update on every 100th batch\n",
    "        if (batch + 1) % 100 == 0: \n",
    "            loss = loss.item()\n",
    "            current = batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    #Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader: \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93418e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hsd_NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(105, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "hsd_model = hsd_NeuralNetwork()\n",
    "\n",
    "learning_rate = 0.0005 \n",
    "epochs = 20\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "hsd_optimizer = torch.optim.SGD(hsd_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb83a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------\n",
      "loss: 1.124002  [ 3200/209452]\n",
      "loss: 1.125776  [ 6400/209452]\n",
      "loss: 1.114456  [ 9600/209452]\n",
      "loss: 1.105363  [12800/209452]\n",
      "loss: 1.093401  [16000/209452]\n",
      "loss: 1.091389  [19200/209452]\n",
      "loss: 1.084311  [22400/209452]\n",
      "loss: 1.074607  [25600/209452]\n",
      "loss: 1.082070  [28800/209452]\n",
      "loss: 1.064788  [32000/209452]\n",
      "loss: 1.063640  [35200/209452]\n",
      "loss: 1.073602  [38400/209452]\n",
      "loss: 1.039221  [41600/209452]\n",
      "loss: 1.060871  [44800/209452]\n",
      "loss: 1.040838  [48000/209452]\n",
      "loss: 1.044946  [51200/209452]\n",
      "loss: 1.018348  [54400/209452]\n",
      "loss: 1.029382  [57600/209452]\n",
      "loss: 1.003249  [60800/209452]\n",
      "loss: 1.061963  [64000/209452]\n",
      "loss: 1.067829  [67200/209452]\n",
      "loss: 1.051223  [70400/209452]\n",
      "loss: 1.007925  [73600/209452]\n",
      "loss: 0.995086  [76800/209452]\n",
      "loss: 0.976389  [80000/209452]\n",
      "loss: 1.023116  [83200/209452]\n",
      "loss: 0.989949  [86400/209452]\n",
      "loss: 1.012623  [89600/209452]\n",
      "loss: 1.028431  [92800/209452]\n",
      "loss: 0.953692  [96000/209452]\n",
      "loss: 0.985859  [99200/209452]\n",
      "loss: 0.953083  [102400/209452]\n",
      "loss: 0.985774  [105600/209452]\n",
      "loss: 0.941409  [108800/209452]\n",
      "loss: 1.006288  [112000/209452]\n",
      "loss: 1.005283  [115200/209452]\n",
      "loss: 0.981204  [118400/209452]\n",
      "loss: 0.975141  [121600/209452]\n",
      "loss: 0.960442  [124800/209452]\n",
      "loss: 1.003121  [128000/209452]\n",
      "loss: 0.967511  [131200/209452]\n",
      "loss: 0.943564  [134400/209452]\n",
      "loss: 0.982581  [137600/209452]\n",
      "loss: 0.954238  [140800/209452]\n",
      "loss: 0.925245  [144000/209452]\n",
      "loss: 0.910007  [147200/209452]\n",
      "loss: 0.958234  [150400/209452]\n",
      "loss: 0.960455  [153600/209452]\n",
      "loss: 1.033596  [156800/209452]\n",
      "loss: 0.970938  [160000/209452]\n",
      "loss: 0.989476  [163200/209452]\n",
      "loss: 1.008998  [166400/209452]\n",
      "loss: 0.958052  [169600/209452]\n",
      "loss: 0.958834  [172800/209452]\n",
      "loss: 0.988292  [176000/209452]\n",
      "loss: 0.965535  [179200/209452]\n",
      "loss: 0.941024  [182400/209452]\n",
      "loss: 0.950640  [185600/209452]\n",
      "loss: 1.009943  [188800/209452]\n",
      "loss: 0.894930  [192000/209452]\n",
      "loss: 0.898898  [195200/209452]\n",
      "loss: 0.958113  [198400/209452]\n",
      "loss: 0.891108  [201600/209452]\n",
      "loss: 0.946589  [204800/209452]\n",
      "loss: 0.968050  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 0.936222 \n",
      "\n",
      "Epoch 2\n",
      "---------------------------\n",
      "loss: 0.974631  [ 3200/209452]\n",
      "loss: 0.882653  [ 6400/209452]\n",
      "loss: 0.891800  [ 9600/209452]\n",
      "loss: 0.869209  [12800/209452]\n",
      "loss: 0.979904  [16000/209452]\n",
      "loss: 0.912159  [19200/209452]\n",
      "loss: 0.881223  [22400/209452]\n",
      "loss: 0.881384  [25600/209452]\n",
      "loss: 0.909367  [28800/209452]\n",
      "loss: 0.903922  [32000/209452]\n",
      "loss: 0.999025  [35200/209452]\n",
      "loss: 0.899185  [38400/209452]\n",
      "loss: 0.933068  [41600/209452]\n",
      "loss: 0.872608  [44800/209452]\n",
      "loss: 1.030684  [48000/209452]\n",
      "loss: 0.960558  [51200/209452]\n",
      "loss: 1.001020  [54400/209452]\n",
      "loss: 0.892676  [57600/209452]\n",
      "loss: 1.013204  [60800/209452]\n",
      "loss: 1.036679  [64000/209452]\n",
      "loss: 0.984009  [67200/209452]\n",
      "loss: 0.869494  [70400/209452]\n",
      "loss: 0.912070  [73600/209452]\n",
      "loss: 0.959133  [76800/209452]\n",
      "loss: 0.849753  [80000/209452]\n",
      "loss: 0.853139  [83200/209452]\n",
      "loss: 0.889828  [86400/209452]\n",
      "loss: 0.876524  [89600/209452]\n",
      "loss: 0.881486  [92800/209452]\n",
      "loss: 0.989711  [96000/209452]\n",
      "loss: 1.026250  [99200/209452]\n",
      "loss: 0.902506  [102400/209452]\n",
      "loss: 1.046266  [105600/209452]\n",
      "loss: 0.941774  [108800/209452]\n",
      "loss: 0.967434  [112000/209452]\n",
      "loss: 0.848244  [115200/209452]\n",
      "loss: 0.985098  [118400/209452]\n",
      "loss: 0.977793  [121600/209452]\n",
      "loss: 0.905447  [124800/209452]\n",
      "loss: 0.862262  [128000/209452]\n",
      "loss: 0.970303  [131200/209452]\n",
      "loss: 0.881639  [134400/209452]\n",
      "loss: 0.941244  [137600/209452]\n",
      "loss: 0.959965  [140800/209452]\n",
      "loss: 0.902990  [144000/209452]\n",
      "loss: 1.020015  [147200/209452]\n",
      "loss: 1.041292  [150400/209452]\n",
      "loss: 1.052055  [153600/209452]\n",
      "loss: 0.816283  [156800/209452]\n",
      "loss: 0.968094  [160000/209452]\n",
      "loss: 0.846521  [163200/209452]\n",
      "loss: 0.918417  [166400/209452]\n",
      "loss: 0.883997  [169600/209452]\n",
      "loss: 0.877289  [172800/209452]\n",
      "loss: 1.076306  [176000/209452]\n",
      "loss: 0.827028  [179200/209452]\n",
      "loss: 0.848627  [182400/209452]\n",
      "loss: 0.872659  [185600/209452]\n",
      "loss: 0.823024  [188800/209452]\n",
      "loss: 0.948530  [192000/209452]\n",
      "loss: 0.910927  [195200/209452]\n",
      "loss: 1.053709  [198400/209452]\n",
      "loss: 0.906633  [201600/209452]\n",
      "loss: 1.011948  [204800/209452]\n",
      "loss: 1.059955  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 0.908624 \n",
      "\n",
      "Epoch 3\n",
      "---------------------------\n",
      "loss: 0.822864  [ 3200/209452]\n",
      "loss: 0.966438  [ 6400/209452]\n",
      "loss: 0.836221  [ 9600/209452]\n",
      "loss: 0.940008  [12800/209452]\n",
      "loss: 1.029415  [16000/209452]\n",
      "loss: 0.925749  [19200/209452]\n",
      "loss: 0.981407  [22400/209452]\n",
      "loss: 0.810343  [25600/209452]\n",
      "loss: 0.885153  [28800/209452]\n",
      "loss: 0.974382  [32000/209452]\n",
      "loss: 0.864805  [35200/209452]\n",
      "loss: 0.963833  [38400/209452]\n",
      "loss: 0.866320  [41600/209452]\n",
      "loss: 0.935033  [44800/209452]\n",
      "loss: 0.860751  [48000/209452]\n",
      "loss: 0.848362  [51200/209452]\n",
      "loss: 0.970244  [54400/209452]\n",
      "loss: 1.016619  [57600/209452]\n",
      "loss: 1.065296  [60800/209452]\n",
      "loss: 0.872541  [64000/209452]\n",
      "loss: 0.783314  [67200/209452]\n",
      "loss: 1.057751  [70400/209452]\n",
      "loss: 0.925749  [73600/209452]\n",
      "loss: 0.762118  [76800/209452]\n",
      "loss: 0.865469  [80000/209452]\n",
      "loss: 0.996524  [83200/209452]\n",
      "loss: 0.964212  [86400/209452]\n",
      "loss: 1.004368  [89600/209452]\n",
      "loss: 0.951649  [92800/209452]\n",
      "loss: 0.926691  [96000/209452]\n",
      "loss: 0.867380  [99200/209452]\n",
      "loss: 0.927623  [102400/209452]\n",
      "loss: 0.955582  [105600/209452]\n",
      "loss: 0.837001  [108800/209452]\n",
      "loss: 1.040123  [112000/209452]\n",
      "loss: 0.859092  [115200/209452]\n",
      "loss: 1.022451  [118400/209452]\n",
      "loss: 0.939851  [121600/209452]\n",
      "loss: 1.038242  [124800/209452]\n",
      "loss: 0.901860  [128000/209452]\n",
      "loss: 0.808132  [131200/209452]\n",
      "loss: 0.863493  [134400/209452]\n",
      "loss: 1.088360  [137600/209452]\n",
      "loss: 0.839320  [140800/209452]\n",
      "loss: 0.810495  [144000/209452]\n",
      "loss: 0.890580  [147200/209452]\n",
      "loss: 1.028199  [150400/209452]\n",
      "loss: 0.890505  [153600/209452]\n",
      "loss: 0.976859  [156800/209452]\n",
      "loss: 0.987914  [160000/209452]\n",
      "loss: 0.871529  [163200/209452]\n",
      "loss: 0.976567  [166400/209452]\n",
      "loss: 0.826090  [169600/209452]\n",
      "loss: 0.920685  [172800/209452]\n",
      "loss: 0.752852  [176000/209452]\n",
      "loss: 0.886881  [179200/209452]\n",
      "loss: 0.880066  [182400/209452]\n",
      "loss: 0.884733  [185600/209452]\n",
      "loss: 0.925302  [188800/209452]\n",
      "loss: 0.926522  [192000/209452]\n",
      "loss: 0.829204  [195200/209452]\n",
      "loss: 0.894055  [198400/209452]\n",
      "loss: 0.851837  [201600/209452]\n",
      "loss: 0.874566  [204800/209452]\n",
      "loss: 0.828952  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 0.897186 \n",
      "\n",
      "Epoch 4\n",
      "---------------------------\n",
      "loss: 0.945040  [ 3200/209452]\n",
      "loss: 1.012506  [ 6400/209452]\n",
      "loss: 0.858337  [ 9600/209452]\n",
      "loss: 0.915422  [12800/209452]\n",
      "loss: 0.976569  [16000/209452]\n",
      "loss: 0.916160  [19200/209452]\n",
      "loss: 1.008274  [22400/209452]\n",
      "loss: 0.899356  [25600/209452]\n",
      "loss: 1.014692  [28800/209452]\n",
      "loss: 0.756584  [32000/209452]\n",
      "loss: 0.824049  [35200/209452]\n",
      "loss: 0.943732  [38400/209452]\n",
      "loss: 0.739384  [41600/209452]\n",
      "loss: 0.862352  [44800/209452]\n",
      "loss: 1.073272  [48000/209452]\n",
      "loss: 0.926237  [51200/209452]\n",
      "loss: 0.836655  [54400/209452]\n",
      "loss: 0.762873  [57600/209452]\n",
      "loss: 0.944538  [60800/209452]\n",
      "loss: 0.888832  [64000/209452]\n",
      "loss: 0.904956  [67200/209452]\n",
      "loss: 0.907227  [70400/209452]\n",
      "loss: 0.784294  [73600/209452]\n",
      "loss: 0.899054  [76800/209452]\n",
      "loss: 0.762127  [80000/209452]\n",
      "loss: 0.913179  [83200/209452]\n",
      "loss: 0.836716  [86400/209452]\n",
      "loss: 0.849710  [89600/209452]\n",
      "loss: 0.964880  [92800/209452]\n",
      "loss: 0.850429  [96000/209452]\n",
      "loss: 0.868766  [99200/209452]\n",
      "loss: 0.801717  [102400/209452]\n",
      "loss: 0.870661  [105600/209452]\n",
      "loss: 0.801883  [108800/209452]\n",
      "loss: 0.787209  [112000/209452]\n",
      "loss: 1.009544  [115200/209452]\n",
      "loss: 0.924948  [118400/209452]\n",
      "loss: 0.954616  [121600/209452]\n",
      "loss: 1.053051  [124800/209452]\n",
      "loss: 0.892919  [128000/209452]\n",
      "loss: 0.851058  [131200/209452]\n",
      "loss: 0.803913  [134400/209452]\n",
      "loss: 0.862694  [137600/209452]\n",
      "loss: 0.906608  [140800/209452]\n",
      "loss: 0.741961  [144000/209452]\n",
      "loss: 0.999715  [147200/209452]\n",
      "loss: 0.731180  [150400/209452]\n",
      "loss: 0.798456  [153600/209452]\n",
      "loss: 0.786204  [156800/209452]\n",
      "loss: 0.903971  [160000/209452]\n",
      "loss: 0.911853  [163200/209452]\n",
      "loss: 0.855648  [166400/209452]\n",
      "loss: 0.848943  [169600/209452]\n",
      "loss: 0.892536  [172800/209452]\n",
      "loss: 0.882081  [176000/209452]\n",
      "loss: 0.833931  [179200/209452]\n",
      "loss: 0.917344  [182400/209452]\n",
      "loss: 0.858482  [185600/209452]\n",
      "loss: 0.812092  [188800/209452]\n",
      "loss: 0.741978  [192000/209452]\n",
      "loss: 1.024752  [195200/209452]\n",
      "loss: 0.914328  [198400/209452]\n",
      "loss: 1.037959  [201600/209452]\n",
      "loss: 0.801834  [204800/209452]\n",
      "loss: 0.782616  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.881118 \n",
      "\n",
      "Epoch 5\n",
      "---------------------------\n",
      "loss: 0.789969  [ 3200/209452]\n",
      "loss: 0.866173  [ 6400/209452]\n",
      "loss: 0.856701  [ 9600/209452]\n",
      "loss: 0.785386  [12800/209452]\n",
      "loss: 0.854172  [16000/209452]\n",
      "loss: 0.950219  [19200/209452]\n",
      "loss: 0.779465  [22400/209452]\n",
      "loss: 0.811749  [25600/209452]\n",
      "loss: 0.715601  [28800/209452]\n",
      "loss: 0.792107  [32000/209452]\n",
      "loss: 0.874452  [35200/209452]\n",
      "loss: 0.779289  [38400/209452]\n",
      "loss: 0.847105  [41600/209452]\n",
      "loss: 0.913053  [44800/209452]\n",
      "loss: 1.003376  [48000/209452]\n",
      "loss: 0.941650  [51200/209452]\n",
      "loss: 0.959112  [54400/209452]\n",
      "loss: 0.884501  [57600/209452]\n",
      "loss: 0.905555  [60800/209452]\n",
      "loss: 0.899072  [64000/209452]\n",
      "loss: 0.800122  [67200/209452]\n",
      "loss: 0.873355  [70400/209452]\n",
      "loss: 0.884036  [73600/209452]\n",
      "loss: 0.830061  [76800/209452]\n",
      "loss: 0.916058  [80000/209452]\n",
      "loss: 0.847745  [83200/209452]\n",
      "loss: 0.792960  [86400/209452]\n",
      "loss: 0.786769  [89600/209452]\n",
      "loss: 0.781670  [92800/209452]\n",
      "loss: 0.825095  [96000/209452]\n",
      "loss: 0.744652  [99200/209452]\n",
      "loss: 0.774766  [102400/209452]\n",
      "loss: 0.725719  [105600/209452]\n",
      "loss: 0.992426  [108800/209452]\n",
      "loss: 0.996176  [112000/209452]\n",
      "loss: 0.826534  [115200/209452]\n",
      "loss: 0.789393  [118400/209452]\n",
      "loss: 0.795236  [121600/209452]\n",
      "loss: 0.829253  [124800/209452]\n",
      "loss: 0.785738  [128000/209452]\n",
      "loss: 0.913220  [131200/209452]\n",
      "loss: 0.869525  [134400/209452]\n",
      "loss: 0.781462  [137600/209452]\n",
      "loss: 1.101539  [140800/209452]\n",
      "loss: 0.850480  [144000/209452]\n",
      "loss: 0.764307  [147200/209452]\n",
      "loss: 0.816733  [150400/209452]\n",
      "loss: 0.823748  [153600/209452]\n",
      "loss: 0.832719  [156800/209452]\n",
      "loss: 0.795269  [160000/209452]\n",
      "loss: 0.945082  [163200/209452]\n",
      "loss: 0.778383  [166400/209452]\n",
      "loss: 0.937717  [169600/209452]\n",
      "loss: 0.853746  [172800/209452]\n",
      "loss: 0.875187  [176000/209452]\n",
      "loss: 0.990626  [179200/209452]\n",
      "loss: 0.729855  [182400/209452]\n",
      "loss: 1.062511  [185600/209452]\n",
      "loss: 1.010393  [188800/209452]\n",
      "loss: 0.950594  [192000/209452]\n",
      "loss: 0.765385  [195200/209452]\n",
      "loss: 0.716644  [198400/209452]\n",
      "loss: 0.889998  [201600/209452]\n",
      "loss: 0.880121  [204800/209452]\n",
      "loss: 0.934393  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.848901 \n",
      "\n",
      "Epoch 6\n",
      "---------------------------\n",
      "loss: 0.907377  [ 3200/209452]\n",
      "loss: 0.793794  [ 6400/209452]\n",
      "loss: 1.035903  [ 9600/209452]\n",
      "loss: 0.812769  [12800/209452]\n",
      "loss: 0.840905  [16000/209452]\n",
      "loss: 0.793339  [19200/209452]\n",
      "loss: 0.766884  [22400/209452]\n",
      "loss: 1.033619  [25600/209452]\n",
      "loss: 0.765824  [28800/209452]\n",
      "loss: 0.686872  [32000/209452]\n",
      "loss: 0.716429  [35200/209452]\n",
      "loss: 0.794661  [38400/209452]\n",
      "loss: 0.777045  [41600/209452]\n",
      "loss: 0.803796  [44800/209452]\n",
      "loss: 0.890818  [48000/209452]\n",
      "loss: 0.748316  [51200/209452]\n",
      "loss: 0.819840  [54400/209452]\n",
      "loss: 0.783721  [57600/209452]\n",
      "loss: 0.790591  [60800/209452]\n",
      "loss: 0.843399  [64000/209452]\n",
      "loss: 0.817555  [67200/209452]\n",
      "loss: 0.923268  [70400/209452]\n",
      "loss: 0.927337  [73600/209452]\n",
      "loss: 0.894704  [76800/209452]\n",
      "loss: 0.746124  [80000/209452]\n",
      "loss: 0.816889  [83200/209452]\n",
      "loss: 0.910238  [86400/209452]\n",
      "loss: 0.672214  [89600/209452]\n",
      "loss: 0.668657  [92800/209452]\n",
      "loss: 0.734991  [96000/209452]\n",
      "loss: 0.736332  [99200/209452]\n",
      "loss: 0.683635  [102400/209452]\n",
      "loss: 0.817565  [105600/209452]\n",
      "loss: 0.784764  [108800/209452]\n",
      "loss: 0.653319  [112000/209452]\n",
      "loss: 0.838973  [115200/209452]\n",
      "loss: 0.786585  [118400/209452]\n",
      "loss: 0.773667  [121600/209452]\n",
      "loss: 0.761954  [124800/209452]\n",
      "loss: 0.797959  [128000/209452]\n",
      "loss: 0.746501  [131200/209452]\n",
      "loss: 0.792756  [134400/209452]\n",
      "loss: 0.913742  [137600/209452]\n",
      "loss: 0.828778  [140800/209452]\n",
      "loss: 0.831677  [144000/209452]\n",
      "loss: 0.657900  [147200/209452]\n",
      "loss: 0.667747  [150400/209452]\n",
      "loss: 0.818661  [153600/209452]\n",
      "loss: 0.931551  [156800/209452]\n",
      "loss: 0.799343  [160000/209452]\n",
      "loss: 0.976257  [163200/209452]\n",
      "loss: 0.699677  [166400/209452]\n",
      "loss: 0.864412  [169600/209452]\n",
      "loss: 0.718515  [172800/209452]\n",
      "loss: 0.942398  [176000/209452]\n",
      "loss: 0.999535  [179200/209452]\n",
      "loss: 0.713049  [182400/209452]\n",
      "loss: 0.837536  [185600/209452]\n",
      "loss: 0.790700  [188800/209452]\n",
      "loss: 0.751435  [192000/209452]\n",
      "loss: 0.940843  [195200/209452]\n",
      "loss: 0.911720  [198400/209452]\n",
      "loss: 0.767785  [201600/209452]\n",
      "loss: 0.885260  [204800/209452]\n",
      "loss: 0.927363  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.782131 \n",
      "\n",
      "Epoch 7\n",
      "---------------------------\n",
      "loss: 0.923953  [ 3200/209452]\n",
      "loss: 0.910932  [ 6400/209452]\n",
      "loss: 0.799527  [ 9600/209452]\n",
      "loss: 0.648916  [12800/209452]\n",
      "loss: 0.717415  [16000/209452]\n",
      "loss: 0.694841  [19200/209452]\n",
      "loss: 0.782328  [22400/209452]\n",
      "loss: 0.711505  [25600/209452]\n",
      "loss: 0.755407  [28800/209452]\n",
      "loss: 1.019525  [32000/209452]\n",
      "loss: 0.852317  [35200/209452]\n",
      "loss: 0.768393  [38400/209452]\n",
      "loss: 0.770896  [41600/209452]\n",
      "loss: 0.676168  [44800/209452]\n",
      "loss: 0.721153  [48000/209452]\n",
      "loss: 0.763160  [51200/209452]\n",
      "loss: 0.765471  [54400/209452]\n",
      "loss: 0.722940  [57600/209452]\n",
      "loss: 0.856510  [60800/209452]\n",
      "loss: 0.735000  [64000/209452]\n",
      "loss: 0.805396  [67200/209452]\n",
      "loss: 0.806421  [70400/209452]\n",
      "loss: 0.805184  [73600/209452]\n",
      "loss: 0.706528  [76800/209452]\n",
      "loss: 0.668940  [80000/209452]\n",
      "loss: 0.810480  [83200/209452]\n",
      "loss: 1.025446  [86400/209452]\n",
      "loss: 0.842206  [89600/209452]\n",
      "loss: 0.732319  [92800/209452]\n",
      "loss: 0.713715  [96000/209452]\n",
      "loss: 0.734698  [99200/209452]\n",
      "loss: 0.749953  [102400/209452]\n",
      "loss: 0.817138  [105600/209452]\n",
      "loss: 0.811813  [108800/209452]\n",
      "loss: 0.849111  [112000/209452]\n",
      "loss: 0.785322  [115200/209452]\n",
      "loss: 0.687504  [118400/209452]\n",
      "loss: 0.794325  [121600/209452]\n",
      "loss: 0.633017  [124800/209452]\n",
      "loss: 0.703809  [128000/209452]\n",
      "loss: 0.728073  [131200/209452]\n",
      "loss: 0.671448  [134400/209452]\n",
      "loss: 0.762559  [137600/209452]\n",
      "loss: 0.777971  [140800/209452]\n",
      "loss: 0.813727  [144000/209452]\n",
      "loss: 0.636814  [147200/209452]\n",
      "loss: 0.623816  [150400/209452]\n",
      "loss: 0.626194  [153600/209452]\n",
      "loss: 1.015522  [156800/209452]\n",
      "loss: 0.582234  [160000/209452]\n",
      "loss: 0.713620  [163200/209452]\n",
      "loss: 0.725675  [166400/209452]\n",
      "loss: 0.608887  [169600/209452]\n",
      "loss: 0.605311  [172800/209452]\n",
      "loss: 0.933171  [176000/209452]\n",
      "loss: 0.736432  [179200/209452]\n",
      "loss: 0.763426  [182400/209452]\n",
      "loss: 0.815526  [185600/209452]\n",
      "loss: 0.662207  [188800/209452]\n",
      "loss: 0.723295  [192000/209452]\n",
      "loss: 0.647439  [195200/209452]\n",
      "loss: 0.584298  [198400/209452]\n",
      "loss: 0.835754  [201600/209452]\n",
      "loss: 0.737785  [204800/209452]\n",
      "loss: 0.683040  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.682527 \n",
      "\n",
      "Epoch 8\n",
      "---------------------------\n",
      "loss: 0.635852  [ 3200/209452]\n",
      "loss: 0.686128  [ 6400/209452]\n",
      "loss: 0.638352  [ 9600/209452]\n",
      "loss: 0.919177  [12800/209452]\n",
      "loss: 0.681334  [16000/209452]\n",
      "loss: 0.754353  [19200/209452]\n",
      "loss: 0.587091  [22400/209452]\n",
      "loss: 0.658565  [25600/209452]\n",
      "loss: 0.769170  [28800/209452]\n",
      "loss: 0.649139  [32000/209452]\n",
      "loss: 0.572552  [35200/209452]\n",
      "loss: 0.777691  [38400/209452]\n",
      "loss: 0.805217  [41600/209452]\n",
      "loss: 0.631839  [44800/209452]\n",
      "loss: 0.696570  [48000/209452]\n",
      "loss: 0.795641  [51200/209452]\n",
      "loss: 0.461981  [54400/209452]\n",
      "loss: 0.563251  [57600/209452]\n",
      "loss: 0.822765  [60800/209452]\n",
      "loss: 0.522822  [64000/209452]\n",
      "loss: 0.755139  [67200/209452]\n",
      "loss: 0.637586  [70400/209452]\n",
      "loss: 0.584568  [73600/209452]\n",
      "loss: 0.756442  [76800/209452]\n",
      "loss: 0.614956  [80000/209452]\n",
      "loss: 0.612030  [83200/209452]\n",
      "loss: 0.548401  [86400/209452]\n",
      "loss: 0.612342  [89600/209452]\n",
      "loss: 0.479077  [92800/209452]\n",
      "loss: 0.635061  [96000/209452]\n",
      "loss: 0.644581  [99200/209452]\n",
      "loss: 0.655736  [102400/209452]\n",
      "loss: 0.412671  [105600/209452]\n",
      "loss: 0.612842  [108800/209452]\n",
      "loss: 0.543080  [112000/209452]\n",
      "loss: 0.597587  [115200/209452]\n",
      "loss: 0.716191  [118400/209452]\n",
      "loss: 0.499798  [121600/209452]\n",
      "loss: 0.582969  [124800/209452]\n",
      "loss: 0.660415  [128000/209452]\n",
      "loss: 0.807432  [131200/209452]\n",
      "loss: 0.571386  [134400/209452]\n",
      "loss: 0.818438  [137600/209452]\n",
      "loss: 0.626726  [140800/209452]\n",
      "loss: 0.527253  [144000/209452]\n",
      "loss: 0.587859  [147200/209452]\n",
      "loss: 0.577943  [150400/209452]\n",
      "loss: 0.749428  [153600/209452]\n",
      "loss: 0.604723  [156800/209452]\n",
      "loss: 0.528067  [160000/209452]\n",
      "loss: 0.550382  [163200/209452]\n",
      "loss: 0.730857  [166400/209452]\n",
      "loss: 0.716340  [169600/209452]\n",
      "loss: 0.468621  [172800/209452]\n",
      "loss: 0.771714  [176000/209452]\n",
      "loss: 0.692818  [179200/209452]\n",
      "loss: 0.835256  [182400/209452]\n",
      "loss: 0.485732  [185600/209452]\n",
      "loss: 0.632775  [188800/209452]\n",
      "loss: 0.580101  [192000/209452]\n",
      "loss: 0.662445  [195200/209452]\n",
      "loss: 0.620843  [198400/209452]\n",
      "loss: 0.560871  [201600/209452]\n",
      "loss: 0.440352  [204800/209452]\n",
      "loss: 0.622297  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.598404 \n",
      "\n",
      "Epoch 9\n",
      "---------------------------\n",
      "loss: 0.619765  [ 3200/209452]\n",
      "loss: 0.632159  [ 6400/209452]\n",
      "loss: 0.557022  [ 9600/209452]\n",
      "loss: 0.722023  [12800/209452]\n",
      "loss: 0.605951  [16000/209452]\n",
      "loss: 0.625440  [19200/209452]\n",
      "loss: 0.534050  [22400/209452]\n",
      "loss: 0.580251  [25600/209452]\n",
      "loss: 0.489597  [28800/209452]\n",
      "loss: 0.579776  [32000/209452]\n",
      "loss: 0.664760  [35200/209452]\n",
      "loss: 0.576980  [38400/209452]\n",
      "loss: 0.618667  [41600/209452]\n",
      "loss: 0.545864  [44800/209452]\n",
      "loss: 0.640590  [48000/209452]\n",
      "loss: 0.468820  [51200/209452]\n",
      "loss: 0.367215  [54400/209452]\n",
      "loss: 0.554860  [57600/209452]\n",
      "loss: 0.681311  [60800/209452]\n",
      "loss: 0.425945  [64000/209452]\n",
      "loss: 0.659724  [67200/209452]\n",
      "loss: 0.668451  [70400/209452]\n",
      "loss: 0.633536  [73600/209452]\n",
      "loss: 0.557294  [76800/209452]\n",
      "loss: 0.458287  [80000/209452]\n",
      "loss: 0.485698  [83200/209452]\n",
      "loss: 0.491975  [86400/209452]\n",
      "loss: 0.487072  [89600/209452]\n",
      "loss: 0.434444  [92800/209452]\n",
      "loss: 0.358211  [96000/209452]\n",
      "loss: 0.553940  [99200/209452]\n",
      "loss: 0.519028  [102400/209452]\n",
      "loss: 0.618923  [105600/209452]\n",
      "loss: 0.612674  [108800/209452]\n",
      "loss: 0.550416  [112000/209452]\n",
      "loss: 0.432856  [115200/209452]\n",
      "loss: 0.518190  [118400/209452]\n",
      "loss: 0.465503  [121600/209452]\n",
      "loss: 0.833493  [124800/209452]\n",
      "loss: 0.476477  [128000/209452]\n",
      "loss: 0.626498  [131200/209452]\n",
      "loss: 0.359013  [134400/209452]\n",
      "loss: 0.781379  [137600/209452]\n",
      "loss: 0.360442  [140800/209452]\n",
      "loss: 0.493001  [144000/209452]\n",
      "loss: 0.483340  [147200/209452]\n",
      "loss: 0.487822  [150400/209452]\n",
      "loss: 0.697149  [153600/209452]\n",
      "loss: 0.511412  [156800/209452]\n",
      "loss: 0.437258  [160000/209452]\n",
      "loss: 0.594558  [163200/209452]\n",
      "loss: 0.676256  [166400/209452]\n",
      "loss: 0.514278  [169600/209452]\n",
      "loss: 0.505708  [172800/209452]\n",
      "loss: 0.555152  [176000/209452]\n",
      "loss: 0.603184  [179200/209452]\n",
      "loss: 0.768035  [182400/209452]\n",
      "loss: 0.462206  [185600/209452]\n",
      "loss: 0.540023  [188800/209452]\n",
      "loss: 0.494276  [192000/209452]\n",
      "loss: 0.526607  [195200/209452]\n",
      "loss: 0.734336  [198400/209452]\n",
      "loss: 0.276429  [201600/209452]\n",
      "loss: 0.447171  [204800/209452]\n",
      "loss: 0.695278  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.549455 \n",
      "\n",
      "Epoch 10\n",
      "---------------------------\n",
      "loss: 0.619947  [ 3200/209452]\n",
      "loss: 0.574203  [ 6400/209452]\n",
      "loss: 0.522782  [ 9600/209452]\n",
      "loss: 0.693307  [12800/209452]\n",
      "loss: 0.493147  [16000/209452]\n",
      "loss: 0.744434  [19200/209452]\n",
      "loss: 0.465274  [22400/209452]\n",
      "loss: 0.606938  [25600/209452]\n",
      "loss: 0.506683  [28800/209452]\n",
      "loss: 0.393173  [32000/209452]\n",
      "loss: 0.730651  [35200/209452]\n",
      "loss: 0.376113  [38400/209452]\n",
      "loss: 0.519278  [41600/209452]\n",
      "loss: 0.508128  [44800/209452]\n",
      "loss: 0.542352  [48000/209452]\n",
      "loss: 0.624962  [51200/209452]\n",
      "loss: 0.485207  [54400/209452]\n",
      "loss: 0.562635  [57600/209452]\n",
      "loss: 0.498194  [60800/209452]\n",
      "loss: 0.530983  [64000/209452]\n",
      "loss: 0.547544  [67200/209452]\n",
      "loss: 0.700643  [70400/209452]\n",
      "loss: 0.434048  [73600/209452]\n",
      "loss: 0.500917  [76800/209452]\n",
      "loss: 0.637151  [80000/209452]\n",
      "loss: 0.622004  [83200/209452]\n",
      "loss: 0.537716  [86400/209452]\n",
      "loss: 0.531940  [89600/209452]\n",
      "loss: 0.473656  [92800/209452]\n",
      "loss: 0.715695  [96000/209452]\n",
      "loss: 0.757340  [99200/209452]\n",
      "loss: 0.569427  [102400/209452]\n",
      "loss: 0.427634  [105600/209452]\n",
      "loss: 0.547605  [108800/209452]\n",
      "loss: 0.643677  [112000/209452]\n",
      "loss: 0.350598  [115200/209452]\n",
      "loss: 0.578114  [118400/209452]\n",
      "loss: 0.370029  [121600/209452]\n",
      "loss: 0.640835  [124800/209452]\n",
      "loss: 0.625227  [128000/209452]\n",
      "loss: 0.549235  [131200/209452]\n",
      "loss: 0.532940  [134400/209452]\n",
      "loss: 0.375126  [137600/209452]\n",
      "loss: 0.419211  [140800/209452]\n",
      "loss: 0.547054  [144000/209452]\n",
      "loss: 0.686504  [147200/209452]\n",
      "loss: 0.508431  [150400/209452]\n",
      "loss: 0.483933  [153600/209452]\n",
      "loss: 0.440145  [156800/209452]\n",
      "loss: 0.543659  [160000/209452]\n",
      "loss: 0.356450  [163200/209452]\n",
      "loss: 0.455137  [166400/209452]\n",
      "loss: 0.619604  [169600/209452]\n",
      "loss: 0.641632  [172800/209452]\n",
      "loss: 0.650755  [176000/209452]\n",
      "loss: 0.575871  [179200/209452]\n",
      "loss: 0.539294  [182400/209452]\n",
      "loss: 0.450124  [185600/209452]\n",
      "loss: 0.560524  [188800/209452]\n",
      "loss: 0.405549  [192000/209452]\n",
      "loss: 0.552205  [195200/209452]\n",
      "loss: 0.681145  [198400/209452]\n",
      "loss: 0.521138  [201600/209452]\n",
      "loss: 0.476723  [204800/209452]\n",
      "loss: 0.572286  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.519225 \n",
      "\n",
      "Epoch 11\n",
      "---------------------------\n",
      "loss: 0.408062  [ 3200/209452]\n",
      "loss: 0.484454  [ 6400/209452]\n",
      "loss: 0.730872  [ 9600/209452]\n",
      "loss: 0.628726  [12800/209452]\n",
      "loss: 0.431972  [16000/209452]\n",
      "loss: 0.519900  [19200/209452]\n",
      "loss: 0.522901  [22400/209452]\n",
      "loss: 0.459738  [25600/209452]\n",
      "loss: 0.519732  [28800/209452]\n",
      "loss: 0.351648  [32000/209452]\n",
      "loss: 0.374214  [35200/209452]\n",
      "loss: 0.416877  [38400/209452]\n",
      "loss: 0.613289  [41600/209452]\n",
      "loss: 0.541765  [44800/209452]\n",
      "loss: 0.622296  [48000/209452]\n",
      "loss: 0.544973  [51200/209452]\n",
      "loss: 0.547353  [54400/209452]\n",
      "loss: 0.534216  [57600/209452]\n",
      "loss: 0.526542  [60800/209452]\n",
      "loss: 0.360145  [64000/209452]\n",
      "loss: 0.531250  [67200/209452]\n",
      "loss: 0.612397  [70400/209452]\n",
      "loss: 0.631678  [73600/209452]\n",
      "loss: 0.520461  [76800/209452]\n",
      "loss: 0.510594  [80000/209452]\n",
      "loss: 0.390353  [83200/209452]\n",
      "loss: 0.478267  [86400/209452]\n",
      "loss: 0.480132  [89600/209452]\n",
      "loss: 0.314691  [92800/209452]\n",
      "loss: 0.589482  [96000/209452]\n",
      "loss: 0.593672  [99200/209452]\n",
      "loss: 0.493578  [102400/209452]\n",
      "loss: 0.339976  [105600/209452]\n",
      "loss: 0.375028  [108800/209452]\n",
      "loss: 0.596577  [112000/209452]\n",
      "loss: 0.415469  [115200/209452]\n",
      "loss: 0.536175  [118400/209452]\n",
      "loss: 0.443112  [121600/209452]\n",
      "loss: 0.663650  [124800/209452]\n",
      "loss: 0.404197  [128000/209452]\n",
      "loss: 0.633744  [131200/209452]\n",
      "loss: 0.474623  [134400/209452]\n",
      "loss: 0.673058  [137600/209452]\n",
      "loss: 0.461559  [140800/209452]\n",
      "loss: 0.460270  [144000/209452]\n",
      "loss: 0.402958  [147200/209452]\n",
      "loss: 0.562975  [150400/209452]\n",
      "loss: 0.288077  [153600/209452]\n",
      "loss: 0.617312  [156800/209452]\n",
      "loss: 0.637888  [160000/209452]\n",
      "loss: 0.564569  [163200/209452]\n",
      "loss: 0.514849  [166400/209452]\n",
      "loss: 0.586290  [169600/209452]\n",
      "loss: 0.476607  [172800/209452]\n",
      "loss: 0.255332  [176000/209452]\n",
      "loss: 0.448413  [179200/209452]\n",
      "loss: 0.595456  [182400/209452]\n",
      "loss: 0.400943  [185600/209452]\n",
      "loss: 0.675645  [188800/209452]\n",
      "loss: 0.506655  [192000/209452]\n",
      "loss: 0.537547  [195200/209452]\n",
      "loss: 0.661700  [198400/209452]\n",
      "loss: 0.251840  [201600/209452]\n",
      "loss: 0.574686  [204800/209452]\n",
      "loss: 0.563674  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.496763 \n",
      "\n",
      "Epoch 12\n",
      "---------------------------\n",
      "loss: 0.683219  [ 3200/209452]\n",
      "loss: 0.472917  [ 6400/209452]\n",
      "loss: 0.380685  [ 9600/209452]\n",
      "loss: 0.673786  [12800/209452]\n",
      "loss: 0.424976  [16000/209452]\n",
      "loss: 0.520267  [19200/209452]\n",
      "loss: 0.629660  [22400/209452]\n",
      "loss: 0.520641  [25600/209452]\n",
      "loss: 0.398939  [28800/209452]\n",
      "loss: 0.428365  [32000/209452]\n",
      "loss: 0.343049  [35200/209452]\n",
      "loss: 0.636638  [38400/209452]\n",
      "loss: 0.402040  [41600/209452]\n",
      "loss: 0.507896  [44800/209452]\n",
      "loss: 0.422697  [48000/209452]\n",
      "loss: 0.585433  [51200/209452]\n",
      "loss: 0.386471  [54400/209452]\n",
      "loss: 0.361508  [57600/209452]\n",
      "loss: 0.505537  [60800/209452]\n",
      "loss: 0.344274  [64000/209452]\n",
      "loss: 0.367106  [67200/209452]\n",
      "loss: 0.725888  [70400/209452]\n",
      "loss: 0.444386  [73600/209452]\n",
      "loss: 0.495775  [76800/209452]\n",
      "loss: 0.423022  [80000/209452]\n",
      "loss: 0.406981  [83200/209452]\n",
      "loss: 0.540477  [86400/209452]\n",
      "loss: 0.475748  [89600/209452]\n",
      "loss: 0.289592  [92800/209452]\n",
      "loss: 0.539037  [96000/209452]\n",
      "loss: 0.439748  [99200/209452]\n",
      "loss: 0.365969  [102400/209452]\n",
      "loss: 0.437541  [105600/209452]\n",
      "loss: 0.420839  [108800/209452]\n",
      "loss: 0.378078  [112000/209452]\n",
      "loss: 0.471995  [115200/209452]\n",
      "loss: 0.359184  [118400/209452]\n",
      "loss: 0.447886  [121600/209452]\n",
      "loss: 0.616921  [124800/209452]\n",
      "loss: 0.629445  [128000/209452]\n",
      "loss: 0.450131  [131200/209452]\n",
      "loss: 0.399457  [134400/209452]\n",
      "loss: 0.361031  [137600/209452]\n",
      "loss: 0.514583  [140800/209452]\n",
      "loss: 0.454005  [144000/209452]\n",
      "loss: 0.567998  [147200/209452]\n",
      "loss: 0.374082  [150400/209452]\n",
      "loss: 0.587371  [153600/209452]\n",
      "loss: 0.562084  [156800/209452]\n",
      "loss: 0.283353  [160000/209452]\n",
      "loss: 0.568557  [163200/209452]\n",
      "loss: 0.513309  [166400/209452]\n",
      "loss: 0.438530  [169600/209452]\n",
      "loss: 0.354128  [172800/209452]\n",
      "loss: 0.656760  [176000/209452]\n",
      "loss: 0.547841  [179200/209452]\n",
      "loss: 0.499018  [182400/209452]\n",
      "loss: 0.625071  [185600/209452]\n",
      "loss: 0.473061  [188800/209452]\n",
      "loss: 0.438955  [192000/209452]\n",
      "loss: 0.676916  [195200/209452]\n",
      "loss: 0.559743  [198400/209452]\n",
      "loss: 0.503492  [201600/209452]\n",
      "loss: 0.472028  [204800/209452]\n",
      "loss: 0.386775  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.477759 \n",
      "\n",
      "Epoch 13\n",
      "---------------------------\n",
      "loss: 0.395551  [ 3200/209452]\n",
      "loss: 0.609395  [ 6400/209452]\n",
      "loss: 0.587539  [ 9600/209452]\n",
      "loss: 0.633040  [12800/209452]\n",
      "loss: 0.525297  [16000/209452]\n",
      "loss: 0.620752  [19200/209452]\n",
      "loss: 0.474683  [22400/209452]\n",
      "loss: 0.440166  [25600/209452]\n",
      "loss: 0.414799  [28800/209452]\n",
      "loss: 0.509043  [32000/209452]\n",
      "loss: 0.562510  [35200/209452]\n",
      "loss: 0.446636  [38400/209452]\n",
      "loss: 0.500192  [41600/209452]\n",
      "loss: 0.662687  [44800/209452]\n",
      "loss: 0.455896  [48000/209452]\n",
      "loss: 0.433052  [51200/209452]\n",
      "loss: 0.561955  [54400/209452]\n",
      "loss: 0.588266  [57600/209452]\n",
      "loss: 0.366160  [60800/209452]\n",
      "loss: 0.674838  [64000/209452]\n",
      "loss: 0.453335  [67200/209452]\n",
      "loss: 0.747773  [70400/209452]\n",
      "loss: 0.516612  [73600/209452]\n",
      "loss: 0.481807  [76800/209452]\n",
      "loss: 0.468662  [80000/209452]\n",
      "loss: 0.446111  [83200/209452]\n",
      "loss: 0.395000  [86400/209452]\n",
      "loss: 0.681213  [89600/209452]\n",
      "loss: 0.393435  [92800/209452]\n",
      "loss: 0.634788  [96000/209452]\n",
      "loss: 0.275754  [99200/209452]\n",
      "loss: 0.421157  [102400/209452]\n",
      "loss: 0.560220  [105600/209452]\n",
      "loss: 0.430207  [108800/209452]\n",
      "loss: 0.594224  [112000/209452]\n",
      "loss: 0.350914  [115200/209452]\n",
      "loss: 0.498473  [118400/209452]\n",
      "loss: 0.452601  [121600/209452]\n",
      "loss: 0.448563  [124800/209452]\n",
      "loss: 0.391380  [128000/209452]\n",
      "loss: 0.555199  [131200/209452]\n",
      "loss: 0.245469  [134400/209452]\n",
      "loss: 0.330756  [137600/209452]\n",
      "loss: 0.453418  [140800/209452]\n",
      "loss: 0.448224  [144000/209452]\n",
      "loss: 0.380286  [147200/209452]\n",
      "loss: 0.545344  [150400/209452]\n",
      "loss: 0.307001  [153600/209452]\n",
      "loss: 0.266252  [156800/209452]\n",
      "loss: 0.429089  [160000/209452]\n",
      "loss: 0.279736  [163200/209452]\n",
      "loss: 0.567662  [166400/209452]\n",
      "loss: 0.442111  [169600/209452]\n",
      "loss: 0.447782  [172800/209452]\n",
      "loss: 0.500627  [176000/209452]\n",
      "loss: 0.454213  [179200/209452]\n",
      "loss: 0.549758  [182400/209452]\n",
      "loss: 0.428415  [185600/209452]\n",
      "loss: 0.608788  [188800/209452]\n",
      "loss: 0.452336  [192000/209452]\n",
      "loss: 0.371376  [195200/209452]\n",
      "loss: 0.343465  [198400/209452]\n",
      "loss: 0.438250  [201600/209452]\n",
      "loss: 0.452795  [204800/209452]\n",
      "loss: 0.497628  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.460345 \n",
      "\n",
      "Epoch 14\n",
      "---------------------------\n",
      "loss: 0.447159  [ 3200/209452]\n",
      "loss: 0.499476  [ 6400/209452]\n",
      "loss: 0.512776  [ 9600/209452]\n",
      "loss: 0.494332  [12800/209452]\n",
      "loss: 0.484950  [16000/209452]\n",
      "loss: 0.319107  [19200/209452]\n",
      "loss: 0.452615  [22400/209452]\n",
      "loss: 0.595429  [25600/209452]\n",
      "loss: 0.481170  [28800/209452]\n",
      "loss: 0.391476  [32000/209452]\n",
      "loss: 0.495463  [35200/209452]\n",
      "loss: 0.620760  [38400/209452]\n",
      "loss: 0.413866  [41600/209452]\n",
      "loss: 0.279444  [44800/209452]\n",
      "loss: 0.344954  [48000/209452]\n",
      "loss: 0.589357  [51200/209452]\n",
      "loss: 0.398040  [54400/209452]\n",
      "loss: 0.330579  [57600/209452]\n",
      "loss: 0.457353  [60800/209452]\n",
      "loss: 0.489779  [64000/209452]\n",
      "loss: 0.537874  [67200/209452]\n",
      "loss: 0.515729  [70400/209452]\n",
      "loss: 0.334981  [73600/209452]\n",
      "loss: 0.289327  [76800/209452]\n",
      "loss: 0.323364  [80000/209452]\n",
      "loss: 0.499292  [83200/209452]\n",
      "loss: 0.585056  [86400/209452]\n",
      "loss: 0.540369  [89600/209452]\n",
      "loss: 0.268256  [92800/209452]\n",
      "loss: 0.438761  [96000/209452]\n",
      "loss: 0.522215  [99200/209452]\n",
      "loss: 0.414154  [102400/209452]\n",
      "loss: 0.508665  [105600/209452]\n",
      "loss: 0.387908  [108800/209452]\n",
      "loss: 0.456125  [112000/209452]\n",
      "loss: 0.582957  [115200/209452]\n",
      "loss: 0.484419  [118400/209452]\n",
      "loss: 0.399736  [121600/209452]\n",
      "loss: 0.491103  [124800/209452]\n",
      "loss: 0.463931  [128000/209452]\n",
      "loss: 0.437867  [131200/209452]\n",
      "loss: 0.518119  [134400/209452]\n",
      "loss: 0.477275  [137600/209452]\n",
      "loss: 0.409487  [140800/209452]\n",
      "loss: 0.477858  [144000/209452]\n",
      "loss: 0.712444  [147200/209452]\n",
      "loss: 0.599540  [150400/209452]\n",
      "loss: 0.287042  [153600/209452]\n",
      "loss: 0.318694  [156800/209452]\n",
      "loss: 0.339393  [160000/209452]\n",
      "loss: 0.493811  [163200/209452]\n",
      "loss: 0.408977  [166400/209452]\n",
      "loss: 0.457024  [169600/209452]\n",
      "loss: 0.505244  [172800/209452]\n",
      "loss: 0.438468  [176000/209452]\n",
      "loss: 0.517572  [179200/209452]\n",
      "loss: 0.388990  [182400/209452]\n",
      "loss: 0.379613  [185600/209452]\n",
      "loss: 0.467440  [188800/209452]\n",
      "loss: 0.308629  [192000/209452]\n",
      "loss: 0.519079  [195200/209452]\n",
      "loss: 0.568462  [198400/209452]\n",
      "loss: 0.287763  [201600/209452]\n",
      "loss: 0.592745  [204800/209452]\n",
      "loss: 0.450785  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.443797 \n",
      "\n",
      "Epoch 15\n",
      "---------------------------\n",
      "loss: 0.302206  [ 3200/209452]\n",
      "loss: 0.404263  [ 6400/209452]\n",
      "loss: 0.425284  [ 9600/209452]\n",
      "loss: 0.540196  [12800/209452]\n",
      "loss: 0.549157  [16000/209452]\n",
      "loss: 0.515814  [19200/209452]\n",
      "loss: 0.534120  [22400/209452]\n",
      "loss: 0.480496  [25600/209452]\n",
      "loss: 0.511173  [28800/209452]\n",
      "loss: 0.512050  [32000/209452]\n",
      "loss: 0.426385  [35200/209452]\n",
      "loss: 0.708902  [38400/209452]\n",
      "loss: 0.479115  [41600/209452]\n",
      "loss: 0.431335  [44800/209452]\n",
      "loss: 0.328215  [48000/209452]\n",
      "loss: 0.614869  [51200/209452]\n",
      "loss: 0.247777  [54400/209452]\n",
      "loss: 0.444967  [57600/209452]\n",
      "loss: 0.356076  [60800/209452]\n",
      "loss: 0.366684  [64000/209452]\n",
      "loss: 0.354853  [67200/209452]\n",
      "loss: 0.416246  [70400/209452]\n",
      "loss: 0.425610  [73600/209452]\n",
      "loss: 0.549014  [76800/209452]\n",
      "loss: 0.393749  [80000/209452]\n",
      "loss: 0.472319  [83200/209452]\n",
      "loss: 0.625340  [86400/209452]\n",
      "loss: 0.408944  [89600/209452]\n",
      "loss: 0.521579  [92800/209452]\n",
      "loss: 0.547947  [96000/209452]\n",
      "loss: 0.599675  [99200/209452]\n",
      "loss: 0.394333  [102400/209452]\n",
      "loss: 0.313987  [105600/209452]\n",
      "loss: 0.273030  [108800/209452]\n",
      "loss: 0.280411  [112000/209452]\n",
      "loss: 0.555429  [115200/209452]\n",
      "loss: 0.245364  [118400/209452]\n",
      "loss: 0.378473  [121600/209452]\n",
      "loss: 0.629525  [124800/209452]\n",
      "loss: 0.371529  [128000/209452]\n",
      "loss: 0.397184  [131200/209452]\n",
      "loss: 0.157396  [134400/209452]\n",
      "loss: 0.438883  [137600/209452]\n",
      "loss: 0.319099  [140800/209452]\n",
      "loss: 0.433899  [144000/209452]\n",
      "loss: 0.495833  [147200/209452]\n",
      "loss: 0.544229  [150400/209452]\n",
      "loss: 0.580716  [153600/209452]\n",
      "loss: 0.366040  [156800/209452]\n",
      "loss: 0.435413  [160000/209452]\n",
      "loss: 0.430116  [163200/209452]\n",
      "loss: 0.509143  [166400/209452]\n",
      "loss: 0.480934  [169600/209452]\n",
      "loss: 0.392858  [172800/209452]\n",
      "loss: 0.525059  [176000/209452]\n",
      "loss: 0.401930  [179200/209452]\n",
      "loss: 0.417393  [182400/209452]\n",
      "loss: 0.460256  [185600/209452]\n",
      "loss: 0.373278  [188800/209452]\n",
      "loss: 0.561301  [192000/209452]\n",
      "loss: 0.404318  [195200/209452]\n",
      "loss: 0.426258  [198400/209452]\n",
      "loss: 0.353992  [201600/209452]\n",
      "loss: 0.548123  [204800/209452]\n",
      "loss: 0.439636  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.427248 \n",
      "\n",
      "Epoch 16\n",
      "---------------------------\n",
      "loss: 0.265893  [ 3200/209452]\n",
      "loss: 0.671441  [ 6400/209452]\n",
      "loss: 0.496870  [ 9600/209452]\n",
      "loss: 0.364614  [12800/209452]\n",
      "loss: 0.405486  [16000/209452]\n",
      "loss: 0.492821  [19200/209452]\n",
      "loss: 0.328528  [22400/209452]\n",
      "loss: 0.437455  [25600/209452]\n",
      "loss: 0.531864  [28800/209452]\n",
      "loss: 0.565020  [32000/209452]\n",
      "loss: 0.411089  [35200/209452]\n",
      "loss: 0.433337  [38400/209452]\n",
      "loss: 0.461845  [41600/209452]\n",
      "loss: 0.402867  [44800/209452]\n",
      "loss: 0.302712  [48000/209452]\n",
      "loss: 0.478024  [51200/209452]\n",
      "loss: 0.536617  [54400/209452]\n",
      "loss: 0.469674  [57600/209452]\n",
      "loss: 0.462056  [60800/209452]\n",
      "loss: 0.456758  [64000/209452]\n",
      "loss: 0.180036  [67200/209452]\n",
      "loss: 0.607282  [70400/209452]\n",
      "loss: 0.356659  [73600/209452]\n",
      "loss: 0.311669  [76800/209452]\n",
      "loss: 0.463167  [80000/209452]\n",
      "loss: 0.549351  [83200/209452]\n",
      "loss: 0.324975  [86400/209452]\n",
      "loss: 0.213791  [89600/209452]\n",
      "loss: 0.230123  [92800/209452]\n",
      "loss: 0.426865  [96000/209452]\n",
      "loss: 0.339925  [99200/209452]\n",
      "loss: 0.352790  [102400/209452]\n",
      "loss: 0.576774  [105600/209452]\n",
      "loss: 0.345556  [108800/209452]\n",
      "loss: 0.280925  [112000/209452]\n",
      "loss: 0.394750  [115200/209452]\n",
      "loss: 0.382638  [118400/209452]\n",
      "loss: 0.326479  [121600/209452]\n",
      "loss: 0.629828  [124800/209452]\n",
      "loss: 0.585346  [128000/209452]\n",
      "loss: 0.289622  [131200/209452]\n",
      "loss: 0.363274  [134400/209452]\n",
      "loss: 0.447614  [137600/209452]\n",
      "loss: 0.518905  [140800/209452]\n",
      "loss: 0.280303  [144000/209452]\n",
      "loss: 0.467416  [147200/209452]\n",
      "loss: 0.365460  [150400/209452]\n",
      "loss: 0.390281  [153600/209452]\n",
      "loss: 0.284375  [156800/209452]\n",
      "loss: 0.386527  [160000/209452]\n",
      "loss: 0.514190  [163200/209452]\n",
      "loss: 0.345741  [166400/209452]\n",
      "loss: 0.529486  [169600/209452]\n",
      "loss: 0.351567  [172800/209452]\n",
      "loss: 0.340531  [176000/209452]\n",
      "loss: 0.635341  [179200/209452]\n",
      "loss: 0.336753  [182400/209452]\n",
      "loss: 0.332496  [185600/209452]\n",
      "loss: 0.457262  [188800/209452]\n",
      "loss: 0.570335  [192000/209452]\n",
      "loss: 0.419451  [195200/209452]\n",
      "loss: 0.370529  [198400/209452]\n",
      "loss: 0.320325  [201600/209452]\n",
      "loss: 0.606839  [204800/209452]\n",
      "loss: 0.430898  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.410220 \n",
      "\n",
      "Epoch 17\n",
      "---------------------------\n",
      "loss: 0.347296  [ 3200/209452]\n",
      "loss: 0.469555  [ 6400/209452]\n",
      "loss: 0.462614  [ 9600/209452]\n",
      "loss: 0.378682  [12800/209452]\n",
      "loss: 0.437931  [16000/209452]\n",
      "loss: 0.591519  [19200/209452]\n",
      "loss: 0.473173  [22400/209452]\n",
      "loss: 0.403839  [25600/209452]\n",
      "loss: 0.545015  [28800/209452]\n",
      "loss: 0.624338  [32000/209452]\n",
      "loss: 0.509432  [35200/209452]\n",
      "loss: 0.343383  [38400/209452]\n",
      "loss: 0.504638  [41600/209452]\n",
      "loss: 0.461802  [44800/209452]\n",
      "loss: 0.376506  [48000/209452]\n",
      "loss: 0.588390  [51200/209452]\n",
      "loss: 0.512804  [54400/209452]\n",
      "loss: 0.272947  [57600/209452]\n",
      "loss: 0.280973  [60800/209452]\n",
      "loss: 0.405454  [64000/209452]\n",
      "loss: 0.410582  [67200/209452]\n",
      "loss: 0.524019  [70400/209452]\n",
      "loss: 0.526235  [73600/209452]\n",
      "loss: 0.477973  [76800/209452]\n",
      "loss: 0.485311  [80000/209452]\n",
      "loss: 0.419605  [83200/209452]\n",
      "loss: 0.379401  [86400/209452]\n",
      "loss: 0.274722  [89600/209452]\n",
      "loss: 0.287152  [92800/209452]\n",
      "loss: 0.484204  [96000/209452]\n",
      "loss: 0.278368  [99200/209452]\n",
      "loss: 0.303066  [102400/209452]\n",
      "loss: 0.343067  [105600/209452]\n",
      "loss: 0.296776  [108800/209452]\n",
      "loss: 0.504155  [112000/209452]\n",
      "loss: 0.255094  [115200/209452]\n",
      "loss: 0.318769  [118400/209452]\n",
      "loss: 0.466289  [121600/209452]\n",
      "loss: 0.451819  [124800/209452]\n",
      "loss: 0.563331  [128000/209452]\n",
      "loss: 0.345176  [131200/209452]\n",
      "loss: 0.426394  [134400/209452]\n",
      "loss: 0.318894  [137600/209452]\n",
      "loss: 0.439578  [140800/209452]\n",
      "loss: 0.387067  [144000/209452]\n",
      "loss: 0.356498  [147200/209452]\n",
      "loss: 0.300662  [150400/209452]\n",
      "loss: 0.344896  [153600/209452]\n",
      "loss: 0.513635  [156800/209452]\n",
      "loss: 0.278690  [160000/209452]\n",
      "loss: 0.332228  [163200/209452]\n",
      "loss: 0.292827  [166400/209452]\n",
      "loss: 0.373181  [169600/209452]\n",
      "loss: 0.465578  [172800/209452]\n",
      "loss: 0.454734  [176000/209452]\n",
      "loss: 0.425967  [179200/209452]\n",
      "loss: 0.535048  [182400/209452]\n",
      "loss: 0.548684  [185600/209452]\n",
      "loss: 0.421671  [188800/209452]\n",
      "loss: 0.411971  [192000/209452]\n",
      "loss: 0.390381  [195200/209452]\n",
      "loss: 0.310807  [198400/209452]\n",
      "loss: 0.512827  [201600/209452]\n",
      "loss: 0.495749  [204800/209452]\n",
      "loss: 0.322059  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.393059 \n",
      "\n",
      "Epoch 18\n",
      "---------------------------\n",
      "loss: 0.343362  [ 3200/209452]\n",
      "loss: 0.351493  [ 6400/209452]\n",
      "loss: 0.720711  [ 9600/209452]\n",
      "loss: 0.340598  [12800/209452]\n",
      "loss: 0.405193  [16000/209452]\n",
      "loss: 0.447229  [19200/209452]\n",
      "loss: 0.440026  [22400/209452]\n",
      "loss: 0.487167  [25600/209452]\n",
      "loss: 0.471674  [28800/209452]\n",
      "loss: 0.387664  [32000/209452]\n",
      "loss: 0.417726  [35200/209452]\n",
      "loss: 0.426620  [38400/209452]\n",
      "loss: 0.299296  [41600/209452]\n",
      "loss: 0.293531  [44800/209452]\n",
      "loss: 0.465229  [48000/209452]\n",
      "loss: 0.415449  [51200/209452]\n",
      "loss: 0.379517  [54400/209452]\n",
      "loss: 0.209048  [57600/209452]\n",
      "loss: 0.301723  [60800/209452]\n",
      "loss: 0.286203  [64000/209452]\n",
      "loss: 0.332961  [67200/209452]\n",
      "loss: 0.445540  [70400/209452]\n",
      "loss: 0.353149  [73600/209452]\n",
      "loss: 0.319430  [76800/209452]\n",
      "loss: 0.326777  [80000/209452]\n",
      "loss: 0.259493  [83200/209452]\n",
      "loss: 0.409423  [86400/209452]\n",
      "loss: 0.456875  [89600/209452]\n",
      "loss: 0.251941  [92800/209452]\n",
      "loss: 0.428998  [96000/209452]\n",
      "loss: 0.567261  [99200/209452]\n",
      "loss: 0.270824  [102400/209452]\n",
      "loss: 0.477510  [105600/209452]\n",
      "loss: 0.406705  [108800/209452]\n",
      "loss: 0.201232  [112000/209452]\n",
      "loss: 0.222172  [115200/209452]\n",
      "loss: 0.450812  [118400/209452]\n",
      "loss: 0.424349  [121600/209452]\n",
      "loss: 0.378025  [124800/209452]\n",
      "loss: 0.534627  [128000/209452]\n",
      "loss: 0.265572  [131200/209452]\n",
      "loss: 0.381250  [134400/209452]\n",
      "loss: 0.566814  [137600/209452]\n",
      "loss: 0.411288  [140800/209452]\n",
      "loss: 0.403521  [144000/209452]\n",
      "loss: 0.387216  [147200/209452]\n",
      "loss: 0.298509  [150400/209452]\n",
      "loss: 0.255809  [153600/209452]\n",
      "loss: 0.422805  [156800/209452]\n",
      "loss: 0.374303  [160000/209452]\n",
      "loss: 0.324664  [163200/209452]\n",
      "loss: 0.511409  [166400/209452]\n",
      "loss: 0.770350  [169600/209452]\n",
      "loss: 0.360222  [172800/209452]\n",
      "loss: 0.289564  [176000/209452]\n",
      "loss: 0.316936  [179200/209452]\n",
      "loss: 0.394080  [182400/209452]\n",
      "loss: 0.276161  [185600/209452]\n",
      "loss: 0.548384  [188800/209452]\n",
      "loss: 0.510120  [192000/209452]\n",
      "loss: 0.359604  [195200/209452]\n",
      "loss: 0.371912  [198400/209452]\n",
      "loss: 0.264692  [201600/209452]\n",
      "loss: 0.325087  [204800/209452]\n",
      "loss: 0.272583  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.376299 \n",
      "\n",
      "Epoch 19\n",
      "---------------------------\n",
      "loss: 0.305514  [ 3200/209452]\n",
      "loss: 0.397736  [ 6400/209452]\n",
      "loss: 0.352554  [ 9600/209452]\n",
      "loss: 0.293613  [12800/209452]\n",
      "loss: 0.360048  [16000/209452]\n",
      "loss: 0.320663  [19200/209452]\n",
      "loss: 0.368234  [22400/209452]\n",
      "loss: 0.337569  [25600/209452]\n",
      "loss: 0.438872  [28800/209452]\n",
      "loss: 0.334444  [32000/209452]\n",
      "loss: 0.496222  [35200/209452]\n",
      "loss: 0.468311  [38400/209452]\n",
      "loss: 0.509192  [41600/209452]\n",
      "loss: 0.513851  [44800/209452]\n",
      "loss: 0.377263  [48000/209452]\n",
      "loss: 0.388190  [51200/209452]\n",
      "loss: 0.216550  [54400/209452]\n",
      "loss: 0.444828  [57600/209452]\n",
      "loss: 0.494513  [60800/209452]\n",
      "loss: 0.446467  [64000/209452]\n",
      "loss: 0.211987  [67200/209452]\n",
      "loss: 0.245291  [70400/209452]\n",
      "loss: 0.271231  [73600/209452]\n",
      "loss: 0.369110  [76800/209452]\n",
      "loss: 0.326748  [80000/209452]\n",
      "loss: 0.433525  [83200/209452]\n",
      "loss: 0.206075  [86400/209452]\n",
      "loss: 0.326718  [89600/209452]\n",
      "loss: 0.308551  [92800/209452]\n",
      "loss: 0.276173  [96000/209452]\n",
      "loss: 0.415585  [99200/209452]\n",
      "loss: 0.397224  [102400/209452]\n",
      "loss: 0.303912  [105600/209452]\n",
      "loss: 0.217684  [108800/209452]\n",
      "loss: 0.268202  [112000/209452]\n",
      "loss: 0.409524  [115200/209452]\n",
      "loss: 0.263855  [118400/209452]\n",
      "loss: 0.688158  [121600/209452]\n",
      "loss: 0.389495  [124800/209452]\n",
      "loss: 0.228749  [128000/209452]\n",
      "loss: 0.261079  [131200/209452]\n",
      "loss: 0.308346  [134400/209452]\n",
      "loss: 0.518760  [137600/209452]\n",
      "loss: 0.254755  [140800/209452]\n",
      "loss: 0.377867  [144000/209452]\n",
      "loss: 0.367738  [147200/209452]\n",
      "loss: 0.356372  [150400/209452]\n",
      "loss: 0.310598  [153600/209452]\n",
      "loss: 0.429385  [156800/209452]\n",
      "loss: 0.415759  [160000/209452]\n",
      "loss: 0.568965  [163200/209452]\n",
      "loss: 0.425543  [166400/209452]\n",
      "loss: 0.330344  [169600/209452]\n",
      "loss: 0.343728  [172800/209452]\n",
      "loss: 0.420580  [176000/209452]\n",
      "loss: 0.245548  [179200/209452]\n",
      "loss: 0.391712  [182400/209452]\n",
      "loss: 0.402468  [185600/209452]\n",
      "loss: 0.365911  [188800/209452]\n",
      "loss: 0.402209  [192000/209452]\n",
      "loss: 0.302637  [195200/209452]\n",
      "loss: 0.473392  [198400/209452]\n",
      "loss: 0.312437  [201600/209452]\n",
      "loss: 0.477886  [204800/209452]\n",
      "loss: 0.354287  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.360180 \n",
      "\n",
      "Epoch 20\n",
      "---------------------------\n",
      "loss: 0.393891  [ 3200/209452]\n",
      "loss: 0.338237  [ 6400/209452]\n",
      "loss: 0.277891  [ 9600/209452]\n",
      "loss: 0.205839  [12800/209452]\n",
      "loss: 0.264460  [16000/209452]\n",
      "loss: 0.303813  [19200/209452]\n",
      "loss: 0.257145  [22400/209452]\n",
      "loss: 0.403490  [25600/209452]\n",
      "loss: 0.301091  [28800/209452]\n",
      "loss: 0.276908  [32000/209452]\n",
      "loss: 0.347606  [35200/209452]\n",
      "loss: 0.279315  [38400/209452]\n",
      "loss: 0.350112  [41600/209452]\n",
      "loss: 0.379495  [44800/209452]\n",
      "loss: 0.318902  [48000/209452]\n",
      "loss: 0.370038  [51200/209452]\n",
      "loss: 0.163664  [54400/209452]\n",
      "loss: 0.276489  [57600/209452]\n",
      "loss: 0.340402  [60800/209452]\n",
      "loss: 0.525230  [64000/209452]\n",
      "loss: 0.318387  [67200/209452]\n",
      "loss: 0.623736  [70400/209452]\n",
      "loss: 0.477092  [73600/209452]\n",
      "loss: 0.351564  [76800/209452]\n",
      "loss: 0.371168  [80000/209452]\n",
      "loss: 0.211221  [83200/209452]\n",
      "loss: 0.319188  [86400/209452]\n",
      "loss: 0.289335  [89600/209452]\n",
      "loss: 0.336929  [92800/209452]\n",
      "loss: 0.210327  [96000/209452]\n",
      "loss: 0.374387  [99200/209452]\n",
      "loss: 0.360642  [102400/209452]\n",
      "loss: 0.294142  [105600/209452]\n",
      "loss: 0.317498  [108800/209452]\n",
      "loss: 0.261682  [112000/209452]\n",
      "loss: 0.431774  [115200/209452]\n",
      "loss: 0.212875  [118400/209452]\n",
      "loss: 0.468414  [121600/209452]\n",
      "loss: 0.315713  [124800/209452]\n",
      "loss: 0.326820  [128000/209452]\n",
      "loss: 0.409879  [131200/209452]\n",
      "loss: 0.430218  [134400/209452]\n",
      "loss: 0.427267  [137600/209452]\n",
      "loss: 0.297530  [140800/209452]\n",
      "loss: 0.408749  [144000/209452]\n",
      "loss: 0.374270  [147200/209452]\n",
      "loss: 0.263184  [150400/209452]\n",
      "loss: 0.336026  [153600/209452]\n",
      "loss: 0.465437  [156800/209452]\n",
      "loss: 0.257415  [160000/209452]\n",
      "loss: 0.354107  [163200/209452]\n",
      "loss: 0.469875  [166400/209452]\n",
      "loss: 0.442406  [169600/209452]\n",
      "loss: 0.580601  [172800/209452]\n",
      "loss: 0.420692  [176000/209452]\n",
      "loss: 0.531234  [179200/209452]\n",
      "loss: 0.414514  [182400/209452]\n",
      "loss: 0.372468  [185600/209452]\n",
      "loss: 0.359727  [188800/209452]\n",
      "loss: 0.264971  [192000/209452]\n",
      "loss: 0.375332  [195200/209452]\n",
      "loss: 0.389178  [198400/209452]\n",
      "loss: 0.426318  [201600/209452]\n",
      "loss: 0.450513  [204800/209452]\n",
      "loss: 0.242606  [208000/209452]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.345833 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------\")\n",
    "    train_loop(hsd_train_dataloader, hsd_model, loss_fn, hsd_optimizer)\n",
    "    test_loop(hsd_test_dataloader, hsd_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
